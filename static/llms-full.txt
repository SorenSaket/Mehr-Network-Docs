# Mehr Network — Complete Documentation

> Decentralized mesh networking infrastructure powered by Proof of Service.
> Source: https://mehr.network
> Generated: 2026-03-01

---

# Overview

## Introduction
<!-- Source: docs/introduction.md -->



# Mehr Network

**Decentralized Mesh Infrastructure Powered by Proof of Service**

Proof of work wastes electricity. Proof of stake rewards capital, not contribution. Mehr uses **proof of service** — a token is minted only when a real service is delivered to a real paying client through a funded payment channel. Relay a packet, store a block, run a computation — that's how MHR enters circulation. No work is wasted. No token is unearned.

Mehr is a decentralized network where every resource — bandwidth, compute, storage, connectivity — is a discoverable, negotiable, verifiable, payable capability. Nodes participate at whatever level their hardware allows. Nothing is required except a cryptographic keypair.

## What Makes Mehr Different

### Proof of Service

Most decentralized networks create tokens through artificial work (hashing) or capital lockup (staking). Mehr mints tokens only when a provider delivers a real service — relaying traffic, storing data, or executing computations — to a client who pays through a funded payment channel. Minting is proportional to real economic activity and capped at 50% of net service income. A 2% burn on every payment creates a deflationary counterforce that keeps supply bounded.

### Zero Trust Economics

The economic layer assumes every participant is adversarial. Two mechanisms make cheating structurally unprofitable in connected networks: **non-deterministic service assignment** (the client can't choose who serves the request) and a **net-income revenue cap** (cycling MHR produces zero minting). No staking, no slashing, no trust scores required. Identity is just a keypair — but opening a payment channel requires visible balance on the [CRDT ledger](economics/crdt-ledger), and building [reputation](protocol/security#reputation) requires sustained honest service, so Sybil identities face economic friction even without explicit identity verification.

In isolated partitions — where an attacker could control all nodes and nullify non-deterministic assignment — three defense layers [bound damage to a predictable amount](economics/token-security#attack-isolated-partition). During bootstrap (epoch < 100,000), **genesis-anchored minting** prevents all minting without provable connectivity to genesis nodes. **Active-set-scaled emission** limits minting to the partition's fraction of the network (`min(active_nodes, 100) / 100` of full emission — a 3-node partition gets at most 3%). A **2% service burn** on every payment provides friction during isolation and absorbs excess supply after reconnection. Cumulative excess is bounded because emission halves geometrically — even an infinitely long 3-node partition produces ~1.5% total supply dilution, and realistic durations (weeks to months) produce less than 0.1% (see [Supply Dynamics Proof](economics/token-security#supply-dynamics-proof)). When a partition reconnects, the [CRDT merge rules](economics/epoch-compaction#partition-safe-merge-rules) adopt the winning epoch's snapshot and recover missed settlements via proofs. Excess supply dilutes all holders equally, and the halving schedule makes any supply shock negligible over time.

### Free Between Friends

Communication within your trust network is free — no tokens, no channels, no economic overhead. A local mesh where everyone trusts each other operates at zero cost. The economic layer only activates when traffic crosses trust boundaries. This mirrors how communities actually work: you help your neighbors for free, but charge strangers for using your infrastructure.

### Self-Sovereign Identity

Your identity is your cryptographic key — not an account on someone else's server. [MHR-ID](services/mhr-id) lets you build a rich profile (name, bio, avatar, linked accounts, achievements) where every field is a signed claim that peers can vouch for or dispute. You control who sees each field: public, trusted friends only, friends-of-friends, or specific people. Geographic presence is verified by radio range proofs; external accounts are verified by [FUTO ID-style](https://docs.polycentric.io/futo-id/) crawler and OAuth challenges. No central identity provider. No data broker.

### Subjective Naming

There is no global DNS. [MHR-Name](services/mhr-name) provides human-readable names (`alice@geo:portland`, `my-blog@topic:tech`) that resolve from each viewer's position in the trust graph. Names registered by people you trust outrank names from strangers. Two communities can have different "alice" users — that's by design. Names can point to people, content, or [distributed applications](services/mhr-app).

### Distributed Applications

Applications on Mehr are not hosted on servers — they are [content-addressed packages](services/mhr-app) stored in the mesh. An AppManifest bundles contract code, UI, state schema, and dependencies into a single installable artifact. Users discover apps by name, install them locally, and upgrade via trust-weighted update propagation. No app store. No platform fee. No single point of removal.

## Vision

### Strengthen Communities

The internet was supposed to connect people. Instead, it routed everything through distant data centers owned by a handful of corporations. Mehr reverses this: communication within a community is **free, direct, and unstoppable**. Trusted neighbors relay for each other at zero cost. The economic layer only activates when traffic crosses trust boundaries — just like the real world.

### Democratize Communication

A village with no ISP should still be able to communicate. A country under internet shutdown should still have a mesh. A community that can't afford $30/month per household should be able to share one uplink across a neighborhood. Mehr makes communication infrastructure a commons, not a product.

### One Decentralized Computer

Every device on the network — from a $30 solar relay to a GPU workstation — contributes what it can. Storage, compute, bandwidth, and connectivity are pooled into a single capability marketplace. Your phone delegates AI inference to a neighbor's GPU. Your Raspberry Pi stores data for the mesh. No single point of failure, no single point of control. The network **is** the computer.

### Share Hardware, Save Money

Most hardware sits idle most of the time. A home internet connection averages less than 5% utilization. A desktop GPU sits unused 22 hours a day. Mehr turns idle capacity into shared infrastructure: you earn when others use your resources, and you pay when you use theirs. The result is that communities need far less total hardware to achieve the same capabilities.

## Why Mehr?

The internet depends on centralized infrastructure: ISPs, cloud providers, DNS registrars, certificate authorities. When any of these fail — through censorship, natural disaster, or economic exclusion — people lose connectivity entirely.

Mehr is designed for a world where:

- A village with no internet can still communicate internally over LoRa radio
- A country with internet shutdowns can maintain mesh connectivity between citizens
- A community can run its own local network and bridge to the wider internet through any available uplink
- Every device — from a $30 solar-powered relay to a GPU workstation — contributes what it can and pays for what it needs

## Core Principles

### 1. Transport Agnostic

Any medium that can move bytes is a valid link. The protocol never assumes IP, TCP, or any specific transport. It works from 500 bps radio to 10 Gbps fiber. A single node can bridge between multiple transports simultaneously.

### 2. Capability Agnostic

Nodes are not classified into fixed roles. A node advertises what it can do. What it cannot do, it delegates to a neighbor and pays for the service. Hardware determines capability; the market determines role.

### 3. Partition Tolerant

Network fragmentation is not an error state — it is expected operation. A village on LoRa **is** a partition. A country with internet cut **is** a partition. Every protocol layer functions correctly during partitions and converges correctly when partitions heal.

The [CRDT ledger](economics/crdt-ledger) prevents unbounded state growth through **epoch compaction**: settlement history is periodically snapshotted into a compact bloom filter, GCounter deltas are rebased to zero, and nodes discard individual settlement records. Epochs are triggered by settlement count (≥ 10,000), memory pressure (≥ 500 KB), or a small-partition floor — so even a 20-node village compacts regularly.

When a partition reconnects after a long offline period, the epoch with the highest settlement count wins. The losing partition's settlements are recovered via [settlement proofs](economics/epoch-compaction#late-arrivals-after-compaction) during a 4-epoch verification window — bounded bandwidth, not an unbounded merge. Constrained devices store only their own balance and their neighbors' balances (~1.2 KB) plus the Merkle root, and verify any other balance [on demand](economics/epoch-compaction#snapshot-scaling) via a 640-byte Merkle proof.

### 4. Anonymous by Default

Packets carry no source address. A relay node knows which neighbor handed it a packet, but not whether that neighbor originated it or is relaying it from someone else. Identity is a cryptographic keypair — not a name, not an IP address, not an account. [Human-readable names](services/mhr-name) are optional and trust-scoped. [Profile fields](services/mhr-id#profile-fields) have per-field [visibility controls](services/mhr-id#visibility-controls) — you decide what to reveal and to whom. You can use the network, earn MHR, host content, and communicate without ever revealing who you are.

This does not conflict with paid relay. [Payment channels](economics/payment-channels#bilateral-payment-channels) are **per-hop bilateral** — each relay settles with the direct neighbor that handed it the packet, not with the original sender. The relay knows its immediate neighbor (link-layer information) but not whether that neighbor originated or forwarded the packet. Attribution for payment happens at each hop independently; no relay ever learns the end-to-end path, and no end-to-end payment coordination is needed. See [Per-Hop Independent Relay Rewards](development/design-decisions#per-hop-independent-relay-rewards) for the full design rationale.

### 5. Free Local, Paid Routed

Direct neighbors communicate for free. You pay only when your packets traverse other people's infrastructure. This mirrors real-world economics — talking to your neighbor costs nothing, sending a letter across the country does.

### 6. Layered Separation

Each layer depends only on the layer below it. Applications never touch transport details. Payment never touches routing internals. Security is not bolted on — it is structural.

## Protocol Stack Overview

Mehr is organized into seven layers, each building on the one below. Click any layer to read its full specification.


## How It Works — A Simple Example

1. **Alice** has a Raspberry Pi with a LoRa radio and WiFi. She's in a rural area with no internet. She's registered as `alice@geo:us/oregon/bend` and her profile shows her bio, avatar, and a verified GitHub link.
2. **Bob** has a gateway node 5 km away with a cellular modem providing internet access. He's Alice's trusted peer — they relay for each other for free.
3. **Carol** is somewhere on the internet and wants to message Alice.

Here's what happens:

- Carol looks up `alice@geo:us/oregon/bend` — the name resolves to Alice's node via trust-weighted resolution
- Carol's message is encrypted end-to-end for Alice's public key
- It routes through the internet to Bob's gateway
- Bob relays it over LoRa to Alice (free, because Alice is his trusted peer)
- Alice's device decrypts and displays the message
- Carol's relay cost to reach Bob's gateway is paid automatically through a bilateral payment channel

Carol can see Alice's public profile fields (bio, avatar, verified GitHub) but not her phone number — Alice set that to DirectTrust visibility, so only her trusted peers can see it.

No central server. No accounts. No subscriptions. Just cryptographic identities, trust-weighted naming, and a marketplace for capabilities.

## Next Steps

- **Understand the protocol**: Start with [Physical Transport](protocol/physical-transport) and work up the stack
- **Explore the economics**: Learn how [MHR tokens](economics/mhr-token) and [stochastic relay rewards](economics/payment-channels) enable decentralized resource markets
- **Identity and naming**: See how [MHR-ID](services/mhr-id) builds self-sovereign profiles and how [MHR-Name](services/mhr-name) provides trust-weighted naming
- **Distributed apps**: Learn how [AppManifests](services/mhr-app) package and distribute applications across the mesh
- **See the real-world impact**: Understand [how Mehr affects existing economics](economics/real-world-impact) and how participants earn
- **See the hardware**: Check out the [reference designs](hardware/reference-designs) for building Mehr nodes
- **Read the full spec**: The complete [protocol specification](specification) covers every detail

---

## FAQ
<!-- Source: docs/faq.md -->

# Frequently Asked Questions

Plain-language answers. No jargon.

## The Basics

<details className="faq-item">
<summary>What is Mehr?</summary>

Mehr is a communication network that doesn't need the internet, phone towers, or any central service. Devices talk directly to each other using radios (LoRa, WiFi, Bluetooth) and relay messages through the mesh — like passing a note through friends until it reaches the person you want.

There's no company in the middle. No account to create. No server to depend on. Your identity is a cryptographic key pair — you generate it on your device, and that's it. See [Introduction](introduction) for the full overview.

</details>

<details className="faq-item">
<summary>How do I join?</summary>

Get a device (even just a phone), install the Mehr app, and power it on. Your device generates a cryptographic identity and starts discovering nearby nodes. There's no sign-up, no email, no phone number required.

To become part of a community, mark your neighbors as trusted — and have them mark you as trusted. That's it. The trust graph *is* the network.

</details>

<details className="faq-item">
<summary>What device do I need?</summary>

Anything from a $5 ESP32 module to a full desktop. The protocol adapts to what you have:

| Device | What It Does |
|--------|-------------|
| ESP32 + LoRa ($5–15) | Basic radio relay — extends mesh coverage |
| Raspberry Pi ($35–50) | Full node — relay, storage, naming, routing |
| Android phone | All services + UI — messaging, social, voice |
| Desktop/server | High-capacity relay, storage provider, compute provider |

See [Device Tiers](hardware/device-tiers) for detailed specifications per tier.

</details>

<details className="faq-item">
<summary>Is it free?</summary>

Talking to your trusted peers (friends, family, neighbors) is always free — no tokens, no fees. This is the [trust neighborhoods](economics/trust-neighborhoods) model: if you trust someone, relaying their traffic costs you nothing.

When your traffic crosses through strangers' infrastructure — people you don't know, who have no reason to carry your traffic for free — that costs a small amount of [MHR tokens](economics/mhr-token). You earn tokens by relaying traffic for others, so for most users the system balances out: you earn by participating and spend by using.

</details>

<details className="faq-item">
<summary>Do I need to buy tokens?</summary>

No. You earn MHR automatically by relaying traffic and providing services. A device that's turned on and connected earns tokens passively. If you don't want to deal with tokens at all, a [gateway operator](economics/token-economics#gateway-operators-fiat-onramp) can handle it — you pay a small monthly fee (in regular money) and they take care of the crypto side.

</details>

---

## Finding Things

<details className="faq-item">
<summary>How do I find local news and events?</summary>

Content on Mehr is tagged with [geographic scopes](economics/trust-neighborhoods#hierarchical-scopes) — like `geo:us/or/portland`. When you open the social feed, you see posts from your neighborhood first, then your city, then your region. It's like a local newspaper that writes itself.

A local event or city feed might look like: `events@geo:us/or/portland`. Anyone in the Portland trust network can post to it. Popular content — posts that lots of people in the scope read — propagates outward automatically.

</details>

<details className="faq-item">
<summary>How do I find my friends?</summary>

By exchanging public keys — either in person (QR code scan) or through a mutual trusted contact. Once you have someone's key, you can always find them on the mesh. They can also register a human-readable name like `alice@geo:us/or/portland` through [MHR-Name](services/mhr-name), and you can look them up by name.

</details>

<details className="faq-item">
<summary>How do I browse without a search engine?</summary>

Three ways:

1. **Trust-based feeds**: You see content from people your community trusts. This is the default experience — open the app and see what your neighborhood is reading.
2. **Curated channels**: People you trust create curated feeds — hand-picked collections of the best content on a topic. Subscribe to feeds that match your interests.
3. **Name resolution**: If you know what you're looking for, type its name. [MHR-Name](services/mhr-name) resolves human-readable names to content — like DNS, but without central authority.

</details>

---

## Creating Content

<details className="faq-item">
<summary>Does it cost money to post?</summary>

Yes — a tiny amount. Every post is stored on the network, and storage costs MHR. This is the anti-spam mechanism: posting costs tokens, so flooding the network with garbage is economically irrational.

Within your trust network (friends and neighbors), posting is free.

</details>

<details className="faq-item">
<summary>Can I earn from my content?</summary>

Yes. When someone pays to read your full post, a portion of their fee goes back to you — this is called **kickback**. You set the percentage when you publish (default is about 50%).

Popular content that earns more kickback than it costs to store becomes **self-funding** — it lives as long as people read it, at no cost to you. Content nobody reads expires when you stop paying for storage.

</details>

<details className="faq-item">
<summary>What kinds of content can I publish?</summary>

Anything: text posts, photo essays, music albums, video courses, scientific papers, games, software, podcasts. The same envelope/post system works for all content types. The preview shows whatever makes sense (track listing for music, abstract for papers, screenshots for games).

</details>

<details className="faq-item">
<summary>What about curators?</summary>

Anyone can be a curator. You create a curated feed — a list of the best posts you've found — and publish it. Others subscribe to your feed. When they read posts you recommended, the original authors earn kickback AND you earn a separate fee for the curation. Two people get paid: the creator and the curator.

</details>

---

## Communication

<details className="faq-item">
<summary>How do I message someone?</summary>

Open the messaging app, pick a contact, type your message. It's end-to-end encrypted — only you and the recipient can read it. If they're offline, the network holds the message and delivers it when they come back online (like email, but encrypted).

</details>

<details className="faq-item">
<summary>Can I make voice calls?</summary>

Yes, on connections with enough bandwidth. WiFi and cellular links support real-time voice. On slow radio links, voice isn't practical — use text messaging instead.

</details>

<details className="faq-item">
<summary>Can I send photos and videos?</summary>

Yes. The app adapts to your connection:

| Connection | What you can send/receive |
|-----------|--------------------------|
| WiFi or cellular | Photos, videos, full media |
| Moderate radio link | Compressed images, text |
| Slow radio (LoRa) | Text only, with tiny image previews |

You never need to think about this — the app handles it automatically.

</details>

<details className="faq-item">
<summary>What happens when I'm moving around?</summary>

Your device automatically handles roaming. It constantly listens for nearby nodes on all its radios (WiFi, Bluetooth, LoRa) and connects to the best one available — no manual switching required.

- **Walk into a cafe with a Mehr WiFi node?** Your device connects in under a second.
- **Walk out of WiFi range?** Traffic shifts to LoRa automatically. Apps adapt (images become text previews).
- **On a voice call while moving?** The call hands off between nodes with less than a second of interruption. Quality may change but the call doesn't drop.

</details>

---

## Community

<details className="faq-item">
<summary>How do communities form?</summary>

You mark people as trusted. They mark you as trusted. When a group of people all trust each other, that's a community — a [trust neighborhood](economics/trust-neighborhoods). Nobody "creates" it or "runs" it — it emerges from real-world relationships.

Each person tags themselves with where they are (e.g., Portland, Oregon) and what they're into (e.g., gaming, science). These tags — called [scopes](economics/trust-neighborhoods#hierarchical-scopes) — are how feeds and names work. No authority approves your tags. Communities converge on naming through social consensus, the same way they do today.

</details>

<details className="faq-item">
<summary>Can I run a local forum?</summary>

Yes. A forum is just a shared space where community members post. A moderator contract enforces whatever rules your community agrees on. Different forums can have different rules — there's no platform-wide content policy.

</details>

<details className="faq-item">
<summary>Can I sell things on a local marketplace?</summary>

Yes. Post a listing (text, photos, price) tagged with your geographic scope, and it's visible to your neighborhood. Buyers contact you directly. Payment can happen in person, through an external service, or through MHR escrow.

</details>

<details className="faq-item">
<summary>Can I host a website or blog?</summary>

Yes, and it's much simpler than traditional hosting:

| Traditional web | Mehr |
|----------------|-------|
| Rent a server | Not needed — content lives in the mesh |
| Buy a domain name ($10–50/year) | Pick a name for free (`myblog@geo:us/or/portland`) |
| Get an SSL certificate | Not needed — everything is encrypted and verified automatically |
| Pay for traffic spikes | Visitors pay their own relay costs, not you |

You pay only for storage (tiny amounts of MHR), and popular content gets cheaper because it's cached everywhere.

</details>

<details className="faq-item">
<summary>Can I store my files on the network?</summary>

Yes. Mehr provides [decentralized cloud storage](applications/cloud-storage) — like Dropbox, but your files are encrypted on your device before being stored across multiple mesh nodes. No cloud provider has access to your files. Your devices sync automatically through the mesh. You can share files with specific people by granting them a decryption key.

If you don't want to deal with tokens, a [gateway operator](economics/token-economics#gateway-operators-fiat-onramp) can offer cloud storage as a fiat-billed service — same experience as any cloud storage app, but backed by the mesh.

</details>

<details className="faq-item">
<summary>Can I earn by sharing my storage?</summary>

Yes — and it's one of the easiest ways to start earning MHR. Any device with spare disk space can offer [storage services](applications/cloud-storage#earning-mhr-through-storage). You configure how much space to share, storage nodes advertise their availability, and clients form agreements with you. You earn μMHR for every epoch your storage is used. No special hardware needed — a Raspberry Pi with a USB drive works fine.

</details>

<details className="faq-item">
<summary>What happens when I move to a different location?</summary>

Your device [roams seamlessly](applications/roaming). Mehr identity is your cryptographic key, not a network address. When you walk from WiFi to LoRa range to another WiFi node, your connections don't drop — traffic shifts to the best available transport in under a second. Apps adapt to link quality (images become previews on slow links, full quality returns on fast links). You can even plug an ethernet cable into different ports at different locations and stay connected with zero configuration.

</details>

---

## Privacy and Safety

<details className="faq-item">
<summary>Is it private?</summary>

Yes. Messages are end-to-end encrypted. Social posts can be public (scoped) or neighborhood-only (unscoped). There is no central server with a copy of your messages, your contacts, or your browsing history. Your identity is a cryptographic key — you never need to provide your real name.

</details>

<details className="faq-item">
<summary>Can someone spy on my messages?</summary>

No. End-to-end encryption means only the sender and recipient can read a message. Relay nodes carry encrypted blobs they cannot decrypt. Even your direct neighbors don't know if a packet originated from you or if you're just relaying it for someone else.

</details>

<details className="faq-item">
<summary>Can someone shut down the network?</summary>

No single point of failure. There's no server to seize, no company to shut down, no domain to block. As long as any two devices can reach each other — by radio, WiFi, Bluetooth, or anything else — the network works.

</details>

<details className="faq-item">
<summary>What about illegal or harmful content?</summary>

There is no central moderator. Instead, [content governance](economics/content-governance) is distributed:

- **Every node decides for itself** what to store, relay, and display. No node is forced to host or forward content it objects to.
- **Trust revocation** is the enforcement mechanism. If your community discovers you're producing harmful content, they remove you from trusted peers — cutting off your free relay, storage, credit, and reputation.
- **Economics limits abuse**: posting costs money, content starts local (doesn't go global without genuine demand), and there's no algorithm to amplify engagement.
- **Curators filter quality**: most readers follow curated feeds, not raw unfiltered streams.

This is the same tradeoff every free society makes: individual freedom with social consequences. No central authority decides what's allowed, but communities enforce their own norms.

</details>

---

## Economy

<details className="faq-item">
<summary>How does money work on Mehr?</summary>

MHR is the network's internal token. Think of it like arcade tokens — valuable inside the arcade (network services), designed to be spent.

- **You earn MHR** by relaying traffic, storing data, or providing other services
- **You spend MHR** when your messages cross through untrusted infrastructure, or when you read paid content
- **Content creators earn MHR** through kickback — a share of what readers pay
- **Talking to friends is always free** — MHR only matters at trust boundaries

</details>

<details className="faq-item">
<summary>What's it worth in real money?</summary>

MHR has no official exchange rate with any fiat currency. But because it buys real services (bandwidth, storage, compute, content), it has real value — and people will likely trade it informally. This is fine. The network's health doesn't depend on preventing exchange; it works as a closed-loop economy regardless.

</details>

<details className="faq-item">
<summary>Can I buy MHR instead of earning it?</summary>

Yes. If someone sells you MHR they earned through relay work, you can spend it on the network. The seller earned those tokens through real service — the network benefited. You're indirectly funding infrastructure. This is no different from buying bus tokens.

</details>

<details className="faq-item">
<summary>What if I don't want to run a relay? Can I just pay to use the network?</summary>

Yes. **Gateway operators** handle this. A gateway is a regular node that accepts fiat payment (subscription, prepaid, or pay-as-you-go) and gives you network access in return. From your perspective, you sign up, pay a monthly bill, and use the network — just like a phone plan. You never see or touch MHR tokens.

The gateway adds you as a trusted peer and extends credit, so your traffic flows through them for free. The gateway handles MHR costs on your behalf. Multiple gateways compete in any area, so pricing stays competitive. You can switch gateways at any time — your identity is yours, not the gateway's.

See [Gateway Operators](economics/token-economics#gateway-operators-fiat-onramp) for details.

</details>

<details className="faq-item">
<summary>Can I get rich from MHR?</summary>

That's not the point. MHR is designed to be spent on services, not hoarded. There's no ICO and no hidden allocation — the genesis gateway receives a transparent, disclosed allocation visible in the ledger from day one. Tail emission (0.1% annual) mildly dilutes idle holdings. Lost keys permanently remove supply. The economic incentive is to earn and spend, not to accumulate.

</details>

---

## Licensing and Digital Assets

<details className="faq-item">
<summary>Can I sell licenses for my work on Mehr?</summary>

Yes. Mehr has a built-in [digital licensing](applications/licensing) system. You publish a **LicenseOffer** alongside your asset (photo, music, software, dataset) specifying terms — price, whether derivatives are allowed, whether commercial use is permitted, and how many licenses can be issued. Buyers pay you directly (in MHR or fiat) and receive a **LicenseGrant** signed by both parties.

</details>

<details className="faq-item">
<summary>How does license verification work?</summary>

A LicenseGrant is cryptographically signed by both the licensor and licensee. Anyone can verify it by checking the Ed25519 signatures — no network connection needed. When someone uses a licensed asset in a derivative work, they include the LicenseGrant hash in their post's references. Readers can follow the chain: derivative work → LicenseGrant → LicenseOffer → original asset.

</details>

<details className="faq-item">
<summary>Can licenses be enforced?</summary>

Not at the protocol level. Mehr proves a license exists (or doesn't) — it doesn't prevent unlicensed use. This is the same as the real world: copyright exists whether or not someone violates it. Enforcement happens through social reputation (community trust) and legal systems (courts). The cryptographic proof makes disputes straightforward to resolve.

</details>

<details className="faq-item">
<summary>Do licenses work outside of Mehr?</summary>

Yes. A LicenseGrant contains public keys and signatures that can be verified with standard cryptographic tools — no Mehr software needed. A website, archive, or court can verify license authenticity from the grant alone. The rights described in the license apply wherever the parties intend them to, not just on the Mehr network.

</details>

---

## Compared to What I Use Now

<details className="faq-item">
<summary>How is this different from the regular internet?</summary>

| | Regular Internet | Mehr |
|--|----------------|-------|
| **Works without ISP** | No | Yes — radio, WiFi, anything |
| **Works during internet shutdown** | No | Yes — local mesh continues |
| **Free local communication** | No — you pay your ISP | Yes — trusted peers are free |
| **Your data on a corporate server** | Yes (Google, Meta, etc.) | No — data stays on your devices and your community's mesh |
| **Can be censored** | Yes — ISPs, DNS, app stores | Extremely difficult — no central control point |
| **Needs an account** | Email, phone number, ID | Just a cryptographic key (anonymous) |
| **Content creators earn** | Platform takes most/all revenue | Direct kickback to creator (~50%) |

</details>

<details className="faq-item">
<summary>Can Mehr replace my internet connection?</summary>

**It depends on where you live.**

In a **dense area** (apartment building, neighborhood, campus) where many nodes run WiFi, the mesh delivers 10–300 Mbps per hop — comparable to cable internet. Add a few shared internet uplinks (Starlink, fiber, cellular) and the community mesh handles distribution. Most people would save 50–75% on connectivity costs.

In a **rural or remote area** with only LoRa radio coverage, Mehr delivers 0.3–50 kbps — enough for text messaging, basic social feeds, and push-to-talk voice, but not video streaming. Here, Mehr provides communication where there was none, or shares one expensive satellite connection across an entire village.

| Your situation | What Mehr does |
|---------------|-----------------|
| Dense urban, many WiFi nodes | Replaces individual ISP subscriptions — share uplinks, save money |
| Suburban, mixed WiFi + LoRa | Supplements your connection — free local communication, shared backup uplink |
| Rural, LoRa only | Provides communication where there is none — text, voice, local services |
| No infrastructure at all | Only option that works — $30 solar radio nodes, no towers needed |

</details>

<details className="faq-item">
<summary>How is this different from Signal or WhatsApp?</summary>

Signal and WhatsApp need internet access and rely on central servers for delivery. Mehr works without internet, stores messages across the mesh (not one company's servers), and the network itself is decentralized. Nobody can block your access because there's nothing to block.

</details>

<details className="faq-item">
<summary>How is this different from Bitcoin?</summary>

Bitcoin is money designed for global financial transactions. MHR is an internal utility token for paying network services. They share some concepts (cryptographic keys, no central authority) but serve completely different purposes. MHR is more like "bus tokens for the network" than a cryptocurrency.

</details>

<details className="faq-item">
<summary>How is this different from Mastodon/Bluesky?</summary>

Mastodon and Bluesky are decentralized social networks that still require internet access and depend on servers run by someone. On Mehr:

| | Mastodon/Bluesky | Mehr |
|---|---|---|
| **Requires internet** | Yes | No — works on radio alone |
| **Requires servers** | Yes (someone hosts instances) | No — content lives on mesh nodes |
| **Content moderation** | Server admin decides | Each node decides for itself |
| **Posting cost** | Free | Small fee (anti-spam) |
| **Creator revenue** | None built-in | Kickback on every read |
| **Works offline** | No | Yes — local mesh continues |

</details>

---

## How It Works (Simple Version)
<!-- Source: docs/eli5.md -->

# How Mehr Works (Simple Version)

No jargon. No technical details. Just how it works.

## The Problem

When you send a message to your neighbor across the street, it travels like this:

```mermaid
graph LR
    You -->|WiFi| Router1[Router] -->|ISP| ISP1[ISP] -->|route| DC[Data center\nmaybe another country] -->|route| ISP2[ISP] -->|ISP| Router2[Router] -->|WiFi| Neighbor
```

Your message travels hundreds of miles to reach someone 50 feet away. You pay a phone company for this. If the company shuts off your service — or a government tells them to — you can't communicate at all.

## The Solution

Mehr lets devices talk directly:

```mermaid
graph LR
    You -->|radio / WiFi| Neighbor
```

No phone company. No data center. No monthly bill. Just devices talking.

## How It Gets Bigger

One house talking to another is nice, but not very useful by itself. Mehr gets useful when many devices form a chain:

```mermaid
graph LR
    You -->|radio| Neighbor -->|WiFi| Cafe -->|internet| World[The World]
```

Each device passes messages along, like a bucket brigade. Your message hops from device to device until it reaches its destination. This chain of devices is called a **mesh**.

```mermaid
graph LR
    You --> Neighbor
    Neighbor --> School --> Library --> More[...]
    Neighbor --> Cafe --> Gateway --> Internet
```

Messages find the best path automatically. If the cafe goes offline, traffic routes through the school instead. No single point of failure.

## Three Rules

**Rule 1: Friends help for free.**

You mark people as "trusted" — like adding a contact. Your devices help each other automatically, at no cost. A neighborhood where everyone trusts each other communicates completely free.

```mermaid
graph LR
    You <-->|trust| Neighbor <-->|trust| Friend
```

> Messages between these three: always free. No tokens. No payments. No overhead.

**Rule 2: Strangers pay a tiny fee.**

If your message passes through a stranger's device, you pay a tiny fee. This is fair — they're using their electricity and bandwidth to help you.

```mermaid
graph LR
    You -->|"trust (free)"| Neighbor -->|"pay (tiny fee)"| Stranger -->|"pay (tiny fee)"| Destination
```

**Rule 3: Helping earns you credit.**

Every time your device passes along someone else's message, you earn credit. This credit is called **MHR**. You spend MHR when you use the network. You earn MHR when others use yours.

```mermaid
graph LR
    Help[You help others] --> Earn[Earn MHR] --> Spend[Spend MHR] --> Others[Others help you] --> Help
```

## What You Can Do

| What | How | Cost |
|------|-----|------|
| Message a friend | Like texting, but encrypted | Free |
| Message a stranger | Same, routed through the mesh | Tiny fee |
| Browse social feeds | See headlines and previews | Free |
| Read full posts | Tap to open content you want | Small fee to author |
| Post content | Text, photos, music, anything | You pay to publish |
| Host a website | Your device serves it | Visitors pay their own way |
| Make a voice call | Works well on WiFi | Same as messaging |
| Store files | Other devices keep copies | Small ongoing fee |

## Social Media on Mehr

Every post has two layers:

```mermaid
graph TD
    Preview["FREE PREVIEW\nHeadlines, summaries, thumbnails\nBrowse as much as you want — zero cost"] -->|tap to open| Full["FULL CONTENT\nArticle, image, song, or video\nTiny fee goes to the author"]
```

**Popular posts pay for themselves.** If enough people read a post, the author earns more than it cost to publish. Posts nobody reads expire naturally — no moderator needed.

**No algorithm decides what you see.** You follow people. You subscribe to topics ("gaming", "local news"). You follow curators — real humans who pick the best stuff. Your feed is what you chose, in chronological order.

## The Money Part

MHR is like tokens at an arcade. You earn them by helping (relaying messages, storing data) and spend them by using (reaching distant people, reading content).

**Can I buy MHR with real money?** The network doesn't have a store, but people may sell tokens to each other. This is fine — someone earned those tokens through real work.

**Can I get rich from MHR?** That's not the point. MHR is designed to be spent, not saved. It's bus tokens, not stocks.

**What if I have no tokens?** You can still talk to your friends for free. MHR only matters when your messages cross through strangers' devices.

## What If I Don't Have Internet?

That's the whole point. Mehr works with:

| Connection | Speed | What you can do |
|-----------|-------|----------------|
| Radio (LoRa) | Slow but long range (up to 15 km) | Text messages, headlines, basic feeds |
| WiFi | Fast, short range | Everything — photos, video, calls |
| Cellular | Fast, wide range | Everything |
| Satellite | Fast, expensive | Gateway for an entire community |
| Anything else | Varies | If it moves bytes, Mehr uses it |

A village with no internet can communicate over radio. Add one satellite dish and the whole village gets internet access through the mesh — shared, at a fraction of the individual cost.

**Traditional model** (everyone pays separately — total: $1,500/month):

```mermaid
graph LR
    H1[House 1] -->|$30/month| ISP1[ISP]
    H2[House 2] -->|$30/month| ISP2[ISP]
    H3[House 3] -->|$30/month| ISP3[ISP]
    H50[House 50] -->|$30/month| ISP50[ISP]
```

**Mehr model** (shared through the mesh — total: ~$30/month + tiny MHR fees):

```mermaid
graph LR
    H1[House 1] -->|radio| H12[House 12] -->|WiFi| GW[Gateway] -->|"$30/month"| Internet
    H2[House 2] -->|radio| H12
    H3[House 3] -->|WiFi| GW
```

> Gateway operator earns from the neighborhood. Everyone else saves 75%+.

## Who Controls It?

Nobody. There is no company behind Mehr. No server to shut down. No account to ban. No terms of service to accept.

- **Your identity** is a cryptographic key on your device. No email, no phone number, no real name required.
- **Your data** lives on your device and your community's devices — not on a corporate server.
- **Your feed** is what you chose to follow — not what an algorithm chose for you.
- **Your content** stays up as long as you (or your readers) pay for it — no platform can remove it.

## The Big Picture

**Traditional internet:**

```mermaid
graph LR
    Everyone1[Everyone] -->|pays| BigCo[Big companies\ncontrol everything] -->|provide| Service
```

**Mehr:**

```mermaid
graph LR
    E1[Everyone] <-->|helps| E2[Everyone]
```

## Want to Learn More?

- [FAQ](faq) — answers to common questions in plain language
- [Introduction](introduction) — the full technical overview
- [MHR Token](economics/mhr-token) — how the economy works in detail
- [Social](applications/social) — how social media works on Mehr
- [Real-World Economics](economics/real-world-impact) — actual cost savings and earnings

---

## Full Specification
<!-- Source: docs/specification.md -->




# Mehr Protocol Specification v1.0

This page is the normative reference for the Mehr protocol. Individual documentation pages provide detailed explanations; this page summarizes the protocol constants, wire formats, and layer dependencies in one place.

## Status

| | |
|---|---|
| **Version** | 1.0 |
| **Status** | Design complete, pre-implementation |
| **Normative sections** | Layers 0–5 (transport through services) |
| **Informative sections** | Layer 6 (applications), hardware reference, roadmap |

## Protocol Constants

| Constant | Value | Defined In |
|----------|-------|-----------|
| Gossip interval | 60 seconds | [Network Protocol](protocol/network-protocol#gossip-protocol) |
| Protocol overhead budget | ≤10% of link bandwidth | [Bandwidth Budget](protocol/network-protocol#bandwidth-budget) |
| CompactPathCost size | 6 bytes (constant) | [Network Protocol](protocol/network-protocol#mehr-extension-compact-path-cost) |
| MehrExtension magic byte | `0x4E` ('N') | [Network Protocol](protocol/network-protocol#mehr-extension-compact-path-cost) |
| Destination hash size | 16 bytes (128-bit) | [Network Protocol](protocol/network-protocol#identity-and-addressing) |
| Smallest MHR unit | 1 μMHR | [MHR Token](economics/mhr-token#properties) |
| Supply ceiling | 2^64 μMHR (asymptotic) | [MHR Token](economics/mhr-token#supply-model) |
| Default relay lottery probability | 1/100 | [Stochastic Rewards](economics/payment-channels#example) |
| Payment channel state size | 200 bytes | [Payment Channels](economics/payment-channels#channel-state) |
| Dispute challenge window | 2,880 gossip rounds (~48h) | [Payment Channels](economics/payment-channels#channel-lifecycle) |
| Channel abandonment threshold | 4 epochs | [Payment Channels](economics/payment-channels#channel-lifecycle) |
| Epoch trigger: settlement count | ≥10,000 batches | [CRDT Ledger](economics/epoch-compaction#epoch-triggers) |
| Epoch trigger: GSet memory | ≥500 KB | [CRDT Ledger](economics/epoch-compaction#epoch-triggers) |
| Epoch acknowledgment threshold | 67% of active set | [CRDT Ledger](economics/epoch-compaction#epoch-lifecycle) |
| Epoch verification window | 4 epochs after activation | [CRDT Ledger](economics/epoch-compaction#epoch-lifecycle) |
| Bloom filter FPR (epoch) | 0.01% | [CRDT Ledger](economics/epoch-compaction#bloom-filter-sizing) |
| DHT replication factor | k=3 | [MHR-DHT](services/mhr-dht#replication-factor) |
| DHT XOR weight (w_xor) | 0.7 | [MHR-DHT](services/mhr-dht#lookup-scoring-function) |
| Storage chunk size | 4 KB | [MHR-Store](services/mhr-store#chunking) |
| Presence beacon size | 20 bytes | [Discovery](marketplace/discovery#presence-beacons) |
| Presence beacon interval | 10 seconds | [Discovery](marketplace/discovery#presence-beacons) |
| Transitive credit limit | 10% per hop, max 2 hops | [Trust & Neighborhoods](economics/trust-neighborhoods#trust-based-credit) |
| Max scopes per node | 8 | [Trust & Neighborhoods](economics/trust-neighborhoods#hierarchical-scopes) |
| Max scope depth | 8 segments | [Trust & Neighborhoods](economics/trust-neighborhoods#wire-format) |
| Max scope segment length | 32 characters | [Trust & Neighborhoods](economics/trust-neighborhoods#wire-format) |
| Vouch expiry (default) | 30 epochs | [MHR-ID](services/mhr-id#vouch-properties) |
| Kickback rate range | 0–255 (u8) | [MHR-Store](services/mhr-store#kickback-rate) |
| Default kickback rate | 128 (~50%) | [Content Propagation](economics/propagation#protocol-constants) |
| IdentityClaim min size | 126 bytes | [MHR-ID](services/mhr-id#wire-format) |
| Vouch size | 121 bytes | [MHR-ID](services/mhr-id#vouch-wire-format) |
| Claim type: GeoPresence | 0 | [MHR-ID](services/mhr-id#claim-types) |
| Claim type: CommunityMember | 1 | [MHR-ID](services/mhr-id#claim-types) |
| Claim type: KeyRotation | 2 | [MHR-ID](services/mhr-id#claim-types) |
| Claim type: Capability | 3 | [MHR-ID](services/mhr-id#claim-types) |
| Claim type: ExternalIdentity | 4 | [MHR-ID](services/mhr-id#claim-types) |
| Claim type: ProfileField | 5 | [MHR-ID](services/mhr-id#profile-fields) |
| Visibility: Public | 0 | [MHR-ID](services/mhr-id#visibility-controls) |
| Visibility: TrustNetwork | 1 | [MHR-ID](services/mhr-id#visibility-controls) |
| Visibility: DirectTrust | 2 | [MHR-ID](services/mhr-id#visibility-controls) |
| Visibility: Named | 3 | [MHR-ID](services/mhr-id#visibility-controls) |
| ProfileField value type: Text | 0 | [MHR-ID](services/mhr-id#value-types) |
| ProfileField value type: ContentHash | 1 | [MHR-ID](services/mhr-id#value-types) |
| ProfileField value type: Coordinates | 2 | [MHR-ID](services/mhr-id#value-types) |
| ProfileField value type: Integer | 3 | [MHR-ID](services/mhr-id#value-types) |
| Identity challenge method: Crawler | 0 | [MHR-ID](services/mhr-id/verification#identity-linking) |
| Identity challenge method: OAuth | 1 | [MHR-ID](services/mhr-id/verification#identity-linking) |
| Max name length | 64 bytes | [MHR-Name](services/mhr-name#name-format) |
| Name binding min size | 122 bytes | [MHR-Name](services/mhr-name#namebinding) |
| Name expiry | 30 epochs | [MHR-Name](services/mhr-name#name-registration) |
| Name target type: NodeID | `0x01` | [MHR-Name](services/mhr-name#wire-format) |
| Name target type: ContentHash | `0x02` | [MHR-Name](services/mhr-name#wire-format) |
| Name target type: AppManifest | `0x03` | [MHR-Name](services/mhr-name#wire-format) |
| Name context sub-type: Register | `0x08` | [MHR-Name](services/mhr-name#message-types) |
| Name context sub-type: Lookup | `0x09` | [MHR-Name](services/mhr-name#message-types) |
| Name context sub-type: LookupResponse | `0x0A` | [MHR-Name](services/mhr-name#message-types) |
| AppManifest format version | 1 | [MHR-App](services/mhr-app#manifest-wire-format) |
| Max contracts per manifest | 15 | [MHR-App](services/mhr-app#appmanifest) |
| Max app dependencies per manifest | 8 | [MHR-App](services/mhr-app#dependencies) |
| Max pub topic templates per manifest | 4 | [MHR-App](services/mhr-app#appmanifest) |
| Max app display name | 32 bytes | [MHR-App](services/mhr-app#appmanifest) |
| App type: Full | 0 | [MHR-App](services/mhr-app#app-types) |
| App type: Headless | 1 | [MHR-App](services/mhr-app#app-types) |
| App type: Static | 2 | [MHR-App](services/mhr-app#app-types) |
| App context sub-type: ManifestPublish | `0x0B` | [MHR-App](services/mhr-app#manifest-message-types) |
| App context sub-type: ManifestLookup | `0x0C` | [MHR-App](services/mhr-app#manifest-message-types) |
| App context sub-type: ManifestLookupResponse | `0x0D` | [MHR-App](services/mhr-app#manifest-message-types) |
| Geo verification: min vouches | 3 (for Verified level) | [Voting](applications/voting#geoverificationlevel) |
| Protocol version encoding | 1 byte (major 4 bits, minor 4 bits) | [Versioning](protocol/versioning#version-field) |
| Extended version escape | Major = 15 → read u16 pair from TLV | [Versioning](protocol/versioning#version-field) |
| Current protocol version | `0x10` (v1.0) | [Versioning](protocol/versioning#version-field) |
| Emission halving shift clamp | max 63 (prevents UB at epoch 6.4M+) | [MHR Token](economics/mhr-token#supply-model) |
| Max curated feed entries | 256 per page | [Social](applications/social#5-curated-feed) |
| LicenseOffer min size | ~160 bytes | [Digital Licensing](applications/licensing#wire-format) |
| LicenseGrant size | 226 bytes | [Digital Licensing](applications/licensing#wire-format) |
| Max custom license terms | 1024 characters | [Digital Licensing](applications/licensing#licenseoffer) |

## Cryptographic Primitives

| Purpose | Algorithm | Output / Key Size |
|---------|-----------|-------------------|
| Identity / Signing | Ed25519 | 256-bit (32-byte public key) |
| Key Exchange | X25519 (Curve25519 DH) | 256-bit |
| Identity Hashing | Blake2b | 256-bit → 128-bit truncated |
| Content Hashing | Blake3 | 256-bit |
| Symmetric Encryption | ChaCha20-Poly1305 | 256-bit key, 96-bit nonce |
| Relay Lottery (VRF) | ECVRF-ED25519-SHA512-TAI (RFC 9381) | 80-byte proof |
| Erasure Coding | Reed-Solomon | Configurable k,m |

## Layer Dependency Graph

```mermaid
graph TD
    L6["Layer 6: Applications<br/>Messaging, Social, Voice, Voting, Licensing, Cloud Storage, Roaming, Hosting"] --> L5
    L5["Layer 5: Service Primitives<br/>MHR-Store, MHR-DHT, MHR-Pub, MHR-Compute, MHR-Name, MHR-ID, MHR-App"] --> L4
    L4["Layer 4: Capability Marketplace<br/>Discovery, Agreements, Verification"] --> L3
    L3["Layer 3: Economic Protocol<br/>MHR Token, Token Economics, Token Security, Stochastic Rewards, CRDT Ledger, Epoch Compaction, Trust Neighborhoods, Propagation"] --> L2
    L2["Layer 2: Security<br/>Link encryption, E2E encryption, Authentication, Key management"] --> L1
    L1["Layer 1: Network Protocol<br/>Identity, Addressing, Routing, Gossip, Congestion Control"] --> L0
    L0["Layer 0: Physical Transport<br/>LoRa, WiFi, Cellular, LTE-M, NB-IoT, Ethernet, BLE, Fiber, Serial"]
```

Each layer depends **only** on the layer directly below it. Applications never touch transport details. Payment never touches routing internals.

## Serialization Rules

All Mehr wire formats use the following conventions:

| Rule | Value |
|------|-------|
| **Byte order** | Little-endian for all multi-byte integers (u16, u32, u64, i64) |
| **Encoding** | Fixed-size binary fields; no self-describing framing (not CBOR, not JSON) |
| **TLV extensions** | Type (u8), Length (u8, max 255), Data (variable). Used in MehrExtension only |
| **Strings** | UTF-8, length-prefixed with u16 (community labels, function IDs) |
| **Hashes** | Raw bytes, no hex encoding on the wire |
| **Signatures** | Raw 64-byte Ed25519 signatures, no ASN.1/DER wrapping |
| **Normalized scores** | Computed on **decoded** values, then divided by the max decoded value in the candidate set. Result is IEEE 754 f32 on nodes that support FP; 16-bit fixed-point (Q0.16, value × 65535) on constrained nodes. Both yield equivalent routing decisions within rounding tolerance |

## Wire Format Summary

### Packet Format (Reticulum-derived)

```
[HEADER 2B] [DEST_HASH 16B] [CONTEXT 1B] [DATA 0-465B]
Max packet size: 484 bytes
Source address: NOT PRESENT (structural sender anonymity)
```

### Mehr Announce Extension

```
[MAGIC 1B: 0x4E] [VERSION 1B] [CompactPathCost 6B] [TLV extensions...]
Minimum: 8 bytes. Carried in announce DATA field.
```

### CompactPathCost

```
[cumulative_cost 2B] [worst_latency_ms 2B] [bottleneck_bps 1B] [hop_count 1B]
Total: 6 bytes (constant regardless of path length)
```

### Payment Channel State

```
[channel_id 16B] [party_a 16B] [party_b 16B] [balance_a 8B]
[balance_b 8B] [sequence 8B] [sig_a 64B] [sig_b 64B]
Total: 200 bytes
```

## Specification Sections

| Spec Section | Documentation Page |
|-------------|-------------------|
| 0. Design Philosophy | [Introduction](introduction) |
| 1. Layer 0: Physical Transport | [Physical Transport](protocol/physical-transport) |
| 2. Layer 1: Network Protocol | [Network Protocol](protocol/network-protocol) |
| 3. Layer 2: Security | [Security](protocol/security) |
| 4. Layer 3: Economic Protocol | [MHR Token](economics/mhr-token), [Token Economics](economics/token-economics), [Token Security](economics/token-security), [Stochastic Relay Rewards](economics/payment-channels), [CRDT Ledger](economics/crdt-ledger), [Epoch Compaction](economics/epoch-compaction), [Trust & Neighborhoods](economics/trust-neighborhoods), [Content Propagation](economics/propagation), [Content Governance](economics/content-governance), [Real-World Economics](economics/real-world-impact) |
| 5. Layer 4: Capability Marketplace | [Overview](marketplace/overview), [Discovery](marketplace/discovery), [Agreements](marketplace/agreements), [Verification](marketplace/verification) |
| 6. Layer 5: Service Primitives | [MHR-Store](services/mhr-store), [MHR-DHT](services/mhr-dht), [MHR-Pub](services/mhr-pub), [MHR-Compute](services/mhr-compute), [MHR-Name](services/mhr-name), [MHR-ID](services/mhr-id), [MHR-App](services/mhr-app) |
| 7. Layer 6: Applications | [Messaging](applications/messaging), [Social](applications/social), [Voice](applications/voice), [Community Apps](applications/community-apps), [Voting](applications/voting), [Digital Licensing](applications/licensing), [Cloud Storage](applications/cloud-storage), [Roaming](applications/roaming), [Hosting](applications/hosting) |
| 8. Hardware Reference | [Reference Designs](hardware/reference-designs), [Device Tiers](hardware/device-tiers) |
| 9. Implementation Roadmap | [Roadmap](development/roadmap) |
| 10. Design Decisions | [Design Decisions](development/design-decisions) |
| 11. Protocol Versioning | [Versioning](protocol/versioning) |
| 12. Open Questions | [Open Questions](development/open-questions) |

## Version

| Version | Status |
|---------|--------|
| **v1.0** | **Current** |

---

*The foundation — Reticulum-based transport, cryptographic identity, Kleinberg small-world routing, stochastic relay rewards, CRDT settlement, epoch compaction, emergent trust neighborhoods, and the capability marketplace — is the protocol. Everything above it — storage, compute, pub/sub, naming, and applications — are services built on that foundation.*

---

## Protocol Stack

### Physical Transport
<!-- Source: docs/protocol/physical-transport.md -->

# Layer 0: Physical Transport

Mehr requires a transport layer that provides transport-agnostic networking over any medium supporting at least a half-duplex channel with ≥5 bps throughput and ≥500 byte MTU. The transport layer is a swappable implementation detail — Mehr defines the interface it needs, not the implementation.

## Transport Requirements

The transport layer must provide:

- **Any medium is a valid link**: LoRa, LTE-M, NB-IoT, WiFi, Ethernet, serial, packet radio, fiber, free-space optical
- **Multiple simultaneous interfaces**: A node can bridge between transports automatically
- **Announce-based routing**: No manual configuration of addresses, subnets, or routing tables
- **Mandatory encryption**: All traffic is encrypted; unencrypted packets are dropped as invalid
- **Sender anonymity**: No source address in packets
- **Constrained-link operation**: Functional at ≥5 bps

## Current Implementation: Reticulum

The current transport implementation uses the [Reticulum Network Stack](https://reticulum.network/), which satisfies all requirements above and is proven on links as slow as 5 bps. Mehr extends it with [CompactPathCost](network-protocol#mehr-extension-compact-path-cost) annotations on announces and an economic layer above.

Reticulum is an implementation choice, not an architectural dependency. Mehr extensions are carried as opaque payload data within Reticulum's announce DATA field — a clean separation that allows the transport to be replaced with a clean-room implementation in the future without affecting any layer above.
> **Key Insight**
The transport layer is a swappable implementation detail. Mehr defines the interface it needs (half-duplex, ≥5 bps, ≥500 byte MTU, mandatory encryption), not the implementation. All economic extensions ride as opaque payload in transport announces — the transport never needs to understand Mehr.

### Participation Levels

Not all nodes need to understand Mehr extensions. Three participation levels coexist on the same mesh:

| Level | Node Type | Understands | Earns MHR | Marketplace |
|-------|-----------|-------------|-----------|-------------|
| **L0** | Transport-only | Wire protocol only | No | No |
| **L1** | Mehr Relay | L0 + CompactPathCost + stochastic rewards | Yes (relay only) | No |
| **L2** | Full Mehr | Everything | Yes | Yes |

**L0 nodes** relay packets and forward announces (including Mehr extensions as opaque bytes) but do not parse economic extensions, earn rewards, or participate in the marketplace. They are zero-cost hops from Mehr's perspective. This ensures the mesh works even when some nodes run the transport layer alone.

**L1 nodes** are the minimum viable Mehr implementation — they parse CompactPathCost, run the VRF relay lottery, and maintain payment channels. This is the target for ESP32 firmware.

**L2 nodes** implement the full protocol stack including capability marketplace, storage, compute, and application services.

### Implementation Strategy

| Platform | Implementation |
|---|---|
| Raspberry Pi, desktop, phone | Rust implementation (primary) |
| ESP32, embedded | Rust `no_std` implementation (L1 minimum) |

All implementations speak the same wire protocol and interoperate on the same network.

## Supported Transports

| Transport | Typical Bandwidth | Typical Range | Duplex | Notes |
|---|---|---|---|---|
| **LoRa (ISM band)** | 0.3-50 kbps | 2-15 km | Half | Unlicensed, low power, high range. [RNode](https://reticulum.network/manual/hardware.html) as reference hardware. |
| **WiFi Ad-hoc** | 10-300 Mbps | 50-200 m | Full | Ubiquitous, short range |
| **WiFi P2P (directional)** | 100-800 Mbps | 1-10 km | Full | Point-to-point backbone links |
| **Cellular (LTE/5G)** | 1-100 Mbps | Via carrier | Full | Requires carrier subscription |
| **LTE-M** | 0.375-1 Mbps | Via carrier | Full | Licensed LPWAN; better building penetration than LoRa, carrier-managed |
| **NB-IoT** | 0.02-0.25 Mbps | Via carrier | Half | Licensed LPWAN; extreme range and battery life, carrier-managed |
| **Ethernet** | 100 Mbps-10 Gbps | Local | Full | Backbone, data center |
| **Serial (RS-232, AX.25)** | 1.2-56 kbps | Varies | Half | Legacy radio, packet radio |
| **Fiber** | 1-100 Gbps | Long haul | Full | Backbone |
| **Bluetooth/BLE** | 1-3 Mbps | 10-100 m | Full | Wearables, phone-to-phone |

A node can have **multiple interfaces active simultaneously**. The network layer selects the best interface for each destination based on cost, latency, and reliability.

## Multi-Interface Bridging

A node with both LoRa and WiFi interfaces automatically bridges between the two networks. Traffic arriving on LoRa can be forwarded over WiFi and vice versa.

The bridge node is where bandwidth characteristics change dramatically — and where the [capability marketplace](../marketplace/overview) becomes valuable. A bridge node can:

- Accept low-bandwidth LoRa traffic from remote sensors
- Forward it over high-bandwidth WiFi to a local network
- Earn relay rewards for the bridging service
- Advertise its bridging capability to nearby nodes

```mermaid
graph LR
    RS["Remote Sensor"] <-- "LoRa (10 kbps)" --> BN["Bridge Node"]
    BN <-- "WiFi (100 Mbps)" --> GW["Gateway"]
```

## Bandwidth Ranges and Their Implications

The 20,000x range between the slowest and fastest supported transports (500 bps to 10 Gbps) has profound implications for protocol design:

> **Trade-off**
Supporting 500 bps to 10 Gbps (a 20,000x range) means every protocol overhead byte must be budgeted. Data objects carry `min_bandwidth` requirements so large transfers are never attempted over constrained links — only hashes and metadata propagate on LoRa.

- **All protocol overhead must be budgeted.** Gossip, routing updates, and economic state consume bandwidth that could carry user data. On a 1 kbps LoRa link, every byte matters.
- **Data objects carry minimum bandwidth requirements.** A 500 KB image declares `min_bandwidth: 10000` (10 kbps). LoRa nodes never attempt to transfer it — they only propagate its hash and metadata.
- **Applications adapt to link quality.** The protocol provides link metrics; applications decide what to send based on available bandwidth.

## NAT Traversal

Residential nodes behind NATs (common for WiFi and Ethernet interfaces) are handled at the transport layer. The Reticulum transport uses its link establishment protocol to traverse NATs — an outbound connection from behind the NAT establishes a bidirectional link without requiring port forwarding or STUN/TURN servers.

For nodes that cannot establish outbound connections (rare), the announce mechanism still propagates their presence. Traffic destined for a NATed node is routed through a neighbor that does have a direct link — functionally equivalent to standard relay forwarding. No special NAT-awareness is needed at the Mehr protocol layers above transport.

## What Mehr Adds Above Transport

The transport layer provides packet delivery, routing, and encryption. Mehr adds everything above:

| Extension | Purpose |
|---|---|
| **[CompactPathCost](network-protocol#mehr-extension-compact-path-cost) on announces** | Enables economic routing — cheapest, fastest, or balanced path selection |
| **[Stochastic relay rewards](../economics/payment-channels)** | Incentivizes relay operators without per-packet payment overhead |
| **[Capability advertisements](../marketplace/overview)** | Makes compute, storage, and connectivity discoverable and purchasable |
| **[CRDT economic ledger](../economics/crdt-ledger)** | Tracks balances without consensus or blockchain |
| **[Trust graph](../economics/trust-neighborhoods)** | Enables free communication between trusted peers |
| **[Congestion control](network-protocol#congestion-control)** | CSMA/CA, per-neighbor fair sharing, priority queuing, backpressure |

These extensions ride on top of the transport's existing gossip and announce mechanisms, staying within the protocol's [bandwidth budget](network-protocol#bandwidth-budget).

<!-- faq-start -->

## Frequently Asked Questions

<details className="faq-item">
<summary>Which radios should I buy to get started with Mehr?</summary>

The easiest entry point is an [RNode](https://reticulum.network/manual/hardware.html) — a LoRa-based radio that serves as the reference hardware for the transport layer. For higher bandwidth, any WiFi-capable device (Raspberry Pi, laptop, phone) can participate. You don’t need a specific radio to join — any supported transport works, and nodes with multiple interfaces bridge between them automatically.

</details>

<details className="faq-item">
<summary>What kind of range can I expect from LoRa?</summary>

LoRa typically achieves 2–15 km line-of-sight depending on antenna height, terrain, and power settings. In urban areas with buildings in the way, expect 1–5 km. Directional antennas and elevated mounting points dramatically improve range. For longer distances, WiFi point-to-point links can reach 1–10 km.

</details>

<details className="faq-item">
<summary>Can Mehr work over the regular internet, or does it require radio hardware?</summary>

Mehr works over any transport — including the internet. Ethernet, WiFi, and cellular connections all function as valid transports. You can run a Mehr node on a home computer connected to your router. Radio hardware (LoRa, packet radio) extends the network into areas without internet connectivity, but it’s not required.

</details>

<details className="faq-item">
<summary>What happens when a message crosses from LoRa to WiFi or vice versa?</summary>

Bridge nodes with multiple interfaces handle this transparently. A packet arriving on LoRa is forwarded over WiFi (or any other interface) by the bridge node. The sender and receiver don’t need to know which transports were used — the routing layer picks the best path automatically. The bridge node can earn relay rewards for providing this service.

</details>

<details className="faq-item">
<summary>Does mixing slow and fast transports create bottlenecks?</summary>

The protocol is designed for this. Data objects carry a `min_bandwidth` field that prevents large transfers from being attempted over slow links. A 500 KB image won’t be pushed over LoRa — only its hash and metadata propagate. Applications adapt to link quality in real time, degrading gracefully on constrained links and resuming full quality on fast ones.

</details>

<!-- faq-end -->

---

### Network Protocol
<!-- Source: docs/protocol/network-protocol.md -->

# MHR-Net: Network Protocol

The network protocol handles identity, addressing, routing, and state propagation across the mesh. It uses [Reticulum](physical-transport) as the transport foundation and extends it with cost-aware routing and economic state gossip.

## Identity and Addressing

> **Specification**
Mehr uses Reticulum's identity model. Every node has a cryptographic identity generated locally with no registrar:

```
NodeIdentity {
    keypair: Ed25519Keypair,            // 256-bit, generated locally
    public_key: Ed25519PublicKey,        // 32 bytes
    destination_hash: [u8; 16],         // truncated hash of public key
    x25519_public: X25519PublicKey,      // derived via RFC 7748 birational map
}
```


### Destination Hash

The destination hash is the node's address — 16 bytes (128 bits), derived from the public key. This provides:

- **Flat address space**: No hierarchy, no subnets, no allocation authority
- **Self-assigned**: Any node can generate an address without asking permission
- **Negligible collision probability**: 2^128 possible addresses
- **Pseudonymous**: The hash is not linked to a real-world identity unless the owner publishes that association

A single node can generate **multiple destination hashes** for different purposes (personal identity, service endpoints, anonymous identities). Each is derived from a separate Ed25519 keypair.

## Packet Format

Mehr uses the [Reticulum packet format](https://reticulum.network/manual/understanding.html):

```
[HEADER 2 bytes] [ADDRESSES 16/32 bytes] [CONTEXT 1 byte] [DATA 0-465 bytes]
```

Header flags encode: propagation type (broadcast/transport), destination type (single/group/plain/link), and packet type (data/announce/link request/proof). Maximum overhead per packet: 35 bytes.

**Critical property** (inherited from Reticulum): The source address is **NOT** in the header. Packets carry only the destination. Sender anonymity is structural.

### Mehr Extension: Compact Path Cost

Mehr extends announces with a constant-size cost summary that each relay updates in-place as it forwards the announce:

```
CompactPathCost {
    cumulative_cost: u16,    // log₂-encoded μMHR/byte (2 bytes)
    worst_latency_ms: u16,   // max latency on any hop in path (2 bytes)
    bottleneck_bps: u8,      // log₂-encoded min bandwidth on path (1 byte)
    hop_count: u8,           // number of relays traversed (1 byte)
}
// Total: 6 bytes (constant, regardless of path length)
```

Each relay updates the running totals as it forwards:
- `cumulative_cost += my_cost_per_byte` (re-encoded to log scale)
- `worst_latency_ms = max(existing, my_measured_latency)`
- `bottleneck_bps = min(existing, my_bandwidth)`
- `hop_count += 1`

**Log encoding for cost**: `encoded = round(16 × log₂(value + 1))`. A u16 covers the full practical cost range with ~6% precision per step.

**Log encoding for bandwidth**: `encoded = round(8 × log₂(bps))`. A u8 covers 1 bps to ~10 Tbps with ~9% precision.

The CompactPathCost is carried in the announce DATA field using a TLV envelope:

```
MehrExtension {
    magic: u8 = 0x4E,           // 'N' — identifies Mehr extension presence
    version: u8,                 // extension format version
    path_cost: CompactPathCost,  // 6 bytes
    extensions: [{               // future extensions via TLV pairs
        type: u8,
        length: u8,
        data: [u8; length],
    }],
}
// Minimum size: 8 bytes (magic + version + path_cost)
```

Nodes that don't understand the `0x4E` magic byte forward the DATA field as opaque payload. Mehr-aware nodes parse and update it.

#### Why No Per-Relay Signatures

> **Key Insight**
Earlier designs signed each relay's cost annotation individually (~84 bytes per relay hop). This is unnecessary for three reasons:

1. **Routing decisions are local.** You select a next-hop neighbor. You only need to trust your neighbor's cost claim — and your neighbor is already authenticated by the link-layer encryption.
2. **Trust is transitive at each hop.** Your neighbor trusts *their* neighbor (link-authenticated), who trusts *their* neighbor, and so on. No node needs to verify claims from relays it has never communicated with.
3. **The market enforces honesty.** A relay that inflates path costs gets routed around. A relay that deflates costs loses money on every packet. Economic incentives are a cheaper and more robust enforcement mechanism than cryptographic proofs for cost claims.

The announce itself remains signed by the destination node (proving authenticity of the route). The path cost summary is trusted transitively through link-layer authentication at each hop — analogous to how BGP trusts direct peers, not every AS along the path.


## Routing

Routing is destination-based with cost annotations, formalized as **greedy forwarding on a small-world graph**. Each node maintains a routing table:

```
RoutingEntry {
    destination: DestinationHash,
    next_hop: InterfaceID + LinkAddress, // which interface, which neighbor

    // From CompactPathCost (6 bytes in announce)
    cumulative_cost: u16,                // log₂-encoded μMHR/byte
    worst_latency_ms: u16,              // max latency on path
    bottleneck_bps: u8,                 // log₂-encoded min bandwidth
    hop_count: u8,                      // relay count

    // Locally computed
    reliability: u8,                     // 0-255 (0=unknown, 255=perfect) — avoids FP on ESP32

    last_updated: Timestamp,
    expires: Timestamp,
}
```

### Small-World Routing Model

Mehr routing is based on the **Kleinberg small-world model**, adapted for a physical mesh with heterogeneous transports. This provides a formal basis for routing scalability.

#### The Network as a Small-World Graph

The destination hash space `[0, 2^128)` forms a **ring**. The circular distance between two addresses is:

```
ring_distance(a, b) = min(|a - b|, 2^128 - |a - b|)
```

The physical mesh naturally provides two types of links, matching Kleinberg's model:

- **Short-range links** (lattice edges): LoRa, WiFi ad-hoc, BLE — these connect geographically nearby nodes, forming an approximate 2D lattice determined by physical proximity.
- **Long-range links** (Kleinberg contacts): Directional WiFi, cellular, internet gateways, fiber — these connect distant nodes, providing shortcuts across the ring.

Kleinberg's result proves that greedy forwarding achieves **O(log² N) expected hops** when long-range link probability follows `P(u→v) ∝ 1/d(u,v)^r` with clustering exponent `r` equal to the network dimension. The distribution of real-world backbone links (many local WiFi, fewer city-to-city, even fewer intercontinental) naturally approximates this harmonic distribution.

#### Greedy Forwarding with Cost Weighting

At each hop, the current node selects the neighbor that minimizes a scoring function:

```
score(neighbor) = α · norm_ring_distance(neighbor, destination)
                + β · norm_cumulative_cost(neighbor)
                + γ · norm_worst_latency(neighbor)
```

Where `norm_*` normalizes each metric to `[0, 1]` across the candidate set. Normalization is performed on **decoded** values (not the log-encoded wire representation):

```
Decoding (for normalization):
  decoded_cost = (2 ^ (encoded / 16.0)) - 1    // inverse of log₂ encoding
  decoded_bw   = 2 ^ (encoded / 8.0)           // inverse of bandwidth encoding

  norm_cumulative_cost = decoded_cost(neighbor) / max_decoded_cost_in_candidate_set
  norm_worst_latency   = neighbor.worst_latency_ms / max_latency_in_candidate_set
```

This preserves the true cost ratios. Log-encoded values compress dynamic range for wire efficiency but must not be used directly in scoring — otherwise a 1000x cost difference would appear as only ~2x.

The weights α, β, γ are derived from the per-packet `PathPolicy`:

```
PathPolicy: enum {
    Cheapest,                           // α=0.1, β=0.8, γ=0.1
    Fastest,                            // α=0.1, β=0.1, γ=0.8
    MostReliable,                       // maximize delivery probability
    Balanced(cost_weight, latency_weight, reliability_weight),
}
```

Pure greedy routing (α=1, β=0, γ=0) guarantees **O(log² N) expected hops**. Cost and latency weighting trades path length for economic efficiency — a path may take more hops if each hop is cheaper or faster.

Applications specify their preferred policy:
- **Voice traffic** uses `Fastest` — latency matters most
- **Bulk storage replication** uses `Cheapest` — cost efficiency matters most
- **Default** is `Balanced` — a weighted combination of all factors

With N nodes where each has O(1) long-range links (typical for relay nodes), expected path length is **O(log² N)**. Backbone nodes with O(log N) connections reduce this to **O(log N)**.

#### Why Mehr Does Not Need Location Swapping

Unlike Freenet/Hyphanet, which uses location swapping to arrange nodes into a navigable topology, Mehr does not need this mechanism:

1. **Destination hashes are self-assigned** — each node's position on the ring is fixed by its Ed25519 keypair.
2. **Announcements build routing tables** — when a node announces itself, it creates routing table entries across the mesh that function as navigable links.
3. **Multi-transport bridges are natural long-range contacts** — a node bridging LoRa to WiFi to internet inherently provides the long-range shortcuts that make the graph navigable.

The announcement propagation itself creates the navigable topology. Each announcement that reaches a distant node via a backbone link creates exactly the kind of long-range routing table entry that Kleinberg's model requires.

### Path Discovery

Path discovery works via announcements:

1. A node announces its destination hash to the network, signed with its Ed25519 key
2. The announcement propagates through the mesh via greedy forwarding, with each relay updating the [CompactPathCost](#mehr-extension-compact-path-cost) running totals in-place (no per-relay signatures — link-layer authentication is sufficient)
3. Receiving nodes record the path (or multiple paths) and select based on the scoring function above
4. Multiple paths are retained and scored — the best path per policy is used, with fallback to alternatives on failure

### Announce Propagation Rules

Announces are **event-driven with periodic refresh**, not purely periodic:

```
Announce triggers:
  - First boot / new identity: immediate announce
  - Interface change: announce on new interface within 1 gossip round
  - Cost change > 25%: re-announce with updated CompactPathCost
  - Periodic refresh: every 30 minutes (1,800 seconds)
  - Forced refresh: on peer request (pull-based for constrained links)
```

**Hop limit**: Announces carry a `max_hops` field (u8, default 128). Each relay decrements by 1; announces at 0 are not forwarded. This prevents unbounded propagation in large meshes while ensuring O(log² N) reachability.

**Expiry**: Routing entries expire at `last_updated + announce_interval × 3` (default 90 minutes). If no refresh is received, the entry is marked stale (still usable at lower priority) for one additional interval, then evicted. On memory pressure, LRU eviction removes the least-recently-used stale entries first, then lowest-reliability active entries.

**Link failure detection**: If a direct neighbor misses 3 consecutive gossip rounds (3 minutes) without response, the link is marked down. All routing entries using that neighbor as next-hop are immediately marked stale (not deleted — the neighbor may return). After 10 missed rounds, entries are evicted.

## Gossip Protocol

> **Specification**
All protocol-level state propagation uses a common gossip mechanism:

```
GossipRound (every 60 seconds with each neighbor):

1. Exchange state summaries (bloom filters of known state)
2. Identify deltas (what I have that you don't, and vice versa)
3. Exchange deltas (compact, only what's new)
4. Apply received state via CRDT merge rules
```


### Gossip Bloom Filter

State summaries use a compact bloom filter to identify deltas without exchanging full state:

```
GossipFilter {
    bits: [u8; N],          // N scales with known state entries
    hash_count: 3,          // 3 independent hash functions (Blake3-derived)
    target_fpr: 1%,         // 1% false positive rate (tolerant — FP only causes redundant delta)
}
```

| Known state entries | Filter size | FPR |
|-------------------|-------------|-----|
| 100 | 120 bytes | ~1% |
| 1,000 | 1.2 KB | ~1% |
| 10,000 | 12 KB | ~1% |

On constrained links (below 10 kbps), the filter is capped at 256 bytes — entries beyond the filter capacity are omitted (pull-only mode for Tiers 3-4 handles this). False positives are harmless: they cause a delta item to not be requested, but the item will be caught in the next round when the bloom filter is regenerated.

**New node joining**: A node with empty state sends an all-zeros bloom filter. The neighbor detects maximum divergence and sends a prioritized subset of state (Tier 1 first, then Tier 2, etc.) spread across multiple gossip rounds to avoid link saturation.

A single gossip round multiplexes all protocol state:
- Routing announcements (with cost annotations)
- Ledger state (settlements, balances)
- Trust graph updates
- Capability advertisements
- DHT metadata
- Pub/sub notifications

### Bandwidth Budget

Total protocol overhead targets **≤10% of available link bandwidth**, allocated by priority tier:

```
Gossip Bandwidth Budget (per link):

  Tier 1 (critical):  Routing announcements         — up to 3%
  Tier 2 (economic):  Payment + settlement state     — up to 3%
  Tier 3 (services):  Capabilities, DHT, pub/sub     — up to 2%
  Tier 4 (social):    Trust graph, names               — up to 2%
```

> **Trade-off**
**On constrained links (< 10 kbps)**, the budget adapts automatically:

- Tiers 3–4 switch to **pull-only** (no proactive gossip — only respond to requests)
- Payment batching interval increases from 60 seconds to **5 minutes**
- Capability advertisements limited to **Ring 0 only** (direct neighbors)

| Link type | Routing | Payment | Services | Trust/Social | Total |
|---|---|---|---|---|---|
| 1 kbps LoRa | ~1.5% | ~0.5% | pull-only | pull-only | ~2% |
| 50 kbps LoRa | ~2% | ~2% | ~1% | ~1% | ~6% |
| 10+ Mbps WiFi | ~1% | ~1% | ~2% | ~2% | ~6% |

This tiered model ensures constrained links are never overwhelmed by protocol overhead, while higher-bandwidth links gossip more aggressively for faster convergence.


### Gossip Congestion Handling

When data traffic consumes most of the available bandwidth, gossip must be protected from starvation while not overwhelming the link:

```
Gossip under congestion — enforcement rules:

  Budget enforcement mechanism:
    Each link maintains a gossip token bucket (separate from user data):
      gossip_bucket {
          capacity: link_bandwidth × 0.10 × window_sec,  // 10% of link BW
          tokens: current available,
          refill_rate: link_bandwidth × 0.10 / 8,         // bytes/sec
          min_guaranteed_rate: max(link_bandwidth × 0.02, 10),  // 2% floor, min 10 bytes/sec
      }

  When gossip exceeds 10% budget:
    1. THROTTLE (not drop): Gossip messages are queued in a dedicated
       gossip queue, separate from user data queues.
    2. Priority within gossip queue:
       Tier 1 (routing) > Tier 2 (economic) > Tier 3 (services) > Tier 4 (social)
       Lower-priority gossip is delayed, not dropped.
    3. If gossip queue exceeds 50 messages: lowest-priority messages
       are dropped (Tier 4 first, then Tier 3).
    4. Tier 1 and Tier 2 gossip are NEVER dropped — only delayed.

  Minimum guaranteed gossip rate:
    Even at 100% data utilization, gossip receives a guaranteed minimum:
      min_gossip_rate = max(link_bandwidth × 0.02, 10 bytes/sec)

    At 1 kbps LoRa: min = 2.5 bytes/sec → ~150 bytes/minute
      Enough for: 1 routing announce per minute (sufficient to maintain
      direct-neighbor routes) + 1 settlement per 2 minutes

    At 50 kbps LoRa: min = 125 bytes/sec → ~7.5 KB/minute
      Enough for: full Tier 1 + Tier 2 gossip

    The minimum is enforced by PREEMPTING user data packets:
      If gossip has been starved for > 3 gossip intervals (3 minutes),
      the next packet slot is reserved for gossip regardless of user
      data queue depth. This prevents gossip starvation indefinitely.

  Starvation detection and recovery:
    If a node receives no gossip updates (DHT, discovery, ledger) for
    > 10 gossip intervals (10 minutes):
      1. Node enters GOSSIP_RECOVERY mode
      2. Gossip budget temporarily increased to 20% of link bandwidth
      3. User data throttled to 80% until gossip state converges
      4. Recovery mode exits after 5 consecutive gossip rounds with
         successful delta exchange

  Priority queue scheduling (gossip vs. user data):
    Gossip and user data share the link via weighted fair queuing:
      gossip_weight = 10 (10% of bandwidth)
      user_data_weight = 90 (90% of bandwidth)
    Under congestion, both are served proportionally.
    The min_guaranteed_rate acts as a strict floor — if weighted fair
    queuing would starve gossip below the floor, gossip preempts.
```

## Congestion Control

User data has three layers of congestion control. Protocol gossip is handled separately by the [bandwidth budget](#bandwidth-budget).

### Link-Level Collision Avoidance (CSMA/CA)

On half-duplex links (LoRa, packet radio), mandatory listen-before-talk:

```
LinkTransmit(packet):
  1. CAD scan (LoRa Channel Activity Detection, ~5ms)
  2. If channel busy:
       backoff = random(1, 2^attempt) × slot_time
       slot_time = max_packet_airtime for this link
                   (~200ms at 1 kbps for 500-byte MTU)
  3. Max 7 backoff attempts → drop packet, signal congestion upstream
  4. If channel clear → transmit
```

On full-duplex links (WiFi, Ethernet), the transport handles collision avoidance natively — this layer is a no-op.

### Per-Neighbor Token Bucket

Each outbound link enforces fair sharing across neighbors:

```
LinkBucket {
    link_id: InterfaceID,
    capacity_tokens: u32,        // link_bandwidth_bps × window_sec / 8
    tokens: u32,                 // current available (1 token = 1 byte)
    refill_rate: u32,            // bytes/sec = measured_bandwidth × (1 - protocol_overhead)
    per_neighbor_share: Map<NodeID, u32>,
}

Bandwidth measurement:
  measured_bandwidth = exponential moving average of successfully-transmitted bytes/sec
  Half-life: 60 seconds (adapts within ~3 half-lives = 3 minutes)
  On transport change (e.g., LoRa → WiFi): reset EMA to the new link's nominal rate,
    then converge from there
  refill_rate = measured_bandwidth × 0.90  (10% reserved for protocol overhead)
  per_neighbor_share = refill_rate / num_active_neighbors  (user data only;
    protocol gossip has its own budget per the bandwidth tiers above)
```

Fair share is `refill_rate / num_active_neighbors` by default. Neighbors with active payment channels get share weighted proportionally to channel balance — paying for bandwidth earns proportional priority.

When a neighbor exceeds its share, packets are queued (not dropped). If the queue exceeds a depth threshold, a backpressure signal is sent.

### Priority Queuing

Four priority levels for user data, scheduled with strict priority and starvation prevention (P3 guaranteed at least 10% of user bandwidth):

| Priority | Traffic Type | Examples | Queue Policy |
|----------|-------------|----------|--------------|
| P0 | Real-time | Voice (Codec2), interactive control | Tail-drop at 500ms deadline |
| P1 | Interactive | Messaging, DHT lookups, link establishment | FIFO, 5s max queue time |
| P2 | Standard | Social posts, pub/sub, MHR-Name | FIFO, 30s max queue time |
| P3 | Bulk | Storage replication, large file transfer | FIFO, unbounded patience |

Within a priority level, round-robin across neighbors. On half-duplex links, preemption occurs at packet boundaries only.

### Backpressure Signaling

When an outbound queue exceeds 50% capacity, a 1-hop signal is sent to upstream neighbors:

```
CongestionSignal {
    severity: enum {          // 2 bits
        Moderate,             // reduce sending rate by 25%
        Severe,               // reduce by 50%, reroute P2/P3 traffic
        Saturated,            // stop P2/P3, throttle P1, P0 only
    },
    scope: enum {             // 2 bits
        ThisLink,             // only the link to the signaling neighbor is congested
        AllOutbound,          // all outbound links on this node are congested
    },
    estimated_drain_ms: u16,  // estimated time until queue drains
}
// Byte 0: [severity (2 bits) | scope (2 bits) | reserved (4 bits)]
// Bytes 1-2: estimated_drain_ms (u16, little-endian)
// Total: 3 bytes
```

The signal does not identify which internal interface is congested — upstream peers only need to know whether to reduce traffic through this node (`ThisLink` for targeted rerouting, `AllOutbound` if the node itself is overloaded). Internal link topology is not exposed.

### Dynamic Cost Response

Congestion increases the effective cost of a link. When queue depth exceeds 50%:

```
effective_cost = base_cost × (1 + (queue_depth / queue_capacity)²)
```

The quadratic term ensures gentle increase at moderate load and sharp increase near saturation. The updated cost propagates in the next gossip round's CompactPathCost, causing upstream nodes to naturally reroute traffic to less-congested paths. This is a local decision — no protocol extension beyond normal cost updates.

## Time Model

Mehr does not require global clock synchronization. Time is handled through three mechanisms:

### Logical Clocks

Packet headers carry a **Lamport timestamp** incremented at each hop. Used for ordering events and detecting stale routing entries. If a node receives a routing announcement with a lower logical timestamp than one already in its table for the same destination, the older announcement is discarded.

### Neighbor-Relative Time

During link establishment, nodes exchange their local monotonic clock values. Each node maintains a `clock_offset` per neighbor. Relative time between any two direct neighbors is accurate to within RTT/2.

Used for: agreement expiry, routing entry TTL, payment channel batching intervals.

### Epoch-Relative Time

Epochs define coarse time boundaries. "Weekly" means approximately **10,000 settlement batches after the previous epoch** — not wall-clock weeks. The epoch trigger is settlement count, not elapsed time.

The "30-day grace period" for epoch finalization is defined as **4 epochs after activation**, tolerating clock drift of up to 50% without protocol failure.

All protocol `Timestamp` fields are `u64` values representing milliseconds on the node's local monotonic clock (not wall-clock). Conversion to neighbor-relative or epoch-relative time is performed at the protocol layer.

<!-- faq-start -->

## Frequently Asked Questions

<details className="faq-item">
<summary>How does Mehr addressing work without a central authority?</summary>

Every node generates its own Ed25519 keypair locally and derives a 16-byte destination hash from the public key. This gives each node a self-assigned, pseudonymous address with negligible collision probability across the 2^128 address space. No registrar, DNS, or allocation authority is involved — any device can create an identity and start participating immediately.

</details>

<details className="faq-item">
<summary>Why doesn't the packet header include the sender's address?</summary>

Sender anonymity is structural in Mehr. Packets carry only the destination hash, never the source address. A relay node knows which direct neighbor handed it a packet but cannot determine whether that neighbor originated it or is forwarding it from someone else. This design, inherited from Reticulum, prevents passive traffic analysis from identifying message senders.

</details>

<details className="faq-item">
<summary>How does cost-aware routing scale to large networks?</summary>

Mehr routing is based on the Kleinberg small-world model. Each announce carries a constant 6-byte CompactPathCost summary that relays update in-place as they forward. With O(1) long-range links per node, expected path length is O(log² N) hops. Backbone nodes with O(log N) connections reduce this to O(log N). The scoring function lets applications trade off between cost, latency, and reliability using a per-packet PathPolicy.

</details>

<details className="faq-item">
<summary>Why are per-relay signatures on cost annotations unnecessary?</summary>

Routing decisions are local — you only trust your direct neighbor's cost claim, and that neighbor is already authenticated by link-layer encryption. Trust is transitive at each hop (analogous to BGP trusting direct peers), and the market enforces honesty: relays that inflate costs get routed around, while relays that deflate costs lose money on every forwarded packet. This avoids ~84 bytes of signature overhead per relay hop.

</details>

<details className="faq-item">
<summary>Can a node have multiple identities on the network?</summary>

Yes. A single node can generate multiple Ed25519 keypairs, each producing a separate destination hash. This allows a node to maintain distinct identities for different purposes — personal communication, service endpoints, or anonymous identities. Each identity operates independently with its own routing announcements, reputation, and payment channels.

</details>

<details className="faq-item">
<summary>How does gossip avoid overwhelming low-bandwidth LoRa links?</summary>

The protocol automatically adapts to constrained links (below 10 kbps). Tiers 3–4 (services, trust/social) switch to pull-only mode, payment batching intervals increase from 60 seconds to 5 minutes, and capability advertisements are limited to direct neighbors only. Total protocol overhead on a 1 kbps LoRa link is approximately 2% of bandwidth — well below the 10% ceiling.

</details>

<details className="faq-item">
<summary>What happens if gossip is starved by heavy user data traffic?</summary>

A minimum guaranteed gossip rate is enforced: at least 2% of link bandwidth (or 10 bytes/sec, whichever is greater) is reserved for gossip. If gossip has been starved for more than 3 gossip intervals (3 minutes), the next packet slot is preemptively reserved for gossip. If starvation persists beyond 10 minutes, the node enters GOSSIP_RECOVERY mode, temporarily increasing the gossip budget to 20% until state converges.

</details>

<details className="faq-item">
<summary>How does congestion on one link affect routing across the mesh?</summary>

When a link's queue depth exceeds 50%, the effective cost increases quadratically: `effective_cost = base_cost × (1 + (queue_depth / queue_capacity)²)`. This updated cost propagates in the next gossip round's CompactPathCost, causing upstream nodes to naturally reroute traffic to less-congested paths. Additionally, a 3-byte backpressure signal is sent to direct upstream neighbors, telling them to reduce sending rate.

</details>

<details className="faq-item">
<summary>Does Mehr require synchronized clocks across the network?</summary>

No. Mehr uses three time mechanisms that avoid global clock synchronization: Lamport timestamps (logical clocks) for event ordering, neighbor-relative time (clock offsets exchanged during link establishment) for local agreement expiry, and epoch-relative time where "weekly" epochs are defined by settlement count (~10,000 batches), not wall-clock time. All timestamp fields use each node's local monotonic clock.

</details>

<details className="faq-item">
<summary>How does the priority queuing system prevent voice traffic from being delayed?</summary>

Voice traffic is assigned priority P0 (real-time), the highest level, with a strict 500ms tail-drop deadline. The scheduler uses strict priority ordering — P0 packets are always sent before P1/P2/P3 traffic. To prevent starvation of lower priorities, P3 (bulk) is guaranteed at least 10% of user bandwidth. On half-duplex links, preemption occurs only at packet boundaries.

</details>

<!-- faq-end -->

---

### Security
<!-- Source: docs/protocol/security.md -->

# Layer 2: Security

Security in Mehr is structural, not bolted on. Every layer of the protocol incorporates cryptographic protections. There is no trusted infrastructure — no certificate authorities, no trusted servers. DNS is used only for initial [genesis gateway discovery](../economics/token-economics#genesis-gateway-discovery), not for protocol operation.

## Threat Model

> **Threat**
Mehr assumes the worst:

- **Open network**: Any node can join. Nodes may be malicious.
- **Hostile observers**: All link-layer traffic may be monitored (especially radio).
- **No trusted infrastructure**: No certificate authorities, no trusted servers. DNS is used only for initial genesis gateway discovery, not for protocol operation.
- **State-level adversaries**: Governments may control internet gateways and operate nodes within the mesh.

Mehr does **not** attempt to defend against:

- **Global traffic analysis**: A sufficiently powerful adversary monitoring all links simultaneously can correlate traffic patterns. [Opt-in onion routing](#onion-routing-opt-in) mitigates this for individual packets but does not defeat a global adversary.
- **Physical compromise**: If an adversary physically captures a node, they obtain its private key and all local state.


## Encryption Model

### Link-Layer Encryption (Hop-by-Hop)

Every link between two nodes is encrypted using a session key derived from X25519 Diffie-Hellman key exchange:

```
Link establishment:
  1. Alice and Bob exchange X25519 ephemeral public keys
  2. Both derive shared_secret = X25519(my_ephemeral, their_ephemeral)
  3. session_key = Blake2b(shared_secret || alice_pub || bob_pub)
  4. All traffic on this link encrypted with ChaCha20-Poly1305(session_key)
     Nonce: 64-bit counter (zero-padded to 96 bits), incremented per packet
     Counter is per session_key — reset to 0 on each key rotation
     No nonce reuse risk: key rotation occurs well before 2^64 packets
  5. Keys rotated periodically (every 1 hour of local monotonic time, or
     max(1 MB, bandwidth_bps × 60s) of data, whichever first — this scales
     the data threshold to ~1 minute of link capacity, preventing excessive
     rotation on fast links). "1 hour" is measured by each node's local
     monotonic clock independently — no synchronization needed. Either side
     of the link can initiate rotation; the peer accepts and derives a new
     session key via fresh ephemeral key exchange
```

This prevents passive observers from reading packet contents or metadata beyond the cleartext header fields needed for routing.

### End-to-End Encryption (Data Payloads)

Data packets are encrypted by the sender for the destination using the destination's public key. Relay nodes **cannot** read the payload:

```
E2E encryption for a message to Bob:
  1. Alice generates ephemeral X25519 keypair
  2. shared_secret = X25519(alice_ephemeral, bob_x25519_public)
  3. payload_key = Blake2b(shared_secret || alice_ephemeral_pub)
  4. encrypted_payload = ChaCha20-Poly1305(payload_key, plaintext)
  5. Packet contains: alice_ephemeral_pub || encrypted_payload
  6. Bob derives the same payload_key and decrypts
```

This provides **forward secrecy per message** — each message uses a unique ephemeral key. Compromise of one message's key does not compromise any other message.

### What Relay Nodes Can See

| Visible | Hidden |
|---------|--------|
| Destination hash | Source address |
| Hop count | Payload contents |
| Packet size | Application-layer data |
| Timing | Sender identity |

## Authentication

Node identity is **self-certifying**. A node proves it owns a destination hash by signing with the corresponding Ed25519 private key. No certificates, no PKI, no trust hierarchy.

- **Payment channels**: Both parties sign every state update. Forgery requires the other party's private key.
- **Capability agreements**: Both provider and consumer sign. Neither can forge the other's commitment.
- **Announcements**: Path announcements are signed by the announcing node. Relay nodes update the [CompactPathCost](network-protocol#mehr-extension-compact-path-cost) running totals (not individually signed — link-layer authentication at each hop is sufficient). Malicious relays can lie about costs, but the economic model disincentivizes this — overpriced nodes are routed around, underpriced nodes lose money.

## Privacy

### Sender Anonymity

Packets do not carry source addresses. A relay node knows which neighbor sent it a packet, but not whether that neighbor originated the packet or is relaying it from someone else.

### Recipient Privacy

Destination hashes are pseudonymous. A hash is not linked to a real-world identity unless the user chooses to publish that association (e.g., via MHR-Name).

### Traffic Analysis Resistance

Basic protections (always active):
- Link-layer encryption prevents content inspection
- Variable-rate padding on LoRa links obscures traffic patterns
- No source address in packet headers

### Onion Routing (Opt-In)

For high-threat environments where an adversary monitors multiple links simultaneously, per-packet layered encryption is available as an opt-in privacy upgrade via `PathPolicy.ONION_ROUTE`:

```
Onion-routed packet (3-hop default):
  1. Sender selects 3 intermediate relays (at least 1 outside trust neighborhood)
  2. Wraps message in 3 encryption layers (outermost = first relay)
  3. Each layer: 16-byte nonce + 16-byte Poly1305 tag = 32 bytes overhead
  4. Each relay decrypts one layer, reads next-hop destination, forwards
  5. Final relay decrypts innermost layer and delivers to destination

Overhead: 32 bytes × 3 hops = 96 bytes
Usable payload on LoRa (465 max): 369 bytes (~79% efficiency)
```

Key properties:
- **Stateless**: No circuit establishment, no relay-side state. Each packet is independently routable
- **Opt-in**: Enabled per-packet via `PathPolicy.ONION_ROUTE` with configurable hop count (default 3)
- **Cover traffic**: Optional constant-rate dummy packets (1/minute, off by default) for timing analysis resistance on high-threat links
- **Not for voice**: The payload overhead and additional latency make onion routing unsuitable for real-time voice. Recommended for text messaging in high-threat scenarios

### Key Rotation

- **Long-lived keys** (Ed25519 identity): Used only for signing, never for encryption
- **Ephemeral keys**: Used for encryption, discarded after use
- Compromise of an ephemeral key does not compromise past or future communications

## Sybil Resistance

> **Threat**
An attacker can generate unlimited identities (Sybil attack). Mehr mitigates this through economic mechanisms rather than identity verification:

1. **Payment channel deposits**: Opening a channel requires visible balance. Sybil nodes with no balance cannot participate in the economy.
2. **Reputation accumulation**: Reputation is earned through verified service delivery over time. New identities start with zero reputation. Creating many identities dilutes rather than concentrates reputation.
3. **Trust graph**: A Sybil attacker needs real social relationships to gain trust. Trusted peers [vouch economically](../economics/trust-neighborhoods#trust-based-credit) — they absorb the debts of nodes they trust, making trust costly to extend.
4. **Proof of service (demand-backed)**: Stochastic relay rewards use a [VRF-based lottery](../economics/payment-channels#how-stochastic-rewards-work) that produces exactly one verifiable outcome per (relay, packet) pair — preventing grinding. However, VRF alone does not prevent traffic fabrication between colluding nodes. The actual Sybil defense is [demand-backed minting](../economics/payment-channels#demand-backed-minting-eligibility): VRF wins only count for minting if the packet traversed a funded payment channel, and [revenue-capped minting](../economics/payment-channels#revenue-capped-minting) ensures self-dealing is always unprofitable (spending Y MHR on fake traffic yields at most 0.5Y in minting).
5. **Transitive credit limits**: Even if a Sybil node gains one trust relationship, transitive credit is capped at 10% per hop and rate-limited for new relationships.


## Reputation

Reputation is a **locally computed, per-neighbor score** — not a global value. There is no network-wide reputation database. Each node maintains its own view of how reliable its peers are.

### Reputation State

```
PeerReputation {
    node_id: NodeID,
    relay_score: u16,       // 0-10000 (fixed-point, 2 decimal places)
    storage_score: u16,     // 0-10000
    compute_score: u16,     // 0-10000
    total_interactions: u32, // number of completed agreements
    failed_interactions: u32,// number of failed/disputed agreements
    first_seen_epoch: u64,  // how long we've known this peer
    last_updated: Timestamp,
}
```

### How Reputation Is Earned

Each completed capability agreement adjusts the relevant score:

```
On agreement completion:
  if successful:
    score += (10000 - score) / 100   // diminishing returns — harder to gain at higher scores
  if failed:
    score -= score / 10              // 10% penalty per failure — fast to lose

Initialization:
  - New peer (no interactions, no referral): score = 0 (unknown)
  - New peer with trusted referral: score = min(5000, referrer_score × 0.3)
  - Referral → first-hand transition: after 5 successful interactions,
    first-hand score fully replaces the referral score
  - Referral expiry: 500 gossip rounds (~8 hours) without refresh
```

### How Reputation Is Used

- **Credit line sizing**: Nodes extend larger credit lines to higher-reputation peers
- **Capability selection**: When multiple providers offer the same capability, the consumer considers reputation alongside cost and latency
- **Storage agreement duration**: Nodes with higher storage scores get offered longer storage contracts
- **Epoch consensus**: Epoch proposals from higher-reputation nodes are preferred when competing proposals conflict

### Properties

- **Local only**: Each node computes its own reputation scores. No gossip of reputation values — this prevents reputation manipulation by flooding the network with fake endorsements
- **First-hand primary**: Scores are based primarily on direct interactions. First-hand experience always takes precedence over third-party information
- **No global score**: There is no way to ask "what is node X's reputation?" There is only "what is my experience with node X?" This makes reputation Sybil-resistant — an attacker can't inflate a score without actually providing good service to the scoring node

### Trust-Weighted Referrals

When a node has no direct experience with a peer, it can query trusted neighbors for their first-hand scores. Referrals help new nodes bootstrap but are tightly bounded to limit manipulation:

- **1-hop only**: Only direct trusted peers can provide referrals. No transitive gossip — a referral from a friend-of-a-friend is not accepted. This limits the manipulation surface to corruption of your direct trusted peers
- **Weight formula**: `referral_weight = trust_score_of_referrer / max_score × 0.3` — even a maximally trusted referrer's opinion carries only 30% of direct experience weight
- **Capped at 50%**: A referred reputation score cannot exceed 5000 (50% of max). A referral alone cannot make a peer fully trusted — direct interaction is required to reach higher scores
- **Overwritten by experience**: Referral scores are advisory. After the first few direct interactions, first-hand experience overwrites the referral entirely
- **Expiry**: Referral scores expire after 500 gossip rounds (~8 hours at 60-second intervals) without refresh from the referrer
- **Anti-collusion**: Since only 1-hop referrals are accepted and each is capped, a colluding cluster must corrupt your direct trusted peers to manipulate scores — which already breaks the trust model regardless of reputation

## Key Management

| Operation | Method |
|-----------|--------|
| **Generation** | Ed25519 keypair from cryptographically secure random source on first boot |
| **Storage** | Private key encrypted at rest (ChaCha20-Poly1305 with user passphrase, or hardware secure element) |
| **Recovery** | Social recovery via Shamir's Secret Sharing — split key into N shares, recover with K-of-N |
| **Revocation** | No global revocation mechanism (intentional — no infrastructure that can be coerced into revoking keys) |

The absence of a revocation mechanism is a deliberate tradeoff. A user who loses their key loses their identity and balance, but no authority can forcibly revoke anyone's identity.

### Key Compromise Advisory

While there is no revocation, a node that detects its key has been compromised can broadcast an advisory:

```
KeyCompromiseAdvisory {
    compromised_key: Ed25519PublicKey,
    new_key: Ed25519PublicKey,          // optional migration target
    sequence: u64,                      // monotonic counter — prevents replay of old advisories
    evidence: enum {
        SignedByBoth(sig_old, sig_new), // proves control of both keys
        SignedByOldOnly(sig_old),       // can only prove old key ownership
    },
    timestamp: u64,
}
```

The `sequence` field is monotonically increasing per compromised key. Receiving nodes only accept an advisory if its sequence is strictly greater than any previously seen advisory for the same `compromised_key`. This prevents an attacker from replaying an old advisory to override a newer one.

This is **advisory, not authoritative**. Receiving nodes may:
- Flag the old identity as potentially compromised
- Require re-authentication for high-value operations
- Accept the new key if evidence includes both signatures (strongest proof)

**Conflict resolution**: An attacker holding the stolen key could issue a counter-advisory claiming the legitimate owner's *new* key is compromised. Receiving nodes resolve conflicting advisories as follows:

1. **`SignedByBoth` always wins over `SignedByOldOnly`** — proving control of both keys is strictly stronger evidence than proving control of only one
2. **Multiple `SignedByOldOnly` advisories cancel out** — if two different advisories both signed only by the old key claim different new keys, both are suspect. Receiving nodes flag the old identity as compromised but accept neither new key automatically.
3. **Trust-weighted resolution** — if the advisory is vouched for by trusted peers (who can attest to knowing the real owner), it is weighted more heavily

This does not prevent an attacker from continuing to use the stolen key. It provides a mechanism for the legitimate owner to signal compromise and begin migration — strictly better than no mechanism at all. The `SignedByBoth` evidence type is the only reliable migration path; users should generate their new keypair **before** the old one is exposed whenever possible.

## Cryptographic Primitives Summary

| Purpose | Algorithm | Key Size |
|---------|-----------|----------|
| Identity / Signing | Ed25519 | 256-bit (32-byte public key) |
| Key Exchange | X25519 (Curve25519 DH) | 256-bit |
| Identity Hashing | Blake2b | 256-bit |
| Content Hashing | Blake3 | 256-bit |
| Symmetric Encryption | ChaCha20-Poly1305 | 256-bit key, 96-bit nonce |
| Address Derivation | Blake2b truncated | 128-bit (16-byte destination hash) |
| Relay Lottery (VRF) | ECVRF-ED25519-SHA512-TAI ([RFC 9381](https://www.rfc-editor.org/rfc/rfc9381)) | Reuses Ed25519 keypair; 80-byte proof |

### Hash Algorithm Split

Mehr uses two hash algorithms for distinct purposes:

- **Blake2b** — Identity derivation and key derivation. Chosen for compatibility with the Ed25519/X25519 ecosystem and its proven security margin. Used in: `destination_hash`, `session_key` derivation.
- **Blake3** — Content addressing and general hashing. Chosen for speed (3x faster than Blake2b on general data) and built-in Merkle tree support for streaming verification. Used in: `DataObject` hash, contract hash, DHT keys, `ChannelState` hash.

Both produce 256-bit outputs. The protocol never mixes them — identity operations use Blake2b, data operations use Blake3.

### Ed25519 to X25519 Conversion

X25519 public keys are derived from Ed25519 public keys using the birational map defined in **RFC 7748 Section 4.1**: the Ed25519 public key (a compressed Edwards point) is converted to Montgomery form by computing `u = (1 + y) / (1 - y) mod p`, where `y` is the Edwards y-coordinate and `p = 2^255 - 19`. This is a standard, well-analyzed transformation used by libsodium, OpenSSL, and other major cryptographic libraries.

<!-- faq-start -->

## Frequently Asked Questions

<details className="faq-item">
<summary>What happens if a node's private key is physically compromised?</summary>

If an adversary physically captures a node, they obtain its private key and all local state. Mehr does not attempt to defend against this. However, the compromised node can broadcast a KeyCompromiseAdvisory signed by both the old and new keys, enabling migration to a new identity. Forward secrecy ensures that past messages (encrypted with unique ephemeral keys) remain secure even after key compromise.

</details>

<details className="faq-item">
<summary>How does Mehr defend against Sybil attacks without proof-of-work or staking?</summary>

Mehr uses economic mechanisms instead of identity verification. Payment channel deposits require visible balance, reputation is earned through verified service delivery over time, trusted peers vouch economically (absorbing debts of nodes they trust), and VRF-based relay rewards are demand-backed — only funded payment channel traffic earns minting. Revenue-capped minting ensures that spending Y MHR on fake traffic yields at most 0.5Y in minting, making self-dealing structurally unprofitable.

</details>

<details className="faq-item">
<summary>Does Mehr provide forward secrecy for messages?</summary>

Yes. Every end-to-end message uses a unique ephemeral X25519 keypair generated by the sender. The shared secret is derived from this ephemeral key and the recipient's public key, then used for ChaCha20-Poly1305 encryption. Since each message has its own ephemeral key that is discarded after use, compromising one message's key reveals nothing about any other message — true per-message forward secrecy.

</details>

<details className="faq-item">
<summary>Can relay nodes see message contents or sender identity?</summary>

No. Data payloads are end-to-end encrypted — relay nodes cannot read them. Additionally, packets carry only the destination hash, never the source address. A relay knows which direct neighbor forwarded a packet but cannot determine if that neighbor originated it or is relaying from someone else. For high-threat scenarios, optional onion routing adds layered encryption across 3 hops.

</details>

<details className="faq-item">
<summary>Why is there no global key revocation mechanism?</summary>

This is a deliberate tradeoff. A global revocation mechanism requires infrastructure that can be coerced — a certificate authority or revocation list that governments could force to revoke identities. Instead, Mehr provides a KeyCompromiseAdvisory system that is advisory, not authoritative. A user who loses their key loses their identity and balance, but no external authority can forcibly revoke anyone's identity.

</details>

<!-- faq-end -->

---

### Protocol Versioning
<!-- Source: docs/protocol/versioning.md -->

# Protocol Versioning

Mehr has no central authority to push upgrades. Nodes can be offline for months. ESP32 nodes may never be updated. Partitioned networks may run different versions simultaneously. The versioning strategy must work under all these conditions.

## Version Field

Every Mehr announce carries a version byte in the [MehrExtension](network-protocol#mehr-extension-compact-path-cost):

```
MehrExtension:
  [MAGIC 0x4E] [VERSION 1B] [CompactPathCost 6B] [TLV extensions...]

VERSION encoding:
  Bits 7-4: major version (0-15)
  Bits 3-0: minor version (0-15)

  Current: 0x10 = major 1, minor 0 (v1.0)

Reserved value:
  Major = 15 (0xF_): EXTENDED VERSION
    When major = 15, the actual version is read from a TLV extension
    in the MehrExtension payload:
      Type: 0x01 (ExtendedVersion)
      Length: 4 bytes
      Data: [major u16 LE] [minor u16 LE]

    This allows up to 65,535 major versions — sufficient for any
    foreseeable protocol lifetime. Old nodes that don't understand
    extended versioning see major=15 and treat it as "unknown future
    version" (the correct behavior for any unrecognized major).
```

Nodes advertise their version in every announce. A node can see what versions its neighbors run without any explicit version query. The extended version escape hatch ensures the 4-bit version field never becomes a hard limit — major 15 bootstraps into an arbitrarily large version space via TLV.

## Change Categories

Not all changes are equal. Mehr classifies protocol changes into three categories based on their impact on the network:

### Soft Extensions (Minor Version Bump)

New features that old nodes can safely ignore:

```
Examples:
  - New TLV extension types in MehrExtension
  - New optional fields in DataObject (old nodes skip unknown fields)
  - New capability bits (bits 8-15 in presence beacon, currently reserved)
  - New scope types beyond Geo and Topic
  - New delivery modes in MHR-Pub
  - New claim types in IdentityClaim

Old node behavior:
  - Ignores unknown TLV types, unknown capability bits, unknown fields
  - Continues operating normally
  - Can still communicate with new nodes
  - No action required by operator
```

Soft extensions are the primary upgrade mechanism. The protocol is designed with reserved fields, TLV extensibility, and optional fields specifically to enable this.

### Hard Changes (Major Version Bump)

Incompatible changes that require nodes to understand the new format:

```
Examples:
  - Changing wire format of existing fields (e.g., CompactPathCost layout)
  - Changing cryptographic primitives (e.g., Ed25519 → post-quantum)
  - Changing CRDT merge semantics
  - Changing VRF algorithm for relay lottery
  - Changing settlement record format

Network impact:
  - Old nodes cannot process new-format messages
  - New nodes must handle both old and new formats during transition
  - Requires coordinated transition period
```

Hard changes should be extremely rare. The protocol is designed to avoid them through extensibility.

### Cryptographic Migration (Special Case)

Cryptographic algorithm replacement is the most likely hard change and gets its own process:

```
Crypto migration timeline:

  Phase 1: DUAL SUPPORT (soft extension)
    New nodes support both old and new algorithms.
    New nodes advertise new algorithm support via TLV extension.
    All communication uses old algorithms (backward compatible).
    Duration: until >80% of active nodes support new algorithms.

  Phase 2: PREFER NEW (soft extension)
    New-capable nodes prefer new algorithms for new channels/agreements.
    Old nodes still work — they just use old algorithms.
    KeyRotation claims allow nodes to migrate identity keys.
    Duration: until <5% of active traffic uses old algorithms.

  Phase 3: DEPRECATE OLD (major version bump)
    New nodes stop accepting old algorithm traffic.
    Old nodes are effectively partitioned (can still talk to each other).
    Operators must upgrade to rejoin the main network.
    Duration: permanent.
```

The [KeyRotation](../services/mhr-id) identity claim is designed specifically for this: a node signs a statement with its old key saying "my new key is X", then the network recognizes the new key. This works even if the old algorithm is weakened — as long as the rotation happens before the old key is compromised.

## Transition Mechanics

### How Nodes Learn About New Versions

```
Version discovery:

  1. PASSIVE: Node sees announces with higher version numbers
     from neighbors. No action needed — just awareness.

  2. GOSSIP: Epoch proposals include a version histogram
     (count of nodes per version in the active set).
     Every node knows the version distribution.

  3. LOCAL DECISION: The node operator decides when to upgrade.
     The protocol never forces an upgrade.
```

### Compatibility Rules

```
Compatibility matrix:

  Same major, any minor:
    ✓ Full interoperability
    Old nodes ignore new extensions
    New nodes include backward-compatible fallbacks

  Different major, during transition:
    ✓ Partial interoperability via bridge behavior
    New nodes accept both old and new format
    Old nodes work within their version cluster

  Different major, after transition:
    ✗ No interoperability
    Old nodes form a separate network
    Upgrade required to rejoin
```

### Partition-Safe Upgrades

Version transitions must be partition-safe. A partition that misses the transition window must still be able to rejoin:

```
Partition rejoining after version transition:

  Scenario: Partition P was isolated during v1 → v2 transition.
  P runs v1. Main network runs v2.

  On reconnection:
    1. P's nodes see v2 announces from main network.
    2. P's nodes cannot process v2-only traffic.
    3. P's v1 traffic is still accepted by main network
       (during transition period: main nodes accept both).
    4. P's operators upgrade to v2.
    5. P's CRDT ledger merges normally (GCounter/GSet merge
       is version-independent — balances are just numbers).
    6. P fully rejoins.

  If P reconnects AFTER the transition period:
    1. Main network no longer accepts v1 traffic.
    2. P's nodes are effectively still partitioned.
    3. P's operators must upgrade before traffic can flow.
    4. CRDT merge still works once P upgrades — no balance loss.
```

## What Doesn't Need Versioning

Several aspects of Mehr are upgradeable without any version mechanism:

| Component | Why No Version Needed |
|-----------|----------------------|
| **Service pricing** | Market-set per node — just change your config |
| **Trust policies** | Local per node — change trusted_peers, credit limits |
| **Content filtering** | Local per node — change relay/storage policies |
| **Application layer** | PostEnvelope, SocialPost, CuratedFeed are DataObjects — new formats are just new DataObject types |
| **Scope naming** | Free-form strings — communities evolve naming conventions socially |
| **Capability types** | New capabilities advertised via reserved beacon bits or TLV extensions |

The layered architecture means most changes happen above the protocol layer and don't require protocol versioning at all.

## Post-Quantum Cryptography

The most likely cryptographic migration is from Ed25519/X25519 to post-quantum algorithms. Current timeline estimates suggest this may be needed within 10-20 years.

```
PQC migration plan:

  Current primitives:
    Identity/Signing:  Ed25519 (32-byte public key)
    Key Exchange:      X25519 (32 bytes)
    VRF:               ECVRF-ED25519-SHA512-TAI

  Candidate replacements:
    Identity/Signing:  ML-DSA (FIPS 204) or SLH-DSA (FIPS 205)
    Key Exchange:      ML-KEM (FIPS 203)
    VRF:               Post-quantum VRF (research area — no standard yet)

  Constraints:
    ML-DSA-44 public key:  1,312 bytes (vs 32 for Ed25519)
    ML-DSA-44 signature:   2,420 bytes (vs 64 for Ed25519)
    ML-KEM-512 ciphertext: 768 bytes (vs 32 for X25519)

    These sizes are challenging for LoRa (484-byte max packet).
    Solutions:
      - Fragment announces across multiple packets
      - Use hash-based short identifiers on LoRa, full keys on WiFi+
      - Hybrid schemes (Ed25519 + PQC) during transition
```

### Post-Quantum VRF Strategy

Post-quantum VRF is an active research area with no production standard. Mehr's strategy is a **two-track approach**: adopt a PQ VRF candidate when one matures, with a hash-based fallback ready if needed.

```
PQ VRF candidates (ranked by viability):

  1. Lattice-based VRF (XVRF, based on Module-LWE)
     Pros: Compact proofs (~1.5 KB), fast verification
     Cons: No NIST standard yet, ongoing cryptanalysis
     Timeline: Likely standardized by 2030-2035

  2. Hash-based VRF (using SLH-DSA / SPHINCS+ signatures)
     Pros: Conservative security assumptions, NIST-standardized base
     Cons: Large proofs (~8-17 KB), requires multi-packet fragmentation
     Timeline: Constructible today from existing standards

  3. Isogeny-based VRF
     Pros: Small key/proof sizes
     Cons: SIDH broken in 2022; remaining schemes immature
     Timeline: Uncertain

Fallback: Hash-chain lottery (no VRF)
  If no PQ VRF standard emerges before quantum threat materializes:

    HashChainLottery {
        relay_id: NodeID,
        packet_hash: Blake3Hash,
        epoch_hash: Blake3Hash,
        chain_link: Blake3(relay_secret || packet_hash || epoch_hash),
        // Deterministic per (relay, packet, epoch) — no grinding
        // relay_secret committed at epoch start via hash(relay_secret)
        // Revealed per-epoch, verified by checking commitment
    }

    Commitment protocol:
      1. At epoch start: relay publishes commitment = Blake3(relay_secret)
      2. Per packet: chain_link = Blake3(relay_secret || packet_hash || epoch_hash)
      3. Win check: chain_link < difficulty_target
      4. At epoch end: relay reveals relay_secret for verification
      5. All wins verified retroactively against the commitment

    Properties:
      - No wasted work (single hash per packet, not proof-of-work)
      - Deterministic (relay cannot grind — secret is committed)
      - Retroactive verification (cheating detected at epoch boundary)
      - Penalty for non-reveal: forfeit all epoch earnings

  This is NOT proof-of-work — it's a committed hash lottery.
  Expected compute: 1 Blake3 hash per packet (~1 μs). No grinding.
```

**Migration path**: The VRF algorithm is negotiated per-link via the [cryptographic migration](#cryptographic-migration-special-case) three-phase process. During Phase 1 (dual support), both the current Ed25519 VRF and the new PQ VRF are accepted. During Phase 2, new channels prefer PQ VRF. Phase 3 deprecates the old VRF. Because VRF verification only involves the two channel parties (relay and sender), migration is per-channel, not network-wide — far simpler than migrating identity keys.

## Protocol Longevity

Can the protocol run for 1000 years without hitting a numerical wall? Every field in the protocol has been audited for overflow at millennial timescales.

### Numerical Overflow Audit

Assumptions: 1 epoch per 10 minutes, 52,560 epochs/year, 1000 years = 52.56 million epochs.

| Field | Type | Capacity | 1000-Year Usage | Headroom |
|-------|------|----------|-----------------|----------|
| `epoch_number` | u64 | 1.84 × 10^19 | 5.26 × 10^7 | Safe for 3.5 × 10^11 years |
| Timestamps | u64 | 5.8 × 10^11 years | 1000 years | Safe for 10^11 years |
| MHR supply ceiling | u64 | 1.84 × 10^19 μMHR | Asymptotic by design | Never reached |
| Channel `sequence` | u64 | 1.84 × 10^19 | ~5.3 × 10^7 per channel (if never closed) | Safe |
| `active_set_size` | u32 | 4.3 × 10^9 | Up to millions | Safe |
| `win_count` | u32 | 4.3 × 10^9 | Per-epoch, resets | Safe |
| Version byte | 4-bit major | 16 versions | ~1 per 50 years = 20 | **Overflow at year ~800** |
| GCounters | u64 | 1.84 × 10^19 | Grows with money velocity | **Theoretical overflow** |
| Emission bit-shift | u64 >> N | N must be 0–63 | At epoch 6.4M (year ~1218): N = 64 | **Undefined behavior** |

### Mitigations

**Version byte (solved)**: Major version 15 is reserved as an escape hatch into [extended versioning](#version-field). When major = 15, the actual version is read from a TLV extension as a u16 pair — supporting 65,535 major versions. At one major version per 50 years, this covers 3.2 million years.

**GCounters (solved)**: [Epoch compaction](../economics/epoch-compaction#gcounter-rebase) rebases GCounters to net balance at each epoch. Instead of tracking cumulative lifetime earnings (which grows with money velocity), the snapshot stores only current balance. Counters never exceed current circulating supply — the protocol runs indefinitely.

**Emission bit-shift (solved)**: The halving formula `10^12 >> (e / 100_000)` overflows when the shift operand reaches 64 at epoch 6.4 million (~year 1218). Implementations must [clamp the shift to 63](../economics/mhr-token#supply-model). At shift = 63, the halved reward is 0, so the tail emission floor takes over — which is the correct behavior.

### What Fundamentally Cannot Overflow

The economic model — free between trusted peers, paid between strangers — doesn't depend on any numerical field. Even if every counter were reset to zero, the trust graph and bilateral relationships would reconstruct the economy. Numbers track accounting; the trust graph *is* the network.

## Governance

Mehr has no central authority for protocol changes. Governance uses a **Mehr Enhancement Proposal (MEP)** process with trust-weighted version signaling — lightweight enough to avoid creating a political attack surface, structured enough to coordinate upgrades across a decentralized mesh.

### Mehr Enhancement Proposals (MEPs)

Any node operator can propose a protocol change by publishing a MEP as a DataObject in MHR-Store:

```
MEP {
    mep_number: u32,                    // sequential, claimed by publisher
    title: String,                       // short description
    author: NodeID,                      // proposer's identity
    status: enum {
        Draft,                           // under discussion
        Proposed,                        // ready for signaling
        Accepted,                        // ≥67% signal weight in target scope
        Implemented,                     // reference implementation available
        Active,                          // ≥80% of active nodes running it
        Rejected,                        // failed to reach acceptance threshold
        Withdrawn,                       // author withdrew
    },
    category: enum {
        SoftExtension,                   // minor version bump
        HardChange,                      // major version bump
        CryptoMigration,                 // cryptographic algorithm change
        Process,                         // governance process change
    },
    target_version: (u16, u16),          // proposed (major, minor)
    spec_hash: Blake3Hash,               // hash of full specification document
    reference_impl_hash: Option<Blake3Hash>, // hash of reference implementation
    created: u64,                        // epoch
    signature: Ed25519Signature,
}
```

MEPs are registered via MHR-Name under `topic:mehr/meps` and propagate through normal gossip.

### Trust-Weighted Version Signaling

Nodes signal support for MEPs via a TLV extension in their announces:

```
VersionSignal TLV (type 0x03):
    mep_count: u8,                       // number of MEPs signaled (max 8)
    signals: [{
        mep_number: u32,                 // which MEP
        support: enum { Support, Oppose, Neutral },  // 1 byte
    }],
```

The epoch proposer aggregates signals into a **version histogram** included in epoch proposals:

```
Version histogram (per epoch):
    For each signaled MEP:
        support_weight  = Σ trust_flow_weight(node) for all Supporting nodes
        oppose_weight   = Σ trust_flow_weight(node) for all Opposing nodes
        neutral_weight  = Σ trust_flow_weight(node) for all Neutral nodes
        total_weight    = support_weight + oppose_weight + neutral_weight

    Acceptance threshold: support_weight / total_weight ≥ 0.67
    Rejection threshold:  oppose_weight / total_weight ≥ 0.34
```

Signaling uses trust flow weight (from the [voting](../applications/voting) TrustFlow algorithm), not node count. This prevents Sybil manipulation of governance — 50 fake nodes with 2 inbound trust edges get ~1.7 total signal weight, not 50.

### Cross-Fork Compatibility

When different communities run different versions:

```
Cross-fork interaction rules:

    Same major version, different minor:
        Full interoperability. Old nodes ignore new TLV extensions.

    Different major versions:
        Gateway nodes that support both versions act as bridges.
        Bridge behavior:
            1. Accept packets in either format
            2. Translate between formats where possible
            3. For untranslatable features: drop gracefully
        Bridge nodes advertise multi-version support via capability bits.

    Permanent fragmentation prevention:
        A fork that loses >90% of trust-weighted signal is considered abandoned.
        MEPs include a sunset_epoch — if acceptance threshold is not reached
        within 50,000 epochs (~1 year), the MEP automatically moves to Rejected.
        This prevents indefinite version limbo.
```

### Why This Works

- **No political attack surface**: No elected committee, no foundation, no voting token. Signal weight comes from the same trust graph used for everything else.
- **Fork-friendly**: Communities can disagree and fork. Gateway bridges maintain connectivity. The trust graph, not the protocol version, defines the real network.
- **Partition-safe**: Version signaling is carried in announces and aggregated in epochs — both are partition-tolerant mechanisms.
- **Lightweight**: The entire governance mechanism is a TLV extension (≤41 bytes) and a DataObject (MEP). No new protocol primitives.

## Blockchain Upgrade Strategies — Comparison

Decentralized protocol upgrades are a solved problem in several ecosystems. Each approach trades off decentralization, speed, and safety. Mehr's design borrows elements from several while avoiding their failure modes.

### Bitcoin: Soft Forks via Miner Signaling

Bitcoin upgrades through backward-compatible soft forks. Miners signal readiness in block headers (BIP-9). Activation requires ≥95% hashrate signaling over a 2-week retarget period (later lowered to 90% via Speedy Trial). Non-signaling miners' blocks remain valid but don't count toward threshold.

**Strengths**: Extremely conservative — nearly unanimous consensus required. Non-upgraded nodes continue operating (soft forks tighten rules, don't break old clients).

**Weaknesses**: Activation can take years (SegWit: 2+ years of political deadlock). Hashrate ≠ user consensus. Miners can block upgrades for strategic reasons (SegWit2x controversy). Hard forks are essentially impossible in Bitcoin's culture — any incompatible change creates a permanent chain split (Bitcoin Cash, Bitcoin SV).

**Mehr equivalent**: MEP trust-weighted signaling at 67% threshold. Like BIP-9 but weighted by trust flow instead of hashrate — preventing both Sybil manipulation and miner-cartel vetoes.

### Ethereum: EIPs with Hard Fork Cadence

Ethereum upgrades via Ethereum Improvement Proposals (EIPs) bundled into scheduled hard forks. Core developers select EIPs through a rough-consensus process (All Core Devs calls). Upgrades are mandatory — all nodes must update by a specific block number. Nodes that don't upgrade are stranded on the old chain.

**Strengths**: Fast iteration (multiple hard forks per year). Community can make breaking changes when needed (EIP-1559 fee market, The Merge PoW→PoS).

**Weaknesses**: Relies on benevolent core developer social consensus — no formal on-chain governance. "Update by block N or get left behind" is coercive. Hard to participate in governance without deep technical engagement. Contentious forks split the community (Ethereum Classic).

**Mehr equivalent**: MEP process and trust-weighted signaling provide formal on-chain governance without a core developer committee. Mehr's transition periods (Phase 1→2→3 for crypto migrations) give partitioned nodes time to rejoin, unlike Ethereum's hard cutover.

### Cosmos SDK: Module-Based Upgrades

Cosmos chains use the SDK's upgrade module. A governance proposal specifies an upgrade height. Validators vote on-chain (token-weighted). If the proposal passes, all validators halt at the upgrade height, swap binaries, and restart. The `x/upgrade` module coordinates this.

**Strengths**: Clean on-chain governance. Validators explicitly vote. Upgrade coordination is built into the protocol.

**Weaknesses**: Requires synchronized halt — the chain stops producing blocks during the upgrade window. Token-weighted voting means large stakers dominate governance. IBC (Inter-Blockchain Communication) adds cross-chain complexity.

**Mehr equivalent**: MEP signaling is continuous (no halt required). Trust-weighted voting prevents whale dominance. Mehr nodes upgrade independently — no coordinated restart needed. The network continues operating during transitions.

### Substrate (Polkadot): Forkless Runtime Upgrades

Substrate stores the runtime (state transition function) as WASM bytecode on-chain. A governance proposal can swap the runtime without any node software update. Nodes automatically execute the new runtime from the next block. No binary changes, no hard fork, no restart.

**Strengths**: Truly forkless upgrades. Fastest upgrade path of any blockchain. No node coordination required.

**Weaknesses**: Only works for runtime logic — consensus/networking changes still need traditional forks. The on-chain WASM runtime adds complexity and overhead. Governance (OpenGov) is sophisticated but complex — conviction voting, referenda tracks, delegated voting.

**Mehr equivalent**: Mehr's TLV extensibility enables soft extensions without version bumps (similar to adding new runtime modules). Hard changes still require the version transition process. Mehr deliberately avoids on-chain runtime storage — constrained ESP32 nodes cannot execute WASM runtimes.

### Summary

| | Bitcoin | Ethereum | Cosmos | Substrate | **Mehr** |
|--|---------|----------|--------|-----------|----------|
| **Governance** | Miner signaling (BIP-9) | Core dev rough consensus | Token-weighted on-chain vote | Conviction voting (OpenGov) | **Trust-weighted MEP signaling** |
| **Threshold** | 90–95% hashrate | Social consensus | >50% stake | Varies by track | **≥67% trust flow weight** |
| **Sybil resistance** | Hashrate (PoW cost) | N/A (social process) | Stake (capital cost) | Stake (capital cost) | **Trust graph (relationship cost)** |
| **Upgrade type** | Soft forks only | Scheduled hard forks | Coordinated halt + restart | Forkless WASM swap | **Phased transition (soft → hard)** |
| **Downtime** | None | None | Brief halt at upgrade height | None | **None** |
| **Partition tolerance** | N/A (global consensus) | N/A (global consensus) | N/A (global consensus) | N/A (global consensus) | **Full — partitioned nodes rejoin after upgrade** |
| **Constrained devices** | Full nodes need ~500 GB | Full nodes need ~1 TB | Validators need moderate resources | Nodes need WASM runtime | **ESP32 nodes participate with 4-bit version byte** |

---

## Core Services

### MHR-App: Distributed Applications
<!-- Source: docs/services/mhr-app/index.md -->

# MHR-App: Distributed Applications

Mehr provides low-level primitives — storage, compute, pub/sub, DHT, naming, identity. This page describes how those primitives compose into distributed applications without a central server, blockchain, or global consensus.

## Application Model

An application on Mehr is a composition of five elements:

```
Application = State + Logic + Events + Identity + Discovery

  State:     CRDT DataObjects in MHR-Store (replicated, eventually consistent)
  Logic:     MHR-Byte / WASM in MHR-Compute (deterministic, verifiable)
             Opaque compute for heavy workloads (GPU/NPU, reputation-verified)
  Events:    MHR-Pub subscriptions (reactive updates, scoped)
  Identity:  Ed25519 keypairs (same as node identity)
  Discovery: MHR-DHT (content-addressed) + MHR-Name (human-readable)
```

There is no "app server." Each user's node runs the application logic, stores its own state, and synchronizes with other participants through the mesh.

## How It Works

```mermaid
graph TD
    subgraph App["Distributed Application"]
        UI["User Interface<br/>(client-side)"]
    end

    subgraph Primitives["Mehr Primitives"]
        Store["MHR-Store<br/>CRDT DataObjects<br/>(state)"]
        Compute["MHR-Compute<br/>MHR-Byte / WASM<br/>(logic)"]
        Pub["MHR-Pub<br/>Subscriptions<br/>(events)"]
        DHT["MHR-DHT<br/>Content-addressed<br/>(discovery)"]
        Name["MHR-Name<br/>Human-readable<br/>(naming)"]
        Identity["Ed25519 Keypair<br/>(identity)"]
    end

    UI --> Store
    UI --> Compute
    UI --> Pub
    UI --> DHT
    UI --> Name
    UI --> Identity

    Store <-->|"CRDT sync"| Store
    Pub -->|"event notifications"| UI
    Compute -->|"validation"| Store
    DHT -->|"locate data"| Store
    Name -->|"resolve names"| DHT
```

**State** lives in [MHR-Store](../mhr-store) as CRDT DataObjects. CRDTs guarantee eventual consistency without consensus — two nodes that edit the same object offline will converge when they reconnect. No conflict resolution protocol needed.

**Logic** runs in [MHR-Compute](../mhr-compute). Deterministic contracts (MHR-Byte or WASM) handle validation, access control, and state transitions. Heavy workloads (ML inference, media processing) use [opaque compute](../mhr-compute#opaque-compute-hardware-accelerated-services) — delegated to GPU/NPU nodes and verified by reputation or redundant execution.

**Events** flow through [MHR-Pub](../mhr-pub). When state changes, the authoring node publishes a notification. Subscribers receive updates reactively — no polling.

**Identity** is an Ed25519 keypair — the same key used for node identity. No separate "app accounts." A user's identity is portable across applications.

**Discovery** uses [MHR-DHT](../mhr-dht) for content-addressed lookups and [MHR-Name](../mhr-name) for human-readable resolution.

## AppManifest

An **AppManifest** binds together an application's components — contract code, UI, state schema, event topics — into a single content-addressed artifact. The manifest is an immutable [DataObject](../mhr-store) in MHR-Store, identified by its Blake3 hash. Human-readable access uses [MHR-Name](../mhr-name) with the [AppManifest target type](../mhr-name#name-targets).

```
AppManifest {
    // ── IDENTITY ──
    manifest_version: u8,               // format version (starts at 1)
    app_version: u16,                   // monotonically increasing application version
    publisher: NodeID,                  // publisher's Ed25519 identity
    created: u64,                       // epoch when this manifest was created

    // ── APP CLASSIFICATION ──
    app_type: u8,                       // 0=Full, 1=Headless, 2=Static
    min_tier: u8,                       // minimum device tier required
    compute_tier: u8,                   // 0=None, 1=MHR-Byte, 2=WASM-Light, 3=WASM-Full

    // ── CODE ──
    contract_count: u8,                 // number of contracts (0-15)
    contracts: [Blake3Hash],            // hashes of MHR-Contract code in MHR-Store
    entry_contract: u8,                 // index of primary contract (0xFF = none)

    // ── UI ──
    ui_root: Option<Blake3Hash>,        // root DataObject of UI bundle

    // ── STATE ──
    state_schema_hash: Option<Blake3Hash>, // hash of CRDT state schema definition

    // ── EVENTS ──
    pub_topic_count: u8,                // number of MHR-Pub topic templates (0-4)
    pub_topics: [PubTopicTemplate],     // scoped event channels

    // ── DEPENDENCIES ──
    dependency_count: u8,               // number of app dependencies (0-8)
    dependencies: [AppDependency],      // other apps/contracts this app requires

    // ── METADATA ──
    name_len: u8,                       // display name length (max 32 bytes)
    name: [u8],                         // UTF-8 display name
    description_hash: Option<Blake3Hash>, // hash of full description DataObject

    // ── SIGNATURE ──
    signature: Ed25519Signature,        // publisher signs all fields above
}

PubTopicTemplate {
    scope_type: u8,                     // 0=Geo (inherit user's geo), 1=Topic
    suffix_len: u8,
    suffix: [u8],                       // UTF-8 topic suffix
}

AppDependency {
    dep_type: u8,                       // 0=Contract, 1=App, 2=StorageObject
    hash: Blake3Hash,                   // hash of the required artifact
}
```

> **Specification**
The AppManifest binds contracts, UI, state schema, event topics, and dependencies into a single content-addressed artifact. Signed by the publisher's Ed25519 key and identified by its Blake3 hash. Three app types: Full (contracts + UI), Headless (contracts only), and Static (UI only).

### App Types

| | Full | Headless | Static |
|---|---|---|---|
| **Contracts** | 1+ | 1+ | 0 |
| **UI** | Required | None | Required |
| **State schema** | Optional | Optional | None |
| **Min tier** | Community+ | Minimal+ | Community+ |
| **Example** | Forum, marketplace | Validation bot, escrow agent | Blog, documentation site |

**Full**: Contracts + UI + state. The typical distributed application.

**Headless**: Contracts + state, no UI. Background services, validation bots, protocol extensions. Can run on ESP32 if using MHR-Byte contracts.

**Static**: UI only, no contracts. A website served from the mesh. Equivalent to the [hosting](../../applications/hosting) model with a formal manifest.

### Dependencies

Apps can declare dependencies on other artifacts:

```
AppDependency types:
  0x00 = Contract    — requires a specific contract by code hash
  0x01 = App         — requires another app's manifest (shared libraries)
  0x02 = StorageObject — requires specific content in MHR-Store (datasets, models, configs)
```

Dependencies are resolved at install time. If a required contract is already cached locally (shared with another app), it is reused. Missing dependencies are fetched from MHR-Store/DHT.

This enables:
- **Shared contracts**: A "CRDT merge library" contract used by multiple apps
- **Data dependencies**: An ML app that requires a specific model stored in MHR-Store
- **App composition**: A dashboard app that depends on a weather app and a forum app

### Manifest Wire Format

| Field | Size | Description |
|-------|------|-------------|
| `manifest_version` | 1 byte | Format version (currently 1) |
| `app_version` | 2 bytes | Application version (u16 LE) |
| `publisher` | 16 bytes | Publisher's destination hash |
| `created` | 8 bytes | Creation epoch (u64 LE) |
| `app_type` | 1 byte | 0=Full, 1=Headless, 2=Static |
| `min_tier` | 1 byte | 0=Minimal, 1=Community, 2=Gateway, 3=Backbone, 4=Inference |
| `compute_tier` | 1 byte | 0=None, 1=MHR-Byte, 2=WASM-Light, 3=WASM-Full |
| `contract_count` | 1 byte | Number of contracts (0-15) |
| `contracts` | 32 x N bytes | Blake3 hashes of contract code |
| `entry_contract` | 1 byte | Index of primary contract (0xFF=none) |
| `ui_root_flag` | 1 byte | 0=absent, 1=present |
| `ui_root` | 0 or 32 bytes | Blake3 hash of UI bundle root |
| `state_schema_flag` | 1 byte | 0=absent, 1=present |
| `state_schema_hash` | 0 or 32 bytes | Blake3 hash of state schema |
| `pub_topic_count` | 1 byte | Number of topic templates (0-4) |
| `pub_topics` | variable | Per template: scope_type (1) + suffix_len (1) + suffix (variable) |
| `dependency_count` | 1 byte | Number of dependencies (0-8) |
| `dependencies` | 33 x N bytes | Per dependency: dep_type (1) + hash (32) |
| `name_len` | 1 byte | Display name length |
| `name` | variable | UTF-8 display name (max 32 bytes) |
| `description_flag` | 1 byte | 0=absent, 1=present |
| `description_hash` | 0 or 32 bytes | Blake3 hash of description DataObject |
| `signature` | 64 bytes | Ed25519 signature over all preceding fields |

**Size examples:**

| App Type | Contracts | Deps | Total |
|----------|-----------|------|-------|
| Headless (1 contract, ESP32) | 32 | 0 | ~185 bytes |
| Static site (no contracts) | 0 | 0 | ~130 bytes |
| Forum (2 contracts + UI) | 64 | 0 | ~285 bytes |
| Complex app (4 contracts + UI + 2 deps) | 128 | 66 | ~395 bytes |

All fit within the 465-byte packet data limit.

### Manifest Message Types

App manifest messages use context byte `0xF7` (social) with sub-types:

| Sub-Type | Name | Description |
|----------|------|-------------|
| `0x0B` | ManifestPublish | Announce a new or updated app manifest |
| `0x0C` | ManifestLookup | Query for an app manifest by hash |
| `0x0D` | ManifestLookupResponse | Return the requested manifest |

## Publishing an Application

```
Developer workflow:

  1. DEVELOP
     Write contract(s) as MHR-Byte bytecode or WASM.
     Build UI as static assets (HTML/CSS/JS).
     Define CRDT state schema (field names, CRDT types, initial values).

  2. STORE CODE
     For each contract:
       Compute Blake3(code) → contract_hash
       Store as immutable DataObject in MHR-Store
       Form StorageAgreements for replication (k≥3 recommended)

  3. STORE UI (if applicable)
     Store each UI asset as an immutable DataObject.
     Create a UI root DataObject linking to all asset hashes.
     Store with replication.

  4. STORE STATE SCHEMA (if applicable)
     Encode CRDT type definitions.
     Store as immutable DataObject.

  5. CREATE MANIFEST
     Populate AppManifest with all hashes, metadata, and dependencies.
     Sign with publisher's Ed25519 key.
     Store manifest as immutable DataObject in MHR-Store.

  6. REGISTER NAME
     Register MHR-Name binding:
       name: "my-forum"
       scope: Topic("apps", "forums")
       target: AppManifest(manifest_hash)    ← target type 0x03
     The name becomes the human-friendly app identifier.

  7. ANNOUNCE (optional)
     Publish notification via MHR-Pub on Scope(Topic("apps"), Prefix).
     Nodes subscribed to app announcements discover the new app.
```

## Discovering and Installing

```
User workflow:

  1. DISCOVER
     Find an app through one of:
       a. Name lookup: "forum-app@topic:apps/forums"
          → resolves to AppManifest hash (target type 0x03)
       b. Topic subscription: subscribe to Scope(Topic("apps"), Prefix)
          → receive announcements of new apps
       c. Peer recommendation: a trusted peer shares a manifest hash
       d. DHT search: query MHR-DHT for a known manifest hash

  2. FETCH MANIFEST
     Retrieve manifest DataObject by hash from MHR-Store/DHT.
     Verify Blake3(manifest_bytes) == expected_hash.
     Verify Ed25519 signature against publisher's public key.
     Check publisher's trust score via MHR-ID:
       trusted publisher     → proceed
       untrusted publisher   → warn user

  3. CHECK COMPATIBILITY
     Compare min_tier against local device capabilities.
     Compare compute_tier against local execution capability:
       MHR-Byte:    any device
       WASM-Light:  Community+ (Pi Zero 2W+)
       WASM-Full:   Gateway+ (Pi 4/5+)
     If underpowered: offer to delegate contract execution via marketplace.

  4. RESOLVE DEPENDENCIES
     For each AppDependency:
       Check local cache — already have it?
       If not: fetch from MHR-Store/DHT by hash.
       Verify content hash on download.
     Dependencies shared across apps are downloaded once.

  5. FETCH COMPONENTS
     Download contract code DataObjects (by hash from contracts[]).
     Download UI root → parse for asset hashes → download assets.
     Download state schema.
     All downloads are content-hash verified.

  6. INITIALIZE STATE
     If state_schema_hash present:
       Parse schema → create local CRDT DataObjects.
       Initialize with default values.
     If joining an existing community (e.g., a forum):
       Discover existing state via DHT/Pub → CRDT merge with peers.

  7. SUBSCRIBE TO EVENTS
     For each PubTopicTemplate:
       Resolve scope_type with user's own scope:
         scope_type=Geo  → use user's Geo scope + suffix
         scope_type=Topic → use suffix directly
       Subscribe via MHR-Pub.
     Subscribe to manifest update channel: Key(manifest_hash).

  8. DONE
     App is "installed" — a purely local operation.
     No server was contacted. No account was created.
```

## App Naming

Apps use the [AppManifest target type](../mhr-name#name-targets) (0x04) in MHR-Name. This lets the naming layer distinguish apps from regular content, enabling app-specific discovery and UI treatment.

**Naming convention**: Apps register under `topic:apps/*` scopes:

```
forum-app@topic:apps/forums
wiki-app@topic:apps/collaboration
weather-bot@topic:apps/utilities
portland-transit@geo:portland           ← geo-scoped app
```

**App lifecycle maps to name lifecycle**:

| App Action | Name Operation |
|-----------|---------------|
| Publish app | Register name → AppManifest(hash) |
| New version | Update name (sequence+1) → AppManifest(new_hash) |
| Unpublish | Revoke name |
| Browse apps | Query `Scope(Topic("apps"), Prefix)` filtered by target type 0x03 |

**Trust-weighted discovery** applies naturally — apps from trusted publishers rank higher in resolution. An untrusted publisher's app scores at the 0.01 floor, making it nearly invisible unless the user explicitly searches for it by hash.

**App browsing**: Subscribe to `Scope(Topic("apps"), Prefix)` via MHR-Pub to receive announcements of new and updated apps within your scope. This is Mehr's equivalent of an app store — decentralized, trust-weighted, with no central curation.

---

### MHR-ID: Identity & Claims
<!-- Source: docs/services/mhr-id/index.md -->

# MHR-ID: Identity & Claims

Mehr identity is **self-certifying** — your public key is your identity, and no authority can revoke it. But identity is more than a key. People want to know: *Where are you? What do you care about? Are you the same person who used to have a different key?* Identity claims and vouches answer these questions through mesh-native peer attestation.

## Claims

An **IdentityClaim** is a signed assertion by a node about itself:

```
IdentityClaim {
    claimant: NodeID,
    public_key: Ed25519PublicKey,   // enables self-verification without prior key exchange
    claim_type: enum {
        GeoPresence {
            scope: HierarchicalScope,       // "I am in Portland"
        },
        CommunityMember {
            scope: HierarchicalScope,       // "I'm in the Pokemon community"
        },
        KeyRotation {
            old_key: PublicKey,             // previous identity
            new_key: PublicKey,             // current identity
        },
        Capability {
            cap_type: CapabilityType,      // "I operate a relay" / "I have 100GB storage"
            evidence: Option<Blake3Hash>,   // hash of proof data
        },
        ExternalIdentity {
            platform: String,              // "github", "twitter", etc.
            handle: String,                // username on that platform
            challenge: Option<IdentityChallenge>,  // verification evidence
        },
        ProfileField {
            key: String,                   // field name (e.g., "display_name", "avatar")
            value: Vec<u8>,                // plaintext or encrypted (depends on visibility)
            value_type: u8,                // 0=Text, 1=ContentHash, 2=Coordinates, 3=Integer
        },
    },
    visibility: Visibility,         // who can read this claim
    evidence: Option<Evidence>,     // proof backing the claim (embedded in claim_data)
    created: Timestamp,
    expires: Option<Timestamp>,     // None = no expiry (must be renewed by vouches)
    signature: Ed25519Sig,          // signed by claimant
}
```

> **Specification**
IdentityClaims are self-certifying: signed by the claimant's Ed25519 key, with per-field visibility controls (Public, TrustNetwork, DirectTrust, Named). Six claim types cover geographic presence, community membership, key rotation, capabilities, external identity, and profile fields. Minimum wire size: 126 bytes.

### Claim Types

| Type | ID | Purpose | Verification Method |
|------|----|---------|-------------------|
| **GeoPresence** | 0 | "I am present in this place" | [RadioRangeProof](./verification.md#radiorangeproof), [trust graph corroboration](./verification.md#trust-graph-corroboration), peer vouches. Virtual: application-specific attestation |
| **CommunityMember** | 1 | "I participate in this interest community" | Self-declared; peer vouches for closed/moderated communities |
| **KeyRotation** | 2 | "My old key migrated to this new key" | Must be signed by both old and new keys |
| **Capability** | 3 | "I provide this service" | [Proof-of-service](../../marketplace/verification) challenge-response |
| **ExternalIdentity** | 4 | "I am this person on an external platform" | [Identity linking](./verification.md#identity-linking) — crawler or OAuth challenge |
| **ProfileField** | 5 | Arbitrary profile key-value pair | Peer vouches (same as any claim) |

**CommunityMember claims are self-declared by default** — anyone can claim interest in Pokemon. The value comes from the social graph: if 50 nodes you trust all claim `Topic("gaming", "pokemon")`, that's a real community. For **closed or moderated communities**, existing members can vouch for newcomers (high confidence) or dispute fraudulent claims (confidence: 0). A community like `Topic("portland-mesh-collective")` might informally require vouches from 2+ existing members before other nodes treat the claim as credible — this emerges from trust-weighted vouch aggregation, not from any protocol-level gate.

**ExternalIdentity** claims link your Mehr identity to external platforms. Verification uses [crawler or OAuth challenges](./verification.md#identity-linking), similar to [FUTO ID](https://docs.polycentric.io/futo-id/). This is optional and only relevant for nodes with internet access — most mesh nodes don't have internet.

**ProfileField** claims are general-purpose key-value pairs for profile information. See [Profile Fields](#profile-fields) for standard keys and value types.

## Profile Fields

A `ProfileField` claim (type 5) is a key-value pair describing something about you. Each field is a separate IdentityClaim — you publish one claim per field, each with its own [visibility](#visibility-controls) and vouch history.

### Standard Keys

These keys are conventions that clients recognize for display. Keys are free-form strings — users can add any key they want.

| Key | Value Type | Example |
|-----|-----------|---------|
| `display_name` | Text | "Alice Chen" |
| `bio` | Text | "Mesh enthusiast from Portland" |
| `avatar` | ContentHash | Blake3 hash of image in MHR-Store |
| `banner` | ContentHash | Blake3 hash of banner image |
| `email` | Text | "alice@example.com" |
| `phone` | Text | "+1-555-0123" |
| `website` | Text | "my-site@topic:tech" (MHR-Name) |
| `address` | Text | "123 Hawthorne Blvd, Portland" |
| `coordinates` | Coordinates | Lat/lon as two i32 (fixed-point, 1e-7 degrees) |
| `pronouns` | Text | "she/her" |
| `achievement` | Text | "3x EU Chess Champion" |
| `organization` | Text | "Portland Mesh Collective" |

### Value Types

| ID | Type | Encoding |
|----|------|----------|
| 0 | Text | UTF-8 string |
| 1 | ContentHash | 32-byte Blake3 hash (references a DataObject in MHR-Store) |
| 2 | Coordinates | 8 bytes: lat (i32 LE) + lon (i32 LE), fixed-point at 1e-7 degrees |
| 3 | Integer | 8 bytes: i64 LE |

### Vouchable Profile Fields

ProfileField claims use the existing vouch system. Your "3x EU Chess Champion" claim can be vouched for (confidence: 255) by peers who know it's true, or disputed (confidence: 0) by peers who know it's false. Trust-weighted vouch aggregation determines how credible each field appears to each viewer.

Sensitive fields like `email` or `phone` should use [DirectTrust or Named visibility](#visibility-controls) — there's no reason to broadcast your phone number to the entire mesh.

## Visibility Controls

Every IdentityClaim has a `visibility` field that controls who can read it. Public claims work like before — gossiped freely, readable by anyone. Non-public claims are encrypted so only authorized nodes can read the `claim_data`.

```
Visibility {
    Public,              // 0 — gossip freely, anyone can read (default)
    TrustNetwork,        // 1 — encrypted for trusted peers + friends-of-friends (2 hops)
    DirectTrust,         // 2 — encrypted for direct trusted peers only
    Named(Vec<NodeID>),  // 3 — encrypted for specific listed nodes
}
```

### Encryption per Visibility Level

All encryption uses existing Mehr primitives — no new cryptographic algorithms.

**Public** (0): `claim_data` is plaintext. Current behavior, no change.

**DirectTrust** (2): The claimant generates a symmetric group key and encrypts `claim_data` with ChaCha20-Poly1305. The group key is distributed individually to each direct trusted peer via an [E2E envelope](../../protocol/security#end-to-end-encryption-data-payloads) (X25519 → ChaCha20-Poly1305), the same pattern used for [group messaging key distribution](../../applications/messaging#key-management). When trust relationships change (peer added or removed), the group key rotates — new key distributed to current trusted peers.

**TrustNetwork** (1): Same as DirectTrust, but each direct trusted peer also re-encrypts and forwards the group key to *their* direct trusted peers (one additional hop). This extends visibility to friends-of-friends. Each forwarding peer wraps the key in an E2E envelope for the next recipient.

**Named** (3): `claim_data` is encrypted individually for each listed NodeID's public key via E2E envelope. Only those specific nodes can decrypt. The `vis_data` field carries the recipient list: count (u8) + NodeID (16 bytes each).

### Gossip Behavior by Visibility

| Visibility | Propagation | Who Can Decrypt |
|-----------|-------------|----------------|
| Public | Gossips within scope (current behavior) | Anyone |
| TrustNetwork | Gossips to trusted peers and their peers (2-hop max) | Direct + friend-of-friend peers |
| DirectTrust | Gossips to direct trusted peers only (1-hop max) | Direct trusted peers only |
| Named | Sent directly to named recipients, not gossiped | Only listed NodeIDs |

Non-public claims still propagate their **existence** — other nodes can see that a claim exists (claimant, claim_type, visibility level) but cannot read the encrypted `claim_data`. This lets the trust graph account for hidden claims without revealing their contents.

### Group Key Management

The per-claimant group key follows the same lifecycle as [group messaging keys](../../applications/messaging#key-management):

- **Creation**: Claimant generates a ChaCha20 key and distributes it via E2E envelopes to each authorized peer (~100 bytes per recipient)
- **Rotation**: When the trust set changes (peer added/removed), generate new key, distribute to current set. Old keys retained so peers can decrypt previously-received claims
- **Practical limit**: ~100 trusted peers (same as group messaging), bounded by key distribution bandwidth

### Key Rotation and Claim Update Semantics

When a user updates a DirectTrust or TrustNetwork claim, the interaction between group key rotation and claim versioning follows these rules:

```
Trust revocation and key rotation:

  Alice revokes trust of Bob:
    1. Alice generates new group key K_new
    2. Alice distributes K_new to all CURRENT trusted peers (excluding Bob)
    3. NEW claims are encrypted with K_new
    4. OLD claims encrypted with K_old are NOT re-encrypted
       → Bob retains access to historical claims encrypted with K_old
       → Bob cannot read any claims encrypted with K_new
    Rationale: Re-encrypting old claims is impractical on constrained
    devices and creates a bandwidth storm. Historical access is acceptable
    because Bob already saw the data when he was trusted. The security
    property is forward secrecy: revoked peers lose access to FUTURE claims.

  Alice adds trust of Carol:
    1. Alice distributes current group key K_current to Carol via E2E envelope
    2. Carol can decrypt all claims encrypted with K_current
    3. Carol cannot decrypt claims encrypted with older keys
       (unless Alice explicitly re-distributes old keys — optional)
```

### Claim Versioning

Claims support monotonic versioning for updates to the same claim type and key:

```
Claim versioning rules:

  Each IdentityClaim has an implicit version derived from:
    claim_version_key = (claimant, claim_type, claim_qualifier)

  Where claim_qualifier is:
    GeoPresence:     scope string
    CommunityMember: scope string
    KeyRotation:     old_key hash
    Capability:      cap_type
    ExternalIdentity: platform + handle
    ProfileField:    key string

  Version ordering:
    Claims with the same claim_version_key are ordered by created timestamp.
    A newer claim (higher created timestamp) supersedes an older one.
    Nodes that receive both keep only the newer claim.

  Vouch migration:
    Vouches reference claims by claim_hash. When a claim is superseded:
      - Existing vouches for the old claim remain valid for that claim
      - The new claim needs fresh vouches
      - Vouchers are notified via MHR-Pub that the claim was updated
      - Vouchers can publish new vouches for the updated claim
```

### Encrypted Claim Caching

When a node fetches an encrypted claim but doesn't hold the decryption key:

```
Encrypted claim caching behavior:

  Node receives an encrypted claim it cannot decrypt:
    1. Store the encrypted ciphertext in local cache
    2. Record: (claim_hash, claimant, claim_type, visibility_level)
       — metadata is visible even for encrypted claims
    3. Cache TTL: same as unencrypted claims (scope-based gossip TTL)
    4. Do NOT retry decryption proactively

  When to attempt re-decryption:
    - When the node receives a new group key distribution from the claimant
      (indicates the node was added to the trust set)
    - The node checks all cached encrypted claims from that claimant
      and attempts decryption with the new key
    - Successfully decrypted claims are promoted to the readable cache

  The node does NOT poll or request keys. Key distribution is push-based
  (claimant → authorized peers). If the trust relationship changes and
  the node receives a key, it can retroactively decrypt cached claims.
```

### Partition Key Rotation Reconciliation

If two partitions both rotate group keys independently:

```
Partition key rotation reconciliation:

  Scenario: Alice is in partition A, some trusted peers in partition B.
  Both sides may rotate keys independently (e.g., Alice revokes someone
  in partition A; a co-admin rotates in partition B for a group).

  On merge:
    1. Both key versions are valid — they encrypt different claims
    2. Claims encrypted with K_A are decryptable by partition A peers
    3. Claims encrypted with K_B are decryptable by partition B peers
    4. Peers in both partitions may hold both keys

  Resolution:
    The claimant (Alice) performs a KEY UNIFICATION after merge:
      1. Generate a fresh key K_merged
      2. Distribute K_merged to the CURRENT trusted set (union of both partitions)
      3. Publish new claims encrypted with K_merged
      4. Old claims remain readable by whoever holds K_A or K_B

  If the claimant is offline after merge:
    No unification occurs. Both key lineages coexist.
    Peers with both keys can read all claims.
    Peers with only one key can read only that lineage's claims.
    Unification happens whenever the claimant next comes online.

  This is consistent with Mehr's eventual consistency model —
  temporary divergence is acceptable, convergence happens naturally.
```

## Profile Assembly

Clients build a user's profile locally by fetching their claims and filtering by visibility. Different viewers see different fields depending on their trust relationship with the profile owner.

```
Profile assembly (performed locally by each viewer):

  1. Fetch UserProfile DataObject for the target node (from social layer)
  2. For each claim hash in UserProfile.claims:
     a. Fetch IdentityClaim from MHR-Store or MHR-DHT
     b. Check visibility:
        - Public: read plaintext claim_data
        - DirectTrust/TrustNetwork: attempt decryption with held group key
        - Named: attempt decryption with own private key
        - If decryption fails: field exists but is hidden
     c. Check verification: aggregate trust-weighted vouches from local trust graph
     d. Categorize by claim_type:
        - GeoPresence → location section
        - CommunityMember → communities/interests section
        - ExternalIdentity → linked accounts (with verification status)
        - ProfileField → structured profile fields
        - Capability → services offered
  3. Render profile locally — each viewer sees a different subset
     based on their trust relationship with the profile owner
```

The [UserProfile](../../applications/social#profile) DataObject in the social layer references claims by hash. The profile is a **view** assembled from claims — not a separate data structure. When a user updates a profile field, they publish a new IdentityClaim (higher sequence or new claim hash) and update the UserProfile's claim list.

### Wire Format

| Field | Size | Description |
|-------|------|-------------|
| `claimant` | 16 bytes | Destination hash |
| `public_key` | 32 bytes | Ed25519 verifying key (enables self-verification without prior key exchange) |
| `claim_type` | 1 byte | 0=GeoPresence, 1=CommunityMember, 2=KeyRotation, 3=Capability, 4=ExternalIdentity, 5=ProfileField |
| `visibility` | 1 byte | 0=Public, 1=TrustNetwork, 2=DirectTrust, 3=Named |
| `vis_data_len` | 1 byte | Length of visibility metadata (0 for Public/TrustNetwork/DirectTrust) |
| `vis_data` | variable | For Named: count (u8) + NodeID list. Empty for others |
| `claim_data_len` | 2 bytes | Length of claim_data (u16 LE) |
| `claim_data` | variable | Type-specific payload (includes evidence if applicable). Encrypted for non-Public visibility |
| `created` | 8 bytes | Unix timestamp |
| `expires` | 1–9 bytes | 1 byte flag (0=no expiry, 1=has expiry) + 8 bytes timestamp if flag=1 |
| `signature` | 64 bytes | Ed25519 signature (covers all fields including visibility, computed over plaintext before encryption) |

Minimum claim size: 126 bytes (no data, no expiry, Public visibility). Fits comfortably in a single LoRa frame.

**Backward compatibility**: Claims without the `visibility` and `vis_data_len` fields (from older nodes) default to Public visibility. New nodes detect old-format claims by checking if the byte at the visibility offset is a valid claim_data_len high byte — since visibility values are 0–3 and old claim_data_len is u16 LE, the disambiguation is unambiguous for all practical payloads.

## Vouches

A **Vouch** is a trust-weighted endorsement of someone else's claim:

```
Vouch {
    voucher: NodeID,                // who is vouching
    claim_hash: Blake3Hash,         // Blake3 hash of the IdentityClaim being vouched for
    confidence: u8,                 // 0-255: how confident the voucher is
    sequence: u64,                  // monotonic counter for superseding/revoking
    signature: Ed25519Sig,          // signed by voucher
}
```

### Vouch Properties

- **Trust-weighted**: A vouch from a node you trust directly is worth more than one from a stranger. Vouch weight decays with trust distance, just like [transitive credit](../../economics/trust-neighborhoods#trust-based-credit): 100% for direct trusted peers, 10% for friend-of-friend, 0 beyond 2 hops.
- **Expiring**: Vouches are valid for a configurable period (default: 30 epochs). After expiry, the vouch must be renewed or the claim loses its verified status. This prevents stale geographic claims from persisting after someone moves.
- **Revocable**: A voucher can publish a revocation (vouch with `confidence: 0` for the same `claim_hash`) at any time.
- **Cumulative**: Multiple vouches for the same claim increase confidence. A geographic claim vouched by 10 trusted peers is stronger than one vouched by 1.

### Vouch Wire Format

| Field | Size | Description |
|-------|------|-------------|
| `voucher` | 16 bytes | Destination hash |
| `claim_hash` | 32 bytes | Blake3 hash of the claim |
| `confidence` | 1 byte | 0-255 |
| `sequence` | 8 bytes | Monotonic counter (LE). Higher sequence supersedes older vouches for the same (voucher, claim_hash) pair. |
| `signature` | 64 bytes | Ed25519 signature |

Total: 121 bytes. Lightweight enough to gossip freely.

## Claim Lifecycle

```
1. CREATE: Node publishes IdentityClaim (signed, stored as immutable DataObject)
2. GOSSIP: Claim propagates via MHR-DHT within relevant scope
3. VOUCH: Peers who can verify the claim publish Vouches
4. VERIFY: Other nodes calculate trust-weighted verification level
5. RENEW: Vouches expire after 30 epochs; vouchers re-vouch if claim still valid
6. REVOKE: Claimant publishes a new claim superseding the old one,
           or vouchers publish confidence=0 revocations
```

### Storage and Propagation

- Claims are stored as **immutable DataObjects** in [MHR-Store](../mhr-store)
- Geographic claims propagate within the claimed scope (a Portland claim gossips within Portland)
- Interest claims propagate within the interest scope
- Vouches propagate with the claims they reference
- Both are lightweight enough for LoRa (~121–126 bytes)

---

### MHR-Store: Content-Addressed Storage
<!-- Source: docs/services/mhr-store.md -->

# MHR-Store: Content-Addressed Storage

MHR-Store is the storage layer of Mehr. Every piece of data is addressed by its content hash — if you know the hash, you can retrieve the data from anywhere in the network. Storage is maintained through bilateral agreements, verified through lightweight challenge-response proofs, and protected against data loss through erasure coding.

## Data Objects

```
DataObject {
    hash: Blake3Hash,               // content hash = address
    content_type: enum { Immutable, Mutable, Ephemeral },
    owner: Option<NodeID>,          // for mutable objects
    created: Timestamp,
    ttl: Option<Duration>,          // for ephemeral objects
    size: u32,
    priority: enum { Critical, Normal, Lazy },
    min_bandwidth: u32,             // don't attempt transfer below this bps

    // Merkle tree root over 4 KB chunks (for verification)
    merkle_root: Blake3Hash,

    payload: enum {
        Inline(Vec<u8>),            // small objects (under 4 KB)
        Chunked([ChunkHash]),       // large objects (4 KB chunks)
    },
}
```

## Content Types

### Immutable

Once created, the content never changes. The hash is the permanent address. Used for: messages, posts, media files, contract code.

### Mutable

The owner can publish updated versions, signed with their key. The highest sequence number wins. Any node can verify the signature. Used for: profiles, status updates, configuration.

**Versioning rules**:
- Sequence numbers must be **strictly monotonic** — each update must have a higher sequence number than the previous one
- Updates are only valid when signed by the owner's Ed25519 key
- **Fork detection**: If two updates with the same sequence number but different content are observed (both validly signed), this is treated as evidence of key compromise or device cloning

**Fork handling**:

```
When a fork is detected (sequence N, two different content hashes, both validly signed):
  1. Record: store (object_key, sequence, both content hashes, detection_time)
  2. Block: reject new updates to this object from this owner for 24 hours
     or until a KeyCompromiseAdvisory with SignedByBoth evidence is received
  3. Gossip: include the fork evidence in the next gossip round as advisory
     metadata (both conflicting hashes + sequence). This is informational —
     receiving nodes independently verify and may apply their own block
  4. Dedup: fork records are retained for 7 days to prevent re-reporting
  5. Resolution: a KeyCompromiseAdvisory with SignedByBoth clears the block
     and allows the new key's updates to proceed. SignedByOldOnly does NOT
     clear the block (could be the attacker)
```

### Ephemeral

Data with a time-to-live (TTL). Automatically garbage-collected after expiration. Used for: presence information, temporary caches, session data.

## Storage Agreements

Storage on Mehr is maintained through **bilateral agreements** between data owners and storage nodes. This is how data stays alive on the network.

> **Specification**
```
StorageAgreement {
    data_hash: Blake3Hash,          // what's being stored
    data_size: u32,                 // bytes
    provider: NodeID,               // who stores it
    consumer: NodeID,               // who pays for it
    payment_channel: ChannelID,     // bilateral channel
    cost_per_epoch: u64,            // MHR per epoch
    duration_epochs: u32,           // how long
    challenge_interval: u32,        // how often to verify (in gossip rounds)
    erasure_role: Option<ShardInfo>,// if part of an erasure-coded set

    // Revenue sharing — for content that earns from reader access
    kickback_recipient: Option<NodeID>,  // original author (receives share of retrieval fees)
    kickback_rate: u8,                   // 0-255: author's share of retrieval fee (rate/255)

    signatures: (Sig_Provider, Sig_Consumer),
}
```


### Payment Model

Storage is **pay-per-duration** — like rent, not like purchase. The data owner pays the storage node a recurring fee via their bilateral [payment channel](../economics/payment-channels).

| Duration | Billing | Use Case |
|----------|---------|----------|
| Short-term (hours) | Per-epoch micro-payments | Temporary caches, session data |
| Medium-term (weeks) | Prepaid for N epochs | Messages, posts, media |
| Long-term (months+) | Recurring per-epoch | Persistent data, profiles, hosted content |

When payment stops, the storage node garbage-collects the data after a grace period (1 epoch). The data owner is responsible for maintaining payment — there is no "permanent storage" guarantee.

### Free Storage Between Trusted Peers

Just like [relay traffic](../economics/trust-neighborhoods), storage between trusted peers is **free**:

```
Storage decision:
  if data owner is trusted:
    store for free (no agreement needed, no payment)
  else:
    require a StorageAgreement with payment
```

A trust neighborhood where members store each other's data operates with zero economic overhead — no tokens, no agreements, no challenges. This is how a community mesh handles local content naturally.

## Revenue Sharing (Kickback)

When content is published for public consumption (social posts, media, curated feeds), the **original author** can earn a share of retrieval fees through the kickback mechanism.

### How It Works

```
Author publishes post → creates StorageAgreement with kickback fields:
    kickback_recipient = Author's NodeID
    kickback_rate = 128  (roughly 50% of retrieval fees go to author)

Reader pays storage node to retrieve post:
    Retrieval fee: 100 μMHR
    Storage node keeps: 100 × (255 - 128) / 255 ≈ 50 μMHR
    Storage node forwards: 100 × 128 / 255 ≈ 50 μMHR → Author

Settlement: via the existing payment channel between storage node and author
```

### Incentive Alignment

- **Storage nodes** are incentivized to host popular content because they earn the non-kickback portion of every retrieval fee
- **Authors** are incentivized to create content people want to read because they earn kickback from every reader
- **Readers** pay the same retrieval fee regardless — the kickback split is between author and storage node
- **Curators** who reference posts in [curated feeds](../applications/social#5-curated-feed) drive traffic to original authors, earning kickback on their own curation feed while generating kickback for the original authors too

### Kickback Rate

The `kickback_rate` field is a `u8` (0–255):

| Rate | Author Share | Storage Node Share | Typical Use |
|------|-------------|-------------------|-------------|
| 0 | 0% | 100% | No kickback (pure storage) |
| 64 | ~25% | ~75% | Low author share, high storage incentive |
| 128 | ~50% | ~50% | Balanced split |
| 192 | ~75% | ~25% | High author share |
| 255 | 100% | 0% | Maximum author share (storage node earns only per-epoch fee) |

If `kickback_recipient` is `None`, there is no kickback — the storage node keeps all retrieval fees. This is the default for non-social content (private data, infrastructure objects, etc.).

### Self-Funding Content

When kickback revenue exceeds storage cost, content becomes **self-funding**. The author can reinvest kickback to extend storage agreements, or an automated [RepairAgent](#automated-repair) can do it:

```
Self-funding threshold:
    If kickback_per_epoch > cost_per_epoch:
        Content is self-sustaining — it lives as long as people read it
    If kickback_per_epoch < cost_per_epoch:
        Author must subsidize — content expires if author stops paying
```

Popular content that crosses the self-funding threshold climbs the [propagation hierarchy](../economics/propagation) automatically. Unpopular content stays local or expires. No algorithm decides what lives and what dies — economics does.

## Proof of Storage

How does a data owner know a storage node actually has their data? Through **lightweight challenge-response proofs** that run on any hardware, including ESP32.

### Challenge-Response Protocol

```
Proof of Storage:
  1. Data owner builds a Merkle tree over 4 KB chunks at storage time
     (stores only the merkle_root — not the full tree)

  2. Periodically, owner sends:
     Challenge {
         data_hash: Blake3Hash,
         chunk_index: u32,          // random chunk to verify
         nonce: [u8; 16],           // prevents pre-computation
     }

  3. Storage node responds:
     Proof {
         chunk_hash: Blake3(chunk_data || nonce),
         merkle_proof: [sibling hashes from chunk to root],
     }

  4. Owner verifies:
     a. Recompute merkle root from chunk_hash + merkle_proof
     b. Compare against stored merkle_root
     c. If match: storage verified
     d. If mismatch: node is lying or lost the data
```

### Why This Works on Constrained Devices

| Operation | Compute Cost | RAM Required |
|-----------|-------------|-------------|
| Generate challenge | 16 random bytes | Negligible |
| Compute chunk hash (storage node) | 1 Blake3 hash of 4 KB | 4 KB |
| Verify Merkle proof (owner) | ~10 Blake3 hashes (for 1 MB file) | ~320 bytes |

An ESP32 can verify a storage proof in under 10ms. No GPU needed, no heavy cryptography, no sealing. This is intentionally simpler than Filecoin's Proof of Replication — we trade the ability to detect deduplicated storage for something that actually runs on mesh hardware.

### Challenge Frequency

| Data Priority | Challenge Interval |
|--------------|-------------------|
| Critical | Every gossip round (60 seconds) |
| Normal | Every 10 gossip rounds (~10 minutes) |
| Lazy | Every 100 gossip rounds (~100 minutes) |

Challenges are staggered across stored objects so a storage node never faces a burst of challenges at once.

### What If a Challenge Fails?

```
Challenge failure handling:
  1. First failure: retry after 1 gossip round (could be transient)
  2. Second consecutive failure: flag the storage node
  3. Third consecutive failure: consider data lost on this node
     → trigger repair (see Erasure Coding below)
     → reduce node's storage reputation
     → terminate the StorageAgreement
```

## Erasure Coding

Full replication is wasteful. Storing 3 complete copies of a file costs 3x the storage. **Erasure coding** achieves the same durability with far less overhead.

### Reed-Solomon Coding

Mehr uses Reed-Solomon erasure coding to split data into **k data shards + m parity shards**, where any k of (k + m) shards can reconstruct the original:

```
Erasure coding example (4, 2):
  Original file: 1 MB
  → Split into 4 data shards (256 KB each)
  → Generate 2 parity shards (256 KB each)
  → 6 shards total, stored on 6 different nodes
  → Any 4 of 6 shards can reconstruct the original
  → Total storage: 1.5 MB (1.5x overhead)

Compare with 3x replication:
  → 3 full copies = 3 MB (3x overhead)
  → Tolerates 2 node failures (same as erasure coding)
  → But uses 2x more storage
```

### Default Erasure Parameters

| Data Size | Scheme | Shards | Overhead | Tolerates |
|-----------|--------|--------|----------|-----------|
| Under 4 KB | No erasure (inline) | 1 | 1x | Replication only |
| 4 KB – 1 MB | (2, 1) | 3 shards | 1.5x | 1 node loss |
| 1 MB – 100 MB | (4, 2) | 6 shards | 1.5x | 2 node losses |
| Over 100 MB | (8, 4) | 12 shards | 1.5x | 4 node losses |

The data owner chooses the scheme based on durability requirements and willingness to pay. Higher redundancy = more shards = more storage agreements = higher cost.

### Shard Distribution

Shards are distributed across nodes in **different trust neighborhoods** to maximize independence:

```
Shard placement strategy:
  1. Prefer nodes in different trust neighborhoods
  2. Prefer nodes with different transport types (LoRa, WiFi, cellular)
  3. Prefer nodes with proven uptime (high storage reputation)
  4. Never place two shards of the same object on the same node
```

This ensures that a single neighborhood going offline (power outage, network split) doesn't lose more shards than the erasure code can tolerate.

## Repair

When a storage node fails challenges or goes offline, the data owner must **repair** — reconstruct the lost shard and store it on a new node.

```
Repair flow:
  1. Detect: storage node fails 3 consecutive challenges
  2. Assess: how many shards are still healthy?
     - If >= k shards remain: reconstruction is possible
     - If fewer than k: data is lost (this is why shard distribution matters)
  3. Reconstruct: download k healthy shards, regenerate the lost shard
  4. Re-store: form a new StorageAgreement with a different node
  5. Upload the reconstructed shard
```

### Automated Repair

For users who can't be online to monitor their data, repair can be delegated:

```
RepairAgent {
    data_hash: Blake3Hash,
    shard_map: Map<ShardIndex, NodeID>,
    merkle_root: Blake3Hash,
    authorized_spender: NodeID,     // can spend from owner's channel
    max_repair_cost: u64,           // budget cap
}
```

A RepairAgent is an [MHR-Compute contract](mhr-compute) that periodically challenges storage nodes on behalf of the data owner. If a shard is lost, it handles reconstruction and re-storage automatically, spending from the owner's pre-authorized budget.

## Bandwidth Adaptation

The `min_bandwidth` field controls how data propagates across links of different speeds:

```
Example:
  A 500 KB image declares min_bandwidth: 10000 (10 kbps)

  LoRa node (1 kbps):
    → Propagates hash and metadata only
    → Never attempts to transfer the full image

  WiFi node (100 Mbps):
    → Transfers normally
```

This is a property of the data object that the storage and routing layers respect. Applications set `min_bandwidth` based on the nature of the data, and the network handles the rest.

## Garbage Collection

Storage nodes manage their disk space through a priority-based garbage collection system:

```
Garbage collection priority (lowest priority deleted first):
  1. Expired TTL + no active agreement → immediate deletion
  2. Unpaid (agreement expired, no renewal) → delete after 1 epoch grace
  3. Cached content (no agreement, just opportunistic) → LRU eviction
  4. Low-priority paid data → delete only under extreme space pressure
  5. Normal paid data → never delete while agreement is active
  6. Critical paid data → never delete while agreement is active
  7. Trusted peer data → never delete while trust relationship exists
```

A storage node never deletes data that has an active, paid agreement. Data whose agreement expires is kept for a 1-epoch grace period (to allow renewal), then garbage-collected.

## Chunking

Large objects are split into 4 KB chunks, each independently addressed by hash:

```
Large file (1 MB):
  → Split into 256 chunks of 4 KB each
  → Each chunk has its own Blake3 hash
  → Merkle tree built over all chunk hashes
  → The DataObject stores the chunk hash list and merkle_root
  → Chunks can be retrieved from different nodes in parallel
  → Missing chunks can be re-requested individually
```

Chunking enables:
- Parallel downloads from multiple peers
- Efficient deduplication (identical chunks across objects are stored once)
- Resumable transfers on unreliable links
- Fine-grained storage proofs (challenge any individual chunk)
- Erasure coding at the chunk level for large objects

### Reassembly

```
Fragment reassembly protocol:
  Per-chunk timeout: 30 seconds (configurable per StorageAgreement)
  Retry policy: exponential backoff (2s, 4s, 8s, max 30s), up to 3 retries
  After 3 retries: mark chunk provider as unreliable, try alternate via DHT
  Overall timeout: 5 minutes (all chunks must arrive within this window)

  Resumable downloads:
    Consumer tracks received chunk indices. To resume, send:
      ChunkRequest { data_hash: Blake3Hash, chunk_indices: Vec<u32> }
    Provider responds with only the requested chunks, avoiding
    retransmission of already-received data.
```

## Comparison with Other Storage Protocols

| Aspect | Filecoin | Arweave | Mehr (MHR-Store) |
|--------|----------|---------|-------------------|
| **Payment** | Per-deal, on-chain | One-time endowment | Per-duration, bilateral channels |
| **Proof** | PoRep + PoSt (GPU-heavy, minutes to seal) | SPoRA (mining-integrated) | Challenge-response (milliseconds, runs on ESP32) |
| **Durability** | Slashing for failures (requires blockchain) | Incentivized mining of historical data | Erasure coding + repair agents |
| **Permanent storage** | No (deals expire) | Yes (pay once) | No (pay per duration, data owner's responsibility) |
| **Blockchain** | Required (proof submission on-chain) | Required (block weave) | Not needed (bilateral agreements) |
| **Minimum hardware** | GPU for sealing | Standard PC | ESP32 for verification, Pi for storage |
| **Partition tolerance** | No (needs chain access) | No (needs chain access) | Yes (bilateral proofs work offline) |
| **Free tier** | No | No | Yes (trusted peer storage) |

Mehr deliberately chooses lightweight proofs over heavy cryptographic guarantees. The tradeoff: a storage node could store the same data once and claim to store it twice (unlike Filecoin's Proof of Replication). This is acceptable because:

1. The economic incentive is weak — the node earns the same fee either way
2. The data owner doesn't care *how* the node stores the data, only that it can return it on demand
3. Erasure coding across multiple nodes provides real redundancy regardless

<!-- faq-start -->

## Frequently Asked Questions

<details className="faq-item">
<summary>How long is my data stored on the network?</summary>

Data persists as long as you pay for it. Storage agreements are pay-per-duration — like rent. When payment stops, the storage node garbage-collects your data after a one-epoch grace period. There is no permanent storage guarantee, but popular content that earns enough kickback revenue can become self-funding and persist indefinitely.

</details>

<details className="faq-item">
<summary>What happens if the node storing my data goes offline?</summary>

If you’re using erasure coding (recommended), your data is split across multiple nodes with redundancy. For example, with a 4+2 scheme, any 4 of 6 chunks can reconstruct the full file. If one node goes offline, an automated RepairAgent detects the shortfall and re-replicates to a replacement node, restoring full redundancy.

</details>

<details className="faq-item">
<summary>Can I delete data I’ve stored on the mesh?</summary>

Yes. Stop paying the storage agreement and the data will be garbage-collected after the grace period. For mutable DataObjects, you can publish a tombstone update. However, if someone else has already retrieved and cached your data, those copies are outside your control — similar to how deleting a social media post doesn’t remove screenshots.

</details>

<details className="faq-item">
<summary>How much does storage cost?</summary>

Storage prices are set by individual providers competing in the marketplace. Trusted peers store each other’s data for free. For non-trusted storage, costs are measured in μMHR per byte per epoch. Typical costs depend on local market conditions — areas with many storage providers will be cheaper than areas with few.

</details>

<details className="faq-item">
<summary>Is my stored data encrypted?</summary>

The protocol supports but does not require encryption. For cloud storage and messaging, data is always encrypted client-side before storage — storage nodes see only ciphertext. For public content (social posts, websites), data is stored unencrypted so anyone can retrieve it. The choice is application-level.

</details>

<!-- faq-end -->

---

### MHR-App: Upgrades & Migration
<!-- Source: docs/services/mhr-app/upgrades.md -->

# MHR-App: Upgrades & Migration

AppManifests are **immutable** — each version is a new DataObject with a new hash. Upgrades work by publishing a new manifest and rebinding the MHR-Name.

## Upgrade Flow

```
  1. Publisher creates new manifest:
     app_version incremented
     Updated contract/UI/schema hashes as needed
     Same publisher identity (continuity of authorship)

  2. Publisher rebinds MHR-Name:
     "forum-app@topic:apps/forums" → AppManifest(new_manifest_hash)
     NameBinding sequence number incremented

  3. Publisher notifies via MHR-Pub:
     Topic = Key(old_manifest_hash)
     Payload = new_manifest_hash + app_version
     All installed nodes receive the notification

  4. User-side upgrade:
     Fetch new manifest, verify signature (same publisher)
     Verify app_version > current version
     Download only changed components (unchanged hashes → already cached)
     Apply state migration if schema changed
```

## State Migration

Three strategies depending on whether the state schema changed:

> **Specification**
`StateSchema` defines fields with explicit CRDT types and defaults. Compatibility is programmatically verified: additive changes (new fields only) are auto-merged via CRDT semantics; breaking changes require a deterministic `migration_contract` that transforms old state into new state.

**Compatible** (schema unchanged): State carries over directly. New contract code works with existing CRDT DataObjects. No migration needed.

**Additive** (new fields added, none removed): New schema lists the old schema hash in a `compatible_with` field. New code initializes new fields with defaults. Old state merges cleanly via CRDT semantics.

**Breaking** (incompatible change): New schema includes a `migration_contract` — a contract that transforms old state into new state. The node runs it locally. If migration fails, the user is warned and the old version remains usable.

```
StateSchema {
    fields: [FieldDef],                     // field names, CRDT types, defaults
    compatible_with: [Blake3Hash],          // previous schema hashes (additive compat)
    migration_contract: Option<Blake3Hash>, // contract for breaking migrations
}

FieldDef {
    name: String,                           // field name (max 32 bytes UTF-8)
    crdt_type: CRDTType,                    // GCounter, GSet, LWWRegister, ORSet, RGA, etc.
    default_value: Vec<u8>,                 // CBOR-encoded default for new fields
    required: bool,                         // false = optional field (can be absent)
}
```

## Schema Compatibility Rules

Compatibility between schema versions is **programmatically verifiable**, not purely declared by the developer. A new schema is **additive-compatible** with an old schema if and only if all of the following hold:

```
Additive compatibility rules (checked by runtime):

  1. NO REMOVED FIELDS: Every field in the old schema exists in the new schema
     with the same name and same CRDT type.

  2. NO TYPE CHANGES: A field's CRDT type CANNOT change between versions.
     Rationale: CRDT merge semantics are type-dependent. A GCounter merged
     with an LWWRegister produces undefined behavior.
     If a field must change type → breaking migration required.

  3. NEW FIELDS ONLY: The new schema may add fields not present in the old schema.
     New fields MUST have default_value specified.

  4. REQUIRED → OPTIONAL allowed: A required field can become optional.
     OPTIONAL → REQUIRED forbidden (would break old state missing the field).

Compatibility checker (runs locally at install/upgrade time):
  fn is_compatible(old_schema: &StateSchema, new_schema: &StateSchema) -> bool {
      for old_field in &old_schema.fields {
          match new_schema.fields.find(|f| f.name == old_field.name) {
              None => return false,                    // field removed
              Some(new_field) => {
                  if new_field.crdt_type != old_field.crdt_type {
                      return false;                    // type changed
                  }
                  if new_field.required && !old_field.required {
                      return false;                    // optional → required
                  }
              }
          }
      }
      true
  }
```

The `compatible_with` field in StateSchema lists schema hashes that pass this check. At install time, the runtime verifies the declaration by running the compatibility checker. A manifest that declares `compatible_with` for an incompatible schema is rejected.

## Old Code Encountering New Schema State

When a node running old code encounters new-schema state via CRDT merge:

```
Forward-compatibility behavior:

  Old node receives state with unknown fields:
    1. Unknown fields are preserved as opaque bytes during CRDT merge
    2. Old code ignores unknown fields for application logic
    3. Unknown fields are forwarded in gossip (pass-through)
    4. When the node upgrades, previously-merged unknown fields
       become usable with the new schema

  This works because CRDT merges are per-field:
    - Known fields: merge normally using the field's CRDT type
    - Unknown fields: merge using a generic LWWRegister fallback
      (last-writer-wins by timestamp — safe for pass-through)

  If the unknown field uses a CRDT type the old node doesn't
  understand (e.g., a new CRDT type added in a later protocol version):
    - The field is stored as opaque bytes
    - Merge uses byte-level LWW fallback
    - On upgrade, the node re-merges using the correct CRDT type
    - Temporary merge inaccuracy during the transition is self-correcting
```

## Migration Contract Execution Semantics

When a breaking schema change requires a `migration_contract`, the execution follows a strict protocol:

```
Migration contract execution:

  Input delivery:
    The migration contract receives the FULL old state as a single CBOR-encoded
    input via the LOAD opcode (key = "migration_input").
    Rationale: incremental migration adds complexity without benefit —
    state sizes are bounded by device tier (ESP32: 32 KB, Pi: 1 MB,
    Gateway: 256 MB), so full-state delivery is practical.

  Contract interface:
    // Entry point
    fn migrate(old_state: CBOR) -> Result<CBOR, MigrationError>

    // The contract:
    //   1. Reads old state from LOAD("migration_input")
    //   2. Transforms fields according to the new schema
    //   3. Writes new state to STORE("migration_output")
    //   4. Returns HALT (success) or ABORT (failure)

  Success criteria:
    1. Contract terminates with HALT (not ABORT)
    2. Output at STORE("migration_output") is valid CBOR
    3. Output conforms to the new StateSchema (all required fields present,
       correct CRDT types)
    4. Migration completes within max_cycles (from contract declaration)

  Failure modes:
    - Runtime exception (ABORT opcode): migration_failed, old state preserved
    - Wrong output shape (schema validation fails): migration_failed
    - Timeout (max_cycles exceeded): migration_failed, old state preserved
    - All failures are recoverable — old app version remains functional

  Determinism guarantee:
    Migration contracts MUST be deterministic (same as all MHR-Byte/WASM).
    Two nodes migrating the same state produce identical outputs.
    If nondeterminism is detected (different nodes get different outputs
    for the same input — detected via hash comparison during CRDT sync):
      - The output with the LOWER Blake3 hash wins (arbitrary but deterministic)
      - This should never happen if the contract is correctly written
      - Detection triggers a warning to the app publisher

  Rollback:
    No automatic rollback. If migration succeeds but causes application bugs:
      1. User can pin to old manifest hash (old state still accessible by hash)
      2. Publisher can release a new version with a fix
      3. Old DataObjects remain in MHR-Store indefinitely

  Partial migration:
    NOT supported. Migration is all-or-nothing per state object.
    Rationale: partial migration creates inconsistent state that violates
    schema invariants. If some fields can migrate independently, they
    should be separate DataObjects in the schema design.
```

## Partition Behavior

> **Trade-off**
Breaking schema migrations are all-or-nothing per state object — partial migration is not supported. If a migration contract fails (runtime exception, wrong output shape, or timeout), the old version is preserved. This prioritizes state consistency over upgrade flexibility.

A partitioned node that missed an upgrade continues running the old version. On reconnection:
1. CRDT state merges normally if schema is compatible
2. If schema is breaking: node detects version mismatch during CRDT sync
3. Node fetches new manifest via MHR-Name re-resolution
4. Runs migration locally, then state converges

## Rollback

There is no protocol-level rollback mechanism. If a new version is broken:
- Users can pin to an old manifest hash (local override)
- The publisher can publish a newer version that reverts changes
- Old manifests, contracts, and state remain available by content hash

## Example: Decentralized Forum

A community forum using Mehr primitives:

```
Forum application:

  State (MHR-Store):
    - ForumConfig: CRDT DataObject with forum name, rules, moderator list
    - Thread: one DataObject per thread (append-only CRDT log of post references)
    - Post: one DataObject per post (mutable — author can edit)

  Logic (MHR-Compute):
    - PostValidator contract: checks post format, size limits, rate limits
    - ModerationContract: checks if author is banned, if content matches filter rules
    - Runs on any node (MHR-Byte — works on ESP32)

  Events (MHR-Pub):
    - subscribe(Scope(Topic("forums", "portland-general")), Push)
    - New posts trigger envelope notifications to all subscribers

  Identity:
    - Forum members are Ed25519 keypairs
    - Moderators are listed in ForumConfig (by NodeID)
    - No separate registration — just start posting

  Discovery:
    - Forum name registered via MHR-Name: "portland-general@geo:portland"
    - Threads discoverable via MHR-DHT by content hash
```

**User experience**: Subscribe to the forum topic. Receive post notifications via MHR-Pub. Browse post envelopes (free). Fetch full posts on demand (paid if outside trust network). Post by creating a DataObject and publishing a notification. Moderators update the ForumConfig to ban users — the ModerationContract enforces it at validation time.

### Forum AppManifest

```
AppManifest {
    manifest_version: 1,
    app_version: 1,
    publisher: 0x3a7f...b2c1,                // forum developer
    created: 1042,                             // epoch 1042

    app_type: 0,                               // Full (contracts + UI)
    min_tier: 1,                               // Community (needs UI rendering)
    compute_tier: 1,                           // MHR-Byte (runs on ESP32)

    contract_count: 2,
    contracts: [
        Blake3("PostValidator bytecode"),
        Blake3("ModerationContract bytecode"),
    ],
    entry_contract: 0,                         // PostValidator is entry point

    ui_root: Some(Blake3("forum UI root")),

    state_schema_hash: Some(Blake3("forum state schema")),

    pub_topic_count: 1,
    pub_topics: [
        { scope_type: 0, suffix: "forums" },   // Geo — inherits user's geo scope
    ],

    dependency_count: 0,
    dependencies: [],

    name: "Mehr Forum",
    description_hash: Some(Blake3("README")),

    signature: Ed25519Sig(...),
}

Published as: "forum-app@topic:apps/forums" → AppManifest(manifest_hash)
```

**Upgrade to v2 (adds search)**:
1. Publisher adds a SearchIndex contract (contract_count: 3)
2. State schema adds `search_index` CRDT field (compatible_with includes v1 schema hash)
3. New manifest: app_version=2, new hashes
4. Rebind MHR-Name, notify via MHR-Pub
5. Users: fetch new manifest, download SearchIndex, init search_index field
6. Old state merges cleanly — additive migration

## Example: Collaborative Wiki

A wiki where multiple authors edit shared documents:

```
Wiki application:

  State (MHR-Store):
    - WikiPage: CRDT DataObject per page (text CRDT — e.g., RGA or Peritext)
    - PageIndex: CRDT DataObject mapping page titles to content hashes
    - EditHistory: append-only log of edit metadata (author, timestamp, summary)

  Logic (MHR-Compute):
    - MergeContract: CRDT merge rules for concurrent edits
    - AccessControl: checks editor permissions (open wiki vs. invited editors)
    - Runs as WASM on Community-tier+ (text CRDTs need more memory than MHR-Byte)

  Events (MHR-Pub):
    - subscribe(Node(wiki_owner_id), Push) for page update notifications
    - Editors receive real-time notifications of concurrent edits

  Identity:
    - Editors identified by Ed25519 keypair
    - Edit attribution is cryptographic (signed edits)

  Discovery:
    - Wiki registered via MHR-Name: "mehr-wiki@topic:documentation"
    - Pages discoverable by title via PageIndex or by hash via MHR-DHT
```

**Offline editing**: An editor on a partitioned node edits a page locally. The text CRDT records the operations. On reconnection, the CRDT merges automatically — no manual conflict resolution. Two editors changing different paragraphs merge cleanly. Two editors changing the same sentence produce a deterministic merge (last-writer-wins per character, or interleaving, depending on CRDT choice).

### Wiki AppManifest

```
AppManifest {
    manifest_version: 1,
    app_version: 1,
    publisher: 0x8e2d...f4a9,
    created: 1050,

    app_type: 0,                               // Full
    min_tier: 1,                               // Community (WASM + UI)
    compute_tier: 2,                           // WASM-Light

    contract_count: 2,
    contracts: [
        Blake3("MergeContract bytecode"),
        Blake3("AccessControl bytecode"),
    ],
    entry_contract: 0,

    ui_root: Some(Blake3("wiki UI root")),
    state_schema_hash: Some(Blake3("wiki state schema")),

    pub_topic_count: 1,
    pub_topics: [
        { scope_type: 1, suffix: "documentation" },  // Topic scope
    ],

    dependency_count: 0,
    dependencies: [],

    name: "Mehr Wiki",
    description_hash: Some(Blake3("Wiki README")),

    signature: Ed25519Sig(...),
}

Published as: "wiki-app@topic:apps/collaboration" → AppManifest(manifest_hash)
```

---

### MHR-DHT: Distributed Hash Table
<!-- Source: docs/services/mhr-dht.md -->

# MHR-DHT: Distributed Hash Table

MHR-DHT maps keys to the nodes that store the corresponding data. It uses proximity-weighted gossip rather than Kademlia-style strict XOR routing, because link quality varies wildly on a mesh network.

### Distance Metrics: Routing vs. DHT

Mehr uses two different distance metrics for different purposes:

- **Ring distance** (routing layer): `min(|a - b|, 2^128 - |a - b|)` over the destination hash space. Used for [greedy forwarding](../protocol/network-protocol#small-world-routing-model) to route packets toward their destination. This is the Kleinberg small-world model.
- **XOR distance** (DHT layer): `a ⊕ b` over DHT key space. Used for determining storage responsibility — which nodes are "closest" to a given key and should store its data.

Both operate over 128-bit spaces derived from the same hash functions, but they serve different roles. Routing cares about navigating to a destination efficiently; the DHT cares about partitioning key-space responsibility among nodes.

## Why Not Kademlia?

Traditional Kademlia routes lookups based on XOR distance between node IDs and key hashes, assuming roughly uniform latency between any two nodes. On a Mehr mesh:

- A node 1 XOR-hop away might be 10 LoRa hops away
- A node 10 XOR-hops away might be a direct WiFi neighbor
- Link quality varies by orders of magnitude

MHR-DHT uses **proximity-weighted gossip** that considers both XOR distance and actual network cost when deciding where to route lookups. XOR distance determines the **target** (which nodes should store a key); network cost determines the **path** (how to reach those nodes efficiently).

## Routing Algorithm

### Lookup Scoring Function

Each DHT lookup hop selects the next node by minimizing:

```
dht_score(candidate, key) = w_xor × norm_xor_distance(candidate.id, key)
                          + (1 - w_xor) × norm_network_cost(candidate)
```

Where:
- `norm_xor_distance` = `xor(candidate.id, key) / max_xor_in_candidate_set`, normalized to [0, 1]
- `norm_network_cost` = `cumulative_cost_to(candidate) / max_cost_in_candidate_set`, normalized to [0, 1]
- `w_xor = 0.7` (default — favor key-space closeness, but avoid expensive paths)

This produces the same iterative-closest-node behavior as Kademlia but routes around expensive links rather than blindly following XOR distance.

> **Specification**
DHT lookups minimize `w_xor × norm_xor_distance + (1 - w_xor) × norm_network_cost` with default `w_xor = 0.7`. XOR distance determines the **target** (who stores the data); network cost determines the **path** (how to reach them efficiently).

### Replication Factor

Each key is stored on the **k=3 closest nodes** in XOR distance that are reachable within a cost budget:

```
Storage responsibility:
  1. Sort all known nodes by xor(node.id, key)
  2. Walk the sorted list; skip nodes whose network cost exceeds 10× the cheapest
  3. First k=3 reachable nodes are the storage set
```

The cost filter prevents a node on the far side of a LoRa link from being assigned storage responsibility for a key it can barely reach. The XOR ordering ensures deterministic agreement on who stores what.

### Rebalancing

- **Node join**: A new node announces itself. Existing nodes check whether any stored keys are now closer (in XOR) to the new node. After **2 gossip rounds** (for announcement convergence), affected keys are pushed via gossip metadata. The new node pulls full data and becomes part of the storage set.
- **Node departure**: Detected via missed heartbeats (3 consecutive gossip rounds = ~3 minutes). The departed node is immediately marked down — no new writes are sent to it. After **6 additional missed rounds** (~6 minutes total since last response), remaining storage-set members initiate re-replication to the next-closest reachable node, restoring k=3. If no reachable replacement exists within the cost budget, durability is temporarily degraded (k=2 or k=1) and a warning is logged. Normal replication resumes when a suitable node becomes available.

## Lookup Process

```
DHT Lookup:
  1. Query direct neighbors for the key
  2. Each responds with either the data or a referral to a closer node
     (selected by dht_score — balancing XOR closeness and network cost)
  3. Follow referrals iteratively until data is found or all k closest nodes queried
  4. Cache result locally with TTL
  5. Parallel lookups: query α=3 nodes concurrently, use first valid response
```

### Bandwidth per Lookup

| Component | Size |
|-----------|------|
| Query | ~64 bytes |
| Response (referral) | ~48 bytes (node_id + cost hint) |
| Response (data found) | ~128 bytes + data size |
| Typical lookup (3-5 hops on LoRa) | 2-3 seconds |

## Publication Process

```
DHT Publication:
  1. Store the object locally
  2. Gossip key + metadata (not full data) to neighbors
  3. Nodes close to the key's hash (k=3 storage set) pull the full data
  4. Neighborhood-scoped objects gossip within the trust neighborhood only
```

Publication gossips only metadata — the full data is pulled on demand. This prevents large objects from flooding the gossip channel.

### Metadata Format

```
DHTMetadata {
    key: [u8; 32],           // Blake3 content hash (32 bytes)
    size: u32,               // object size in bytes (4 bytes)
    content_type: u8,        // 0=Immutable, 1=Mutable, 2=Ephemeral (1 byte)
    owner: [u8; 16],         // publisher's destination hash (16 bytes)
    ttl_remaining: u32,      // seconds until expiry (4 bytes)
    lamport_ts: u64,         // publisher's Lamport timestamp (8 bytes, for mutable ordering)
    signature: [u8; 64],     // Ed25519 signature over (key || size || content_type || lamport_ts) (64 bytes)
}
// Total: 129 bytes per metadata entry
// Signature prevents metadata forgery; content hash prevents data forgery
```

For **mutable objects**, `lamport_ts` determines freshness — the highest timestamp with a valid signature wins. For **immutable objects**, `lamport_ts` is the publication time and the content hash is sufficient for verification.

**Cache invalidation**: Mutable objects are invalidated by receiving a metadata entry with a higher `lamport_ts` for the same `owner` and logical key. There is no push-invalidation mechanism — caches rely on TTL expiry and periodic re-query of the storage set for freshness-critical data.

## Cache TTL

Cache lifetime follows a two-level policy:

- **Publisher TTL**: Set by the data owner. Maximum lifetime for the cached copy. Range: 60 seconds to 30 days.
- **Local cap**: `min(publisher_ttl, 24 hours)`. Prevents stale caches from persisting when publishers update their data.
- **Access refresh**: Accessing a cached item resets its local TTL to `min(remaining_publisher_ttl, 24 hours)`. Frequently accessed items stay cached; idle items expire.
- **Eviction**: When local cache exceeds its storage budget, least-recently-used entries are evicted first regardless of remaining TTL.

## Neighborhood-Scoped DHT

Objects can be scoped to a [trust neighborhood](../economics/trust-neighborhoods), meaning:

- Their metadata only gossips between trusted peers and their neighbors
- Only nodes within the trust neighborhood can discover them
- Storage nodes within the neighborhood are preferred
- Cross-neighborhood lookups require explicit queries (Ring 3 discovery)

This is useful for community content that doesn't need global visibility. Scoping emerges naturally from the trust graph — there is no explicit "zone" to configure.

## Caching

Lookup results are cached locally with a TTL (time-to-live). This means:

- Frequently accessed data is served from local cache
- The DHT is queried only when the cache expires
- Popular content naturally distributes across many caches
- Cache TTL is set by the data publisher

## Light Client Verification

Mobile nodes and other constrained devices delegate DHT lookups to a nearby relay rather than participating in the DHT directly. This creates a trust problem: how does the light client know the relay's response is honest?

Three verification tiers handle this, scaled by data type:

### Tier 1 — Content-Addressed Lookups (Zero Overhead)

Most DHT objects are stored by content hash. Verification is automatic:

```
Light client lookup by hash:
  1. Request key K from relay
  2. Relay returns data D
  3. Verify: Blake3(D) == K
  4. Match → data is authentic (relay honesty irrelevant)
  5. Mismatch → discard, flag relay, retry via different node
```

No extra bandwidth, no extra queries. The hash the client already knows is the proof.

### Tier 2 — Signed Object Lookups (Signature Check)

For mutable data (MHR-Name records, capability advertisements, profile updates), objects carry the owner's Ed25519 signature:

```
Light client lookup for mutable object:
  1. Request mutable key from relay
  2. Relay returns: { data, owner_pubkey, signature, lamport_timestamp }
  3. Verify: Ed25519_verify(owner_pubkey, data || timestamp, signature)
  4. Valid → data is authentic (relay cannot forge owner's signature)
  5. Invalid → discard, flag relay
```

A malicious relay can return **stale but validly signed** data. It cannot forge new data. Staleness is handled by Tier 3.

### Tier 3 — Multi-Source Queries (Anti-Censorship, Anti-Staleness)

For lookups where censorship or staleness matters, the client queries multiple independent nodes:

```
Multi-source lookup (quorum_size N):
  1. Send lookup to N independent nodes (relay + N-1 others from Ring 0/1)
  2. Collect responses with timeout
  3. Content-addressed: any valid response is sufficient
  4. Mutable: highest lamport_timestamp with valid signature wins
  5. "Not found" accepted only if unanimous across all N
  6. Divergent results: flag dissenting node(s), trust majority
```

Default quorum sizes:

| Lookup Type | Default N | Notes |
|------------|----------|-------|
| Content-addressed | 1 | Hash verification is sufficient |
| Name resolution | 2 | First resolution of unknown name uses N=3 |
| Mutable object | 2 | N=3 if freshness is critical |
| Service discovery | 1 | N=2 if results seem incomplete |

### Trusted Relay Shortcut

If the client's relay is in its [trust graph](../economics/trust-neighborhoods), single-source queries (N=1) are sufficient for all tiers. Multi-source queries are only needed for untrusted relays. A trusted relay has economic skin in the game — trust means absorbing the trusted node's debts, making dishonesty self-punishing.

### Overhead

| Scenario | Extra Queries | Extra Bandwidth |
|----------|--------------|----------------|
| Content-addressed, any relay | 0 | 0 |
| Mutable, trusted relay | 0 | 0 |
| Mutable, untrusted relay | +1 | ~192 bytes |
| Name resolution | +1 | ~192 bytes |

<!-- faq-start -->

## Frequently Asked Questions

<details className="faq-item">
<summary>What is a DHT and why does Mehr need one?</summary>

A Distributed Hash Table (DHT) is a decentralized lookup system — given a key (like a content hash), it tells you which nodes store the corresponding data. Mehr needs it so any node can find any piece of data without asking a central server. Think of it as a phone book that’s spread across everyone’s devices instead of sitting in one place.

</details>

<details className="faq-item">
<summary>How do lookups work if some nodes are offline?</summary>

Each key is stored on k=3 nodes for redundancy. If one node is offline, the other two can still serve the data. Lookups query multiple nodes in parallel (α=3 concurrently) and use the first valid response. If a node goes offline for too long (~6 minutes), the remaining storage nodes re-replicate the data to a replacement, restoring full redundancy.

</details>

<details className="faq-item">
<summary>Are there limits on how much data I can store in the DHT?</summary>

The DHT itself stores only lightweight metadata (~129 bytes per entry) — not the full data. Full data lives in MHR-Store. The metadata includes the content hash, size, owner, and a signature. There’s no hard per-node limit, but cache eviction uses least-recently-used policy when local storage is full, and TTLs ensure stale entries expire.

</details>

<details className="faq-item">
<summary>Can a malicious node return fake data from a DHT lookup?</summary>

For content-addressed lookups (most common), fakery is impossible — the hash you already know *is* the proof. If `Blake3(returned_data) != requested_hash`, the data is rejected. For mutable data, the owner’s Ed25519 signature prevents forgery. A malicious relay can return stale data but cannot fabricate new data. Multi-source queries catch staleness.

</details>

<!-- faq-end -->

---

### MHR-ID: Verification & Linking
<!-- Source: docs/services/mhr-id/verification.md -->

# MHR-ID: Verification & Linking

## Identity Linking

ExternalIdentity claims (type 4) link your Mehr identity to accounts on external platforms. Verification uses two methods inspired by [FUTO ID](https://docs.polycentric.io/futo-id/#identity-linking): crawler challenges and OAuth challenges.

### Enhanced ExternalIdentity

```
ExternalIdentity {
    platform: String,                      // "github", "twitter", "mastodon", etc.
    handle: String,                        // username on that platform
    challenge: Option<IdentityChallenge>,  // verification evidence (None = unverified)
}

IdentityChallenge {
    method: u8,                            // 0=CrawlerChallenge, 1=OAuthChallenge
    challenge_hash: Blake3Hash,            // Blake3 hash of the challenge string
    verified_by: Option<NodeID>,           // oracle that performed verification
    verified_at: Option<u64>,              // epoch when verified
}
```

### Crawler Challenge

The user posts a signed challenge string on their external platform profile, and a verification oracle crawls the platform to confirm it.

```
Crawler challenge flow:
  1. User generates challenge string:
       "mehr-id:<NodeID_hex>:<nonce>:<signature>"
     where signature = Ed25519Sign(NodeID || platform || handle || nonce)

  2. User posts challenge string on their platform profile/bio/gist/post

  3. User publishes ExternalIdentity claim with:
       challenge.method = 0 (CrawlerChallenge)
       challenge.challenge_hash = Blake3(full challenge string)

  4. Verification oracle (gateway node with internet) crawls the platform URL

  5. Oracle verifies:
       - Challenge string contains the correct NodeID
       - Ed25519 signature is valid for the claimant's public key
       - Platform handle matches the claim

  6. Oracle publishes a Vouch for the claim with high confidence (200–255)

  7. Multiple independent oracles increase confidence
```

### OAuth Challenge

The user authenticates with the external platform via an OAuth flow mediated by a verification oracle. The oracle never receives the user's platform password.

```
OAuth challenge flow:
  1. User connects to a verification oracle (gateway node with internet + OAuth config)
  2. Oracle redirects user to platform's OAuth authorization page
  3. User authenticates directly with platform (password never touches oracle)
  4. Platform confirms identity to oracle via OAuth token
  5. Oracle verifies platform username matches the ExternalIdentity claim handle
  6. Oracle publishes a Vouch for the claim with high confidence (200–255)
```

### Verification Oracles

Verification oracles are **regular gateway nodes** — not special infrastructure. They:

- Advertise `Capability(verification_oracle, ...)` in their own IdentityClaims
- Have internet access (gateway tier or higher)
- Run crawler and/or OAuth verification software
- Publish vouches like any other peer
- Are subject to the same trust graph — a vouch from a trusted oracle carries more weight than one from an unknown oracle

No single oracle is authoritative. Multiple independent oracles vouching for the same ExternalIdentity claim increases confidence through the standard vouch aggregation mechanism.

### Self-Verification (No Oracle)

A user can verify their own ExternalIdentity without any oracle:

1. Post the crawler challenge string on the external platform
2. Publish the ExternalIdentity claim with the challenge hash
3. Include the platform profile URL in the claim data
4. Any peer with internet access can manually visit the URL and verify the challenge string
5. Peers who verify it publish vouches directly

This works without any oracle infrastructure — just normal peer vouching applied to a publicly visible challenge.

### Supported Platforms

Platforms are free-form strings. These are conventions clients should recognize:

| Platform | Crawler URL Pattern | OAuth Support |
|----------|-------------------|---------------|
| `github` | `github.com/{handle}` (bio or gist) | Yes |
| `twitter` | `twitter.com/{handle}` (bio) | Yes |
| `mastodon` | `{instance}/@{handle}` (bio) | Yes |
| `reddit` | `reddit.com/user/{handle}` (bio) | Yes |
| `keybase` | `keybase.io/{handle}` | No (crawler only) |
| `dns` | TXT record at `{handle}` domain | No (crawler only) |

The `dns` platform enables domain verification — post a TXT record containing the challenge string at your domain, proving you control the domain.

## Verification Methods

```
                    Verification Hierarchy

  Country ─── aggregation of regions ──────────────── Lowest precision
     │
  Region ──── aggregation of cities
     │
  City ────── aggregation of neighborhoods
     │
  Neighborhood ── RadioRangeProof (LoRa beacons) ──── Highest precision
     │
  ┌──┴────────────────────────────────────────┐
  │  [Alice]  ···radio···  [Bob]              │
  │     │                    │                │
  │   witness              witness            │
  │     │                    │                │
  │     └──── [Prover] ─────┘                 │
  │           broadcasts                      │
  │           signed beacon                   │
  └───────────────────────────────────────────┘
```

### RadioRangeProof

The mesh-native equivalent of physical presence verification. If you can hear a node's LoRa radio, you're within physical range.
> **Specification**
`RadioRangeProof` uses existing presence beacons (broadcast every 10 seconds) as the proof mechanism. Nearby nodes sign witness attestations with RSSI and SNR, triangulating the prover’s approximate position. It runs on constrained devices (ESP32) with no heavy crypto or GPU.

```
RadioRangeProof {
    prover: NodeID,                 // node proving presence
    witnesses: Vec<Witness>,        // nodes that heard the prover
    timestamp: Timestamp,
}

Witness {
    node_id: NodeID,
    rssi: i8,                       // received signal strength (dBm)
    snr: i8,                        // signal-to-noise ratio (dB)
    signature: Ed25519Sig,          // witness signs the observation
}
```

**How it works:**

1. Node broadcasts a signed presence beacon on LoRa (this already happens every 10 seconds via [presence beacons](../../marketplace/discovery#presence-beacons))
2. Nearby nodes that receive the beacon can sign a Witness attestation: "I heard this node at this signal strength at this time"
3. Multiple witnesses from known locations triangulate the prover's approximate position
4. Witnesses with verified GeoPresence claims for the same area provide stronger attestation

**Range and precision:**

| Transport | Typical Range | Position Precision |
|-----------|-------------|-------------------|
| LoRa (rural) | 5–15 km | City/neighborhood level |
| LoRa (urban) | 1–5 km | Neighborhood level |
| WiFi | 30–100 m | Building level |
| Bluetooth | 10–30 m | Room level |

RadioRangeProof verifies **neighborhood-level** physical geo claims. It cannot verify city, region, or country claims directly — those use bottom-up aggregation. It also cannot verify virtual geo scopes (game servers, organizations) — those use application-specific verification such as server-signed attestations, admin vouches, or invite-chain proofs, handled at the application layer.

### Bottom-Up Aggregation (Physical Geo Scopes)

Higher-level physical geo claims are verified by aggregating verified sub-scope claims:

```
Verification levels:

Neighborhood: RadioRangeProof
  "I'm in Hawthorne" ← proved by radio witnesses in Hawthorne

City: Aggregation of neighborhoods
  "Portland exists" ← N nodes have verified claims for Portland neighborhoods
  (hawthorne + pearl + alberta + ... = Portland)

Region: Aggregation of cities
  "Oregon exists" ← nodes have verified claims across Portland, Eugene, Bend

Country: Aggregation of regions
  And so on upward.
```

No single node proves "I'm in Oregon." The **network** proves Oregon exists collectively because many nodes have independently verified neighborhood-level presence across Oregon's geography. This is inherently Sybil-resistant — you can't fake physical presence across multiple locations simultaneously.
> **Key Insight**
Geographic verification is bottom-up: neighborhood claims are machine-verified via RadioRangeProof, then aggregated upward into city, region, and country claims. No single node proves a broad claim — the network collectively establishes geographic truth through independent neighborhood-level proofs.

**Aggregation thresholds:**

| Level | Minimum Verified Sub-claims | Description |
|-------|---------------------------|-------------|
| City | 3+ verified neighborhoods | At least 3 distinct neighborhood clusters |
| Region | 2+ verified cities | At least 2 cities with verified neighborhoods |
| Country | 2+ verified regions | At least 2 regions with verified cities |

These thresholds are intentionally low — the system bootstraps from small meshes. As the network grows, the aggregation becomes denser and more trustworthy naturally.

### Peer Attestation

For claims that can't be machine-verified, trusted peers vouch based on personal knowledge:

```
Alice knows Bob is her neighbor:
    → Alice vouches for Bob's GeoPresence("...", "portland", "hawthorne")
    → Alice's vouch weight = her trust score relative to the verifier

Dave knows Eve runs a reliable relay:
    → Dave vouches for Eve's Capability(relay, ...)
    → Dave's vouch weight = his trust score relative to the verifier
```

Peer attestation is the **fallback** for everything. RadioRangeProof automates geographic verification, proof-of-service automates capability verification, but peer attestation always works — even for claims no machine can verify ("this person is a good curator").

### Trust Graph Corroboration

When the trust graph around a claimant is consistent with the claim, that's evidence the claim is legitimate — even without machine verification. This applies to **all claim types**, not just GeoPresence.

**GeoPresence**: If a node's trusted peers all have verified GeoPresence for Portland, and the node claims Portland too, the trust graph corroborates the claim — even without a RadioRangeProof.

**CommunityMember**: If a node claims `Topic("gaming", "pokemon")` and its trusted peers all have the same community claim, the trust graph corroborates membership — the node is embedded in that community.

**Capability**: If a node claims relay capability and its trusted peers have forwarded traffic through it successfully, the trust graph reflects real service history.

**ExternalIdentity**: If multiple trusted peers have independently vouched for the same ExternalIdentity claim (even without oracle verification), the social corroboration is meaningful.

```
Trust graph corroboration example (GeoPresence):

  Alice trusts: Bob, Carol, Dave, Eve (all verified Geo("portland"))
  Frank claims: Geo("portland"), no RadioRangeProof yet

  Alice's view of Frank's claim:
    - Frank is trusted by Bob and Carol (friend-of-friend)
    - Bob and Carol both have verified Portland claims
    - Frank's GeoPresence is corroborated: his trusted peers
      are in the same place he claims to be

  Corroboration score:
    count of trusted peers with verified matching claims
    ────────────────────────────────────────────────────
    total trusted peers who vouch for the claim
```

Trust graph corroboration is weaker than machine verification (a remote attacker could build trust relationships with Portland nodes without being in Portland). But it provides useful signal, especially during network bootstrap when RadioRangeProof witnesses or oracle infrastructure may be sparse. Nodes can weight corroboration below machine verification but above bare self-attestation in their local verification scoring.

### Transitive Confidence

Vouch weight decays with trust distance, following the same model as [transitive credit](../../economics/trust-neighborhoods#trust-based-credit):

```
Vouch from direct trusted peer:      confidence × 1.0
Vouch from friend-of-friend:         confidence × 0.1
Vouch from 3+ hops away:             ignored (0 weight)
```

This means a node calculates the **effective verification level** of any claim by summing trust-weighted vouches from its own perspective. Different nodes may see different verification levels for the same claim, depending on their position in the trust graph. This is by design — there is no global authority on what's verified.

---

### MHR-App: Security
<!-- Source: docs/services/mhr-app/security.md -->

# MHR-App: Security Considerations

## Publisher Authentication

Every AppManifest is signed by the publisher's Ed25519 key. The publisher's identity is verifiable via [MHR-ID](../mhr-id/) claims and vouches. A manifest from an unknown or untrusted publisher triggers a trust warning at install time. Trust scoring follows the same model as [MHR-Name resolution](../mhr-name#resolution-priority).

## Content Integrity

Every component is content-addressed by Blake3 hash — contract code, UI assets, state schema, and the manifest itself. A malicious storage node cannot tamper with any component without changing its hash, which breaks the manifest reference. Verification is automatic and requires no trust in storage providers.

## Malicious Code

MHR-Byte and WASM contracts are fully sandboxed — no I/O, no network, no filesystem access. Malicious contract code cannot escape the compute sandbox. UI code (HTML/JS) is **not sandboxed** by the protocol — it runs in the user's local rendering environment. Mitigations: content-hash verification ensures UI hasn't been tampered with, and trusted publisher vouches via MHR-ID provide social proof of safety.

## Supply Chain Attacks

A compromised publisher key could push a malicious update. Mitigations:
- Old versions remain available by content hash (immutable)
- Key rotation via [MHR-ID](../mhr-id/) revokes the compromised key
- Users can pin to a specific manifest hash (version pinning)
- Publisher key change triggers a warning (like SSH host key warnings)

## Dependency Integrity

Dependencies are resolved by content hash — a dependency on `Blake3(0x1a2b...)` always resolves to the exact same bytes regardless of where it's fetched from. An attacker cannot substitute a malicious dependency without changing the hash, which would break the manifest's dependency list.

## State Poisoning

A malicious node could inject corrupted CRDT state during sync. Mitigations: contracts validate state transitions (invalid state is rejected), CRDT merge semantics are deterministic (invalid state that passes validation merges consistently), and state mutations are signed by the authoring node.

## Comparison with Other Frameworks

| | Mehr | Freenet | Holochain | Ethereum |
|---|---|---|---|---|
| **State model** | CRDT (eventually consistent) | Contracts (per-key replicated) | Agent-centric (source chains) | Global state (blockchain) |
| **Compute** | Explicit, paid, no global state | Implicit in storage ops | Validation functions | Global EVM |
| **Consensus** | None (CRDT convergence) | None (contract logic) | Per-app validation | Global PoS |
| **Storage** | Paid per-duration | Donated | Agent-hosted | On-chain (expensive) |
| **Hardware** | ESP32 to datacenter | Desktop+ | Desktop+ | Full node required |
| **Offline** | Full partition tolerance | Limited | Offline-first | No |
| **App packaging** | AppManifest (content-addressed) | Contract + State (content-addressed) | DNA + UI bundle (hApp) | No standard (dApps are websites) |
| **App discovery** | MHR-Name + trust-weighted | Key-based (must know contract key) | App store / out-of-band | Out-of-band (URLs) |
| **Private local agent** | Node identity + local storage | Delegate (actor model) | Agent (source chain) | Wallet (external) |
| **UI distribution** | DataObjects in MHR-Store | Contract state (WASM-rendered) | Bundled in hApp | Traditional web hosting |
| **Upgrade model** | New manifest + name rebind | New contract key (no migration) | DNA versioning | Contract is immutable |
| **Dependencies** | Content-addressed by hash | Not formalized | Zome composition | Contract composability |

## What Mehr Does NOT Provide

- **No global state machine** — no blockchain, no global consensus. Applications that need "everyone agrees on one truth" must use CRDTs (eventual consistency) or application-level coordination.
- **No automatic code execution at storage nodes** — storage is dumb. A storage node stores bytes and serves them on request. It does not execute contracts on stored data. Compute is always explicit and paid.
- **No contract composability** — contracts don't call other contracts. Each contract is an independent unit of execution. Applications compose at the application layer, not the contract layer. Apps can share contracts via dependencies, but contracts cannot invoke each other at runtime.
- **No transaction atomicity across nodes** — you cannot atomically update state on two different nodes. CRDTs provide eventual consistency, not transactional guarantees.
- **No curated app store** — app discovery is decentralized and trust-weighted. There is no central authority that reviews, approves, or ranks applications. Community curation happens organically through vouches and trust relationships.

These are deliberate: global state and atomic transactions require consensus, which contradicts partition tolerance. Mehr chooses partition tolerance and eventual consistency over global coordination — the right tradeoff for a mesh network where disconnection is normal.

<!-- explanation-start -->

#### Design Decision: No Delegate Concept {#design-decision-no-delegate-concept}

>
[Freenet](https://freenet.org) uses a three-component model: **contracts** (public replicated state), **delegates** (private local agents holding secrets), and **UIs** (web frontends). The delegate acts as a local policy enforcer — it holds private keys, manages per-app secrets, and communicates via actor-model message passing.

Mehr does not need a separate delegate concept. The same functionality is covered by existing primitives:

| Freenet Delegate Feature | Mehr Equivalent |
|--------------------------|-----------------|
| Hold private keys | Node's Ed25519 identity + local keystore |
| Per-app secret state | Local node storage (never replicated) |
| Policy enforcement | MHR-Compute contracts running locally |
| Message passing | MHR-Pub subscriptions + direct messages |
| Authorized actions | Contract logic checks `SENDER` opcode |

A Mehr app that needs private state stores it locally on the node — it never enters MHR-Store, is never gossiped, and is never visible to other nodes. Contracts running locally via MHR-Compute have no I/O or network access, providing the same sandboxing guarantees as Freenet delegates. Adding a formal delegate concept would introduce a new abstraction layer with minimal benefit over the existing primitives.


<!-- explanation-end -->

---

### MHR-ID: Mobility & Integration
<!-- Source: docs/services/mhr-id/mobility.md -->

# MHR-ID: Mobility & Integration

## Geographic Mobility

What happens when you move from Portland to Tehran? Your cryptographic identity stays the same — [roaming](../../applications/roaming) handles the transport layer seamlessly. But your GeoPresence claim, name bindings, and local trust relationships all need to adapt.

### Moving Between Locations

```
Alice moves from Portland to Tehran:

  1. UPDATE GEO SCOPE
     TrustConfig.scopes: Geo("portland") → Geo("tehran", "district-6")
     Publish new GeoPresence claim for Tehran (sequence+1)

  2. OLD CLAIMS FADE
     Portland GeoPresence vouches expire naturally (30 epochs)
     Portland name binding (alice@geo:portland) expires unless renewed
     Portland trusted peers still trust Alice — trust is location-independent

  3. NEW CLAIMS BUILD
     Tehran peers hear Alice's announces, witness RadioRangeProof
     Alice builds trust relationships with Tehran nodes
     Alice registers alice@geo:tehran
     Trust graph corroboration: Portland friends who trust Alice
       + Tehran peers who witness her presence = strong corroboration

  4. CROSS-LOCATION TRUST PERSISTS
     Portland friends can still message Alice (routed via mesh)
     Alice's reputation, vouches she gave, payment channels — all intact
     Only geo-scoped privileges change (can't vote on Portland issues anymore)
```

### Nomadic Users

Not everyone has a fixed location. Digital nomads, traveling merchants, mobile relay operators, and people between homes may not want a Geo scope at all.

**No Geo scope**: A node can operate without any Geo scope. Trust relationships, payment channels, and Topic-scoped communities all work regardless of location. The node simply can't participate in geo-scoped voting or register geo-scoped names.

**Broad Geo scope**: A nomad can use a broad scope like `Geo("north-america")` or `Geo("asia")` — accurate but imprecise. This enables regional content feeds without claiming a specific city.

**Frequent updates**: A node that moves often can update its Geo scope and GeoPresence claim each time. The 30-epoch vouch expiry means old location claims fade naturally. Trusted peers who travel with the node (e.g., family members, convoy partners) can vouch for each new location.

**Mobile relays**: A relay mounted on a vehicle (bus, truck, boat) changes location continuously. It can either use a broad Geo scope or update its GeoPresence claim at each stop. Its value as a relay is proven by [proof-of-service](../../marketplace/verification), not by geographic stability — a mobile relay that reliably forwards traffic earns reputation regardless of where it is.

> **Trade-off**
Nomadic nodes sacrifice geo-scoped features (voting rights, local name bindings, local feed visibility) in exchange for location flexibility. A node without a Geo scope can fully participate in economics, trust, and Topic-scoped communities — but cannot vote on Portland issues or register `alice@geo:portland`.

### Trust Portability

Trust relationships are **location-independent** — they are between identity keys, not between places. When Alice moves from Portland to Tehran:

- Bob in Portland still trusts Alice. His `trusted_peers` set contains Alice's NodeID, not "Alice in Portland."
- Alice can still use credit lines from Portland peers for Tehran-bound traffic
- Vouches Alice gave to Portland peers remain valid (until expiry)
- Alice's verification history travels with her key — new Tehran peers can see she was previously verified in Portland

The only things that change are **geo-scoped privileges**: geo-scoped voting eligibility, geo-scoped name bindings, and which local feeds her content appears in. Everything else — trust, reputation, payment channels, Topic-scoped communities — is portable.

> **Key Insight**
Trust relationships are between identity keys, not between places. When you move cities, your entire trust graph, reputation history, and payment channels travel with you. Only geo-scoped privileges (local voting, local name bindings, local feed placement) change.

## Integration with Existing Systems

### Reputation

Claims feed into the existing [reputation system](../../protocol/security#sybil-resistance):

```
PeerReputation additions:
    claim_verification_level: u8,   // 0-255: aggregate verification score
    vouch_count: u16,               // number of active vouches received
```

The `claim_verification_level` is computed locally by each node based on the trust-weighted vouches it sees. A node with high verification level + high service reputation is the most trustworthy participant in the network.

### Key Rotation

The `KeyRotation` claim type works alongside the existing [KeyCompromiseAdvisory](../../protocol/security#key-compromise-advisory):

```
Key migration flow:
  1. Generate new key pair
  2. Publish KeyRotation claim signed by BOTH old and new keys
     (equivalent to KeyCompromiseAdvisory with SignedByBoth evidence)
  3. Trusted peers vouch for the rotation
  4. Services (storage, compute, pub/sub) migrate agreements to new key
  5. Old key's reputation transfers to new key (trust-weighted)
```

KeyRotation claims with only the old key's signature are treated with suspicion (could be attacker with compromised key). Both-key signatures are strong evidence of legitimate migration.

### Naming

Geographic claims enable scoped naming: `alice@geo:portland` resolves only if Alice has a verified GeoPresence claim for Portland scope — see [MHR-Name](../mhr-name).

### Voting

Verified geographic claims are **prerequisites for geographic voting** — a node cannot vote on Portland issues without a verified Portland-area GeoPresence claim. See [Voting](../../applications/voting) for the eligibility model.

## Comparison with Other Identity Systems

| | FUTO ID (Polycentric) | Mehr Identity Claims |
|---|---|---|
| **Primary purpose** | Link centralized platform accounts | Verify mesh-native properties (location, service, community) + profile + platform linking |
| **Identity linking** | Crawler challenges + OAuth challenges | Same methods ([crawler](./verification.md#crawler-challenge) + [OAuth](./verification.md#oauth-challenge)), via decentralized verification oracles |
| **Verification** | Crawlers scrape platforms / OAuth challenges | RadioRangeProof / proof-of-service / peer attestation / crawler / OAuth |
| **Trust model** | PGP-style Web of Trust (binary vouch) | Trust-weighted vouches with transitive decay |
| **Visibility controls** | Not specified | [Per-claim visibility](./index.md#visibility-controls) — Public, TrustNetwork, DirectTrust, Named |
| **Profile fields** | Application-level | [Protocol-level claims](./index.md#profile-fields) with standard keys, vouchable |
| **Key recovery** | None | KeyRotation claim (signed by both keys) |
| **Internet required** | Yes (must reach platforms) | No (works on LoRa mesh with zero internet; identity linking needs internet) |
| **Geographic proof** | Not supported | RadioRangeProof via LoRa beacon witnesses |
| **Sybil resistance** | Social (number of vouches) | Economic (trust = absorb debts) + social (vouches) |
| **Confidence** | Binary (vouched or not) | Graduated (0–255 confidence × trust distance decay) |

---

### MHR-Pub: Publish/Subscribe
<!-- Source: docs/services/mhr-pub.md -->

# MHR-Pub: Publish/Subscribe

MHR-Pub provides a publish/subscribe system for real-time notifications across the mesh. It supports multiple subscription types and delivery modes, allowing applications to choose the right tradeoff between immediacy and bandwidth.

## Subscriptions

```
Subscription {
    subscriber: NodeID,
    topic: enum {
        Key(hash),              // specific key changed
        Prefix(hash_prefix),    // any key with prefix changed
        Node(NodeID),           // any publication by this node
        Neighborhood(label),    // any publication in this community label (deprecated)
        Scope(ScopeMatch),      // any publication matching a hierarchical scope
    },
    delivery: enum {
        Push,                   // immediate, full payload
        Digest,                 // batched summaries, periodic
        PullHint,               // hash-only notification
    },
}

ScopeMatch {
    scope: HierarchicalScope,   // from trust-neighborhoods
    match_mode: enum {
        Exact,                  // this scope level only
        Prefix,                 // this scope and all children
    },
}
```

## Subscription Topics

| Topic Type | Use Case |
|-----------|----------|
| **Key** | Watch a specific data object for changes (e.g., a friend's profile) |
| **Prefix** | Watch a category of keys (e.g., all posts in a forum) |
| **Node** | Follow all publications from a specific user |
| **Neighborhood** | Watch all activity from nodes with a given community label (deprecated — use Scope) |
| **Scope** | Watch all activity matching a [hierarchical scope](../economics/trust-neighborhoods#hierarchical-scopes) — geographic or interest |

## Delivery Modes

### Push

Full payload delivered immediately when published. Best for high-bandwidth links where real-time updates matter.

**Use on**: WiFi, Ethernet, Cellular

### Digest

Batched summaries delivered periodically. Reduces bandwidth by aggregating multiple updates into a single digest.

**Use on**: Moderate bandwidth links, or when real-time isn't critical

### PullHint

Only the hash of new content is delivered. The subscriber decides whether and when to pull the full data.

**Use on**: LoRa and other constrained links where bandwidth is precious

> **Key Insight**
Three delivery modes — Push, Digest, PullHint — let the application match notification overhead to link capacity. LoRa nodes receive 32-byte hash hints; WiFi nodes get full payloads. The protocol provides the tools; the application chooses the tradeoff.

## Application-Driven Delivery Selection

Delivery mode selection is the **application's responsibility**, informed by link quality. The protocol provides tools; the application decides:

```
// Application code (pseudocode)
let link = query_link_quality(publisher_node);

if link.bandwidth_bps > 1_000_000 {
    subscribe(topic, Push);       // WiFi: get everything immediately
} else if link.bandwidth_bps > 10_000 {
    subscribe(topic, Digest);     // moderate: batched summaries
} else {
    subscribe(topic, PullHint);   // LoRa: just tell me what's new
}
```

The pub/sub system doesn't make this decision — the application does, based on `query_link_quality()` from the capability layer.

## Bandwidth Characteristics

| Delivery Mode | Per-notification overhead | Suitable for |
|--------------|-------------------------|-------------|
| Push | Full object size | WiFi, Ethernet |
| Digest | ~50 bytes per item (hash + summary) | Moderate links |
| PullHint | ~32 bytes (hash only) | LoRa, constrained links |

### Envelope-Aware Delivery

For [Social](../applications/social) content, MHR-Pub notifications carry the [PostEnvelope](../applications/social#postenvelope-free-layer) — the free preview layer — rather than the full post:

| Delivery Mode | What's Delivered | Reader Cost |
|--------------|-----------------|-------------|
| Push | Full PostEnvelope (~300-500 bytes) | Free |
| Digest | Batched envelopes (headline + hash per item) | Free |
| PullHint | Post hash only (32 bytes) | Free (envelope fetched on demand) |

In all modes, the reader browses envelopes at zero cost. Fetching the full [SocialPost](../applications/social#socialpost-paid-layer) content is a separate, paid retrieval from the storage node. This separation means even LoRa nodes on PullHint subscriptions can browse headlines and decide what's worth fetching over a higher-bandwidth link later.

## Scope Subscriptions

Scope subscriptions are the primary mechanism for geographic feeds, interest feeds, and community content discovery. They build on [Hierarchical Scopes](../economics/trust-neighborhoods#hierarchical-scopes) to enable structured content routing.

### Geographic Feeds

Subscribe to content from a geographic area at any level of the hierarchy:

```
// All posts from Portland
subscribe(Scope(Geo("north-america", "us", "oregon", "portland"), Exact), Push);

// All posts from anywhere in Oregon
subscribe(Scope(Geo("north-america", "us", "oregon"), Prefix), Digest);
```

Geographic scope subscriptions naturally bias toward local content — nearby nodes have cheaper relay paths and higher cache density, so local content arrives faster and costs less.

### Interest Feeds

Subscribe to content by topic at any level of the hierarchy:

```
// All Pokemon content globally
subscribe(Scope(Topic("gaming", "pokemon"), Prefix), Digest);

// Only competitive Pokemon
subscribe(Scope(Topic("gaming", "pokemon", "competitive"), Exact), Push);
```

Interest subscriptions are **sparse** — they span geography. A Pokemon subscription connects Portland, Tokyo, and Berlin through interest relay nodes that bridge geographic clusters.

### Intersection Feeds

A client can subscribe to both a geographic and interest scope simultaneously and filter locally for the intersection:

```
// Subscribe to Portland content AND Pokemon content
subscribe(Scope(Geo("north-america", "us", "oregon", "portland"), Exact), Push);
subscribe(Scope(Topic("gaming", "pokemon"), Prefix), Push);

// Client-side: show posts that appear in BOTH feeds
// Result: Portland Pokemon community
```

Intersection is always client-side — the protocol delivers by individual scope, and the application composes.

### Scope Routing

When a node publishes with scopes, notifications propagate to subscribers at each level:

```
Post published with scopes:
  Geo("north-america", "us", "oregon", "portland", "hawthorne")
  Topic("gaming", "pokemon")

Notifications delivered to:
  Scope(Geo("...", "hawthorne"), Exact)     ✓  exact match
  Scope(Geo("...", "portland"), Prefix)      ✓  portland covers hawthorne
  Scope(Geo("...", "oregon"), Prefix)        ✓  oregon covers portland
  Scope(Topic("gaming", "pokemon"), Exact)   ✓  exact match
  Scope(Topic("gaming"), Prefix)             ✓  gaming covers pokemon
  Scope(Geo("...", "seattle"), Exact)        ✗  wrong city
  Scope(Topic("science"), Prefix)            ✗  wrong topic
```

### Delivery Mode by Scope Level

Applications should select delivery mode based on both link quality and scope breadth:

| Scope Level | Typical Volume | Recommended Default |
|-------------|---------------|-------------------|
| Neighborhood | Low | Push |
| City | Moderate | Push or Digest |
| Region | High | Digest |
| Country/Global | Very high | PullHint |
| Narrow interest topic | Low-moderate | Push |
| Broad interest topic | High | Digest |

## Security Considerations

<details className="security-item">
<summary>Topic Flooding / Subscription Spam</summary>

**Vulnerability:** A malicious node publishes at high volume to popular scopes, exhausting subscriber bandwidth or storage quotas.

**Mitigation:** Publishing costs relay fees (MHR). Subscribers filter by trust score — untrusted publishers' notifications are deprioritized or dropped. Scope-level rate limits cap notifications per epoch per publisher. The PullHint delivery mode ensures constrained links receive only 32-byte hashes, never full payloads, limiting the impact of flooding.

</details>

<details className="security-item">
<summary>Scope Impersonation</summary>

**Vulnerability:** A node publishes content tagged with a geographic or interest scope it doesn't genuinely belong to (e.g., claiming to be in Portland while physically elsewhere).

**Mitigation:** Geographic scopes are trust-weighted — content tagged with `geo:us/or/portland` only propagates if nodes in that scope trust the publisher. A remote attacker with no Portland trust edges gets near-zero visibility. Interest-based scopes are inherently open (anyone can join `topic:gaming`), but curation and trust-weighted ranking ensure quality.

</details>

<details className="security-item">
<summary>Notification Poisoning</summary>

**Vulnerability:** A relay delivers fake PullHint hashes that point to nonexistent or malicious content, wasting subscriber retrieval attempts and bandwidth.

**Mitigation:** Content-addressed lookups are self-verifying — if `Blake3(retrieved_data) != hint_hash`, the data is rejected. Subscribers track relay reliability; relays that consistently deliver invalid hints lose trust and are deprioritized. The cost of producing valid-looking but useless hashes is paid by the attacker in relay fees.

</details>

<!-- faq-start -->

## Frequently Asked Questions

<details className="faq-item">
<summary>How is MHR-Pub different from regular messaging?</summary>

Messaging is point-to-point: Alice sends a message to Bob. MHR-Pub is publish/subscribe: Alice publishes to a topic, and anyone subscribed to that topic receives the notification. It’s the backbone for feeds, real-time updates, sync notifications, and event-driven applications — one publisher can reach many subscribers without knowing who they are.

</details>

<details className="faq-item">
<summary>Can I subscribe to topics from communities across the mesh, not just my local area?</summary>

Yes. Scope subscriptions support both geographic and interest-based topics at any level of the hierarchy. You can subscribe to `Topic("gaming", "pokemon")` globally or `Geo("us", "oregon", "portland")` locally. Cross-scope subscriptions are bridged by interest relay nodes that connect geographic clusters. Content from distant scopes just costs more in relay fees.

</details>

<details className="faq-item">
<summary>Won’t pub/sub flood my LoRa link with notifications?</summary>

No. MHR-Pub offers three delivery modes precisely for this. On LoRa, you’d use **PullHint** — only a 32-byte hash is delivered per notification. You decide what’s worth fetching later over a faster link. On WiFi, you can use **Push** for full payloads. The application selects the mode based on current link quality, so constrained links are never overwhelmed.

</details>

<details className="faq-item">
<summary>What happens to notifications if I’m offline?</summary>

Notifications are not stored indefinitely by the pub/sub system — it’s real-time delivery. However, applications handle offline scenarios by storing state in MHR-Store. When you come back online, your node queries MHR-Store and MHR-DHT to catch up on missed updates. For social feeds, the envelope/post architecture means you can quickly scan what’s new without downloading everything.

</details>

<!-- faq-end -->

---

### MHR-App: FAQ
<!-- Source: docs/services/mhr-app/faq.md -->

# MHR-App: Frequently Asked Questions

<details>
<summary><strong>How is an "app" different from a smart contract?</strong></summary>

A Mehr app is a **composition** of primitives — state (CRDTs in MHR-Store), logic (contracts in MHR-Compute), events (MHR-Pub), identity, and discovery. A smart contract is just the logic piece. An app bundles everything needed to run into a single installable [AppManifest](./index.md#appmanifest), including UI, state schema, event topics, and dependencies.

</details>

<details>
<summary><strong>Can apps call each other's contracts?</strong></summary>

No. Contracts are independent execution units — they cannot invoke each other at runtime. Apps **compose at the application layer**: an app can depend on another app's contract code (shared via content-addressed hashes in the dependency list), but the calling app runs the contract locally. There is no cross-contract call stack.

</details>

<details>
<summary><strong>What happens if an app publisher disappears?</strong></summary>

The app continues to work. AppManifests, contracts, and state are all **content-addressed and immutable** — they don't depend on the publisher being online. Users who have installed the app keep their local copy. Other nodes can serve the app's artifacts from MHR-Store. The only thing that stops working is **upgrades** — no new versions will be published. Users can fork the app by publishing a new manifest referencing the same contracts.

</details>

<details>
<summary><strong>Why not use a delegate/agent model like Freenet or Holochain?</strong></summary>

Mehr's existing primitives already cover the delegate's responsibilities: the node's Ed25519 identity handles keys, local node storage handles secrets, MHR-Compute contracts handle policy enforcement, and MHR-Pub handles message passing. Adding a formal delegate concept would create a new abstraction layer with minimal benefit. See the [design decision](./security.md#design-decision-no-delegate-concept) for details.

</details>

<details>
<summary><strong>Can I run an app on an ESP32?</strong></summary>

Yes, if the app uses MHR-Byte contracts (compute_tier=1). MHR-Byte is Mehr's stack-based VM designed for constrained devices. Apps requiring WASM need at least a Community-tier device (Raspberry Pi Zero 2W or equivalent). If your device is underpowered, you can delegate contract execution to a more capable node via the [capability marketplace](../../marketplace/overview).

</details>

<details>
<summary><strong>How do app upgrades work across partitions?</strong></summary>

A partitioned node that missed an upgrade continues running the old version. On reconnection, CRDT state merges normally if the schema is compatible. If the schema change is breaking, the node detects the version mismatch during sync, fetches the new manifest via MHR-Name, and runs the migration contract locally. State then converges. See [Partition Behavior](./upgrades.md#partition-behavior) for details.

</details>

<details>
<summary><strong>Is there an app store?</strong></summary>

Not a centralized one. App discovery is decentralized and trust-weighted. Subscribe to `Scope(Topic("apps"), Prefix)` via MHR-Pub to receive announcements of new apps. Apps from trusted publishers rank higher in resolution. There is no central authority that reviews, approves, or ranks applications — community curation happens organically through vouches and trust relationships.

</details>

<details>
<summary><strong>What if a schema migration fails?</strong></summary>

All migration failures are recoverable. If the migration contract aborts, times out, or produces invalid output, the old app version remains functional. There is no automatic rollback — users keep using the old version, and the publisher can release a fix. See [Migration Contract Execution Semantics](./upgrades.md#migration-contract-execution-semantics) for the full failure mode list.

</details>

---

### MHR-Compute: Contract Execution
<!-- Source: docs/services/mhr-compute.md -->

# MHR-Compute: Contract Execution

MHR-Compute provides a restricted execution environment for data validation, state transitions, and access control. It supports two execution tiers: MHR-Byte (a minimal bytecode for constrained devices) and WASM (for capable nodes).

## MHR-Byte: Minimal Bytecode

> **Specification**
```
MHR-Contract {
    hash: Blake3Hash,
    code: Vec<u8>,              // MHR-Byte bytecode
    max_memory: u32,
    max_cycles: u64,
    max_state_size: u32,
    state_key: Hash,            // current state in MHR-Store
    functions: [FunctionSignature],
}
```


MHR-Byte is a minimal bytecode with a ~50 KB interpreter, designed to run on constrained devices like the ESP32. It supports:

| Capability | Description |
|-----------|-------------|
| **Cryptographic primitives** | Hash, sign, verify |
| **CRDT operations** | Merge, compare |
| **CBOR/JSON manipulation** | Structured data processing |
| **Bounded control flow** | Loops with hard cycle limits |

MHR-Byte explicitly **does not** support:
- I/O operations
- Network access
- Filesystem access
- Unbounded computation

All execution is **pure deterministic computation**. Given the same inputs, any node running the same contract produces the same output. This is what makes [verification](../marketplace/verification) possible.

### Opcode Set (47 Opcodes)

| Category | Opcodes | Cycle Cost | Description |
|----------|---------|-----------|-------------|
| **Stack** (6) | PUSH, POP, DUP, SWAP, OVER, ROT | 1 | Stack manipulation |
| **Arithmetic** (9) | ADD, SUB, MUL, DIV, MOD, NEG, ABS, MIN, MAX | 1–3 | 64-bit integer, overflow traps |
| **Bitwise** (6) | AND, OR, XOR, NOT, SHL, SHR | 1 | Bitwise operations |
| **Comparison** (6) | EQ, NEQ, LT, GT, LTE, GTE | 1 | Pushes 0 or 1 |
| **Control** (7) | JMP, JZ, JNZ, CALL, RET, HALT, ABORT | 2–5 | Bounded control flow |
| **Crypto** (3) | HASH, VERIFY_SIG, VERIFY_VRF | 500–2000 | Blake3, Ed25519, ECVRF |
| **System** (10) | BALANCE, SENDER, SELF, EPOCH, TRANSFER, LOG, LOAD, STORE, MSIZE, EMIT | 2–50 | State access and side effects |

**Cycle cost model**: The base unit is 1 cycle ≈ 1 μs on ESP32 (the reference platform). Faster hardware executes more cycles per wall-clock second but charges the same cycle cost per opcode. Gas price in μMHR/cycle is set by each compute provider in their capability advertisement.

**Specification approach**: The reference interpreter (in Rust) serves as the authoritative specification. A comprehensive test vector suite ensures cross-platform conformance. Formal specification (Yellow Paper-style) is deferred until the opcode set stabilizes through real-world usage.

## WASM: Full Execution

Gateway nodes and more capable hardware can offer full WASM (WebAssembly) execution as an additional compute capability. A contract declares its WASM requirement tier:

```
Contract execution path:
  1. Contract specifies: wasm_tier: None
     → Can run on any node with MHR-Byte interpreter (~50 KB)

  2. Contract specifies: wasm_tier: Light
     → Requires Community-tier or above (Pi Zero 2W+)
     → 16 MB memory limit, 10^8 fuel limit, 5 second wall-clock

  3. Contract specifies: wasm_tier: Full
     → Requires Gateway-tier or above (Pi 4/5+)
     → 256 MB memory limit, 10^10 fuel limit, 30 second wall-clock
     → Delegated via capability marketplace if local node can't execute
```

### WASM Sandbox

The WASM execution environment uses **Wasmtime** (Bytecode Alliance, Rust-native) as the reference runtime. Wasmtime provides AOT compilation on Gateway+ nodes, fuel-based execution metering that maps to MHR-Byte cycle accounting, and configurable memory limits per contract.

```
WasmSandbox {
    runtime: Wasmtime,
    max_memory: u32,             // from contract's max_memory field
    max_fuel: u64,               // from contract's max_cycles (1 fuel ≈ 1 MHR-Byte cycle)
    max_wall_time_ms: u32,       // 5,000 (Light) or 30,000 (Full)
}
```

**Host imports**: WASM contracts call back into the Mehr system through a restricted host API mirroring the MHR-Byte System opcodes:

| Host Function | MHR-Byte Equivalent | Fuel Cost |
|--------------|---------------------|-----------|
| `mehr_balance(node_id) → u64` | BALANCE | 10 |
| `mehr_sender() → [u8; 16]` | SENDER | 2 |
| `mehr_self() → [u8; 16]` | SELF | 2 |
| `mehr_epoch() → u64` | EPOCH | 5 |
| `mehr_transfer(to, amount) → bool` | TRANSFER | 50 |
| `mehr_log(data)` | LOG | 10 |
| `mehr_store_load(key) → Vec<u8>` | LOAD | 3 |
| `mehr_store_save(key, value)` | STORE | 3 |
| `mehr_hash(data) → [u8; 32]` | HASH | 500 |
| `mehr_verify_sig(pubkey, msg, sig) → bool` | VERIFY_SIG | 1000 |

No other host imports are available. WASM contracts cannot access the filesystem, network, clock, or random number generator — all execution remains pure and deterministic.

### Light WASM (Community Tier)

Community-tier devices (Pi Zero 2W, 512 MB RAM) support a restricted WASM profile: 16 MB max memory, 10^8 max fuel, 5-second wall-clock limit, interpreted via Cranelift baseline (no AOT). Contracts exceeding Light WASM limits are automatically delegated to a more capable node via [compute delegation](#compute-delegation).

## Compute Delegation

If a node can't execute a contract locally, it delegates to a capable neighbor via the [capability marketplace](../marketplace/overview):

```
Delegation flow:
  1. Node receives request to execute contract
  2. Node checks: can I run this locally?
  3. If no: query nearby capabilities for compute
  4. Find a provider, form agreement, send execution request
  5. Receive result, verify (per agreement's proof method)
  6. Return result to requester
```

This is transparent to the original requester — they don't need to know whether their contract ran locally or was delegated.

## Opaque Compute: Hardware-Accelerated Services

> **Key Insight**
ML inference, transcription, translation, text-to-speech, and any other heavy computation are **not protocol primitives**. They are compute capabilities offered by nodes that have the hardware. The pattern is **opaque compute**: input goes in, output comes out. The protocol does not sandbox, inspect, or guarantee the compute method — the node can use GPU, NPU, FPGA, or any hardware.


```
A GPU/NPU node advertises:
  offered_functions: [
    { function_id: hash("whisper-small"), cost: 50 μMHR/minute },
    { function_id: hash("piper-tts"), cost: 30 μMHR/minute },
  ]
```

A consumer requests execution of that function through the standard compute delegation path. The protocol is **agnostic to what the function does** — it only cares about discovery, negotiation, execution, verification, and payment. Trust comes from reputation, not verification of the compute method.

**Hardware examples:**

| Accelerator Type | Examples |
|-----------------|----------|
| **GPU** | NVIDIA RTX series, AMD Radeon |
| **NPU** | Apple Neural Engine, Qualcomm Hexagon, MediaTek APU |
| **FPGA** | Xilinx, Intel/Altera |
| **TPU** | Google Edge TPU |

### Result Verification for Opaque Compute

Since opaque compute provides no built-in execution guarantee, consumers choose a verification strategy based on their trust requirements and budget:

| Strategy | How It Works | Cost | Trust Level |
|----------|-------------|------|-------------|
| **1. Reputation (default)** | Node builds reputation through consistent outputs. Bad outputs → trust removal → lost income stream. | None (built into trust system) | Moderate |
| **2. Redundant execution** | Client sends same input to 2–3 nodes. Majority agreement = accepted result. | 2–3x compute fees | High |
| **3. Spot-checking** | Client occasionally sends inputs with known outputs. Wrong answer → node flagged, agreement terminated. | ~5% overhead | Moderate–High |
| **4. Cryptographic verification** | ZK proofs for deterministic contracts; TEE attestation for opaque compute. See below. | 10–1000x (ZK) or ~5% (TEE) | Highest |

### Cryptographic Verification Details

ZK proofs and TEE attestation provide the strongest verification guarantees. Their applicability depends on the workload:

```
Cryptographic verification tiers:

  Tier A: ZK Proofs for MHR-Byte / WASM Contracts (deterministic compute)

    Viable ZK systems for Mehr's constraint environment:
      1. RISC Zero (zkVM) — proves correct execution of RISC-V programs
         Circuit: RISC-V trace → STARK proof
         Proof size: ~200 KB (compressible to ~50 KB with SNARK wrapping)
         Prover cost: ~1000x slowdown vs native execution
         Verifier cost: ~10 ms on Raspberry Pi, <1 ms on Gateway+
         Viable for: MHR-Byte and WASM contracts up to ~10^6 cycles

      2. SP1 (Succinct) — similar approach, optimized prover
         Proof size: ~100-300 KB
         Prover cost: ~500-1000x slowdown
         Viable for: same as RISC Zero, slightly better performance

      3. Jolt (a]16) — optimized for structured computations
         Proof size: ~50-150 KB
         Prover cost: ~200-500x slowdown for simple contracts
         Viable for: contracts with bounded loops and array operations

    Practical overhead for typical workloads:
      Simple validation contract (10^4 cycles): ~10 second prover time
      Complex state migration (10^6 cycles): ~15 minutes prover time
      ZK proofs are IMPRACTICAL above ~10^8 cycles with current technology

    Cost model:
      zk_proof_cost = base_compute_cost × zk_overhead_factor
      zk_overhead_factor = 500 (default, adjustable by provider)
      Consumer pays for the proof generation as part of the compute fee

    Integration:
      ComputeResult {
          job_hash: Blake3Hash,
          output_hash: Blake3Hash,
          proof: Option<ZKProof>,    // present when ZK verification requested
          proof_system: u8,          // 0=None, 1=RISC_Zero, 2=SP1, 3=Jolt
      }
      Verifier checks proof against (contract_hash, input_hash, output_hash)
      Any node can verify without re-executing the contract

  Tier B: TEE Attestation for Opaque Compute (non-deterministic / large models)

    ZK proofs are impractical for large ML models (10^12+ operations).
    TEE attestation is the viable path for high-assurance opaque compute:

      TEE platforms:
        AMD SEV-SNP: Available now on EPYC processors
        NVIDIA H100 Confidential Computing: GPU compute inside TEE
        ARM CCA (Confidential Compute Architecture): Emerging

      Attestation flow:
        1. Provider generates attestation report from TEE hardware
        2. Report binds: TEE identity + loaded code hash + platform state
        3. Consumer verifies report against vendor's root of trust
        4. If valid: the code running in the TEE matches expectations

      Cost: ~5% overhead vs plaintext (hardware-level, nearly free)
      Trust assumption: hardware vendor is honest

    For Mehr:
      TEE attestation is preferred over ZK for models > 10^6 parameters.
      Nodes advertise TEE support in capability bits.
      Attestation reports are ~2-4 KB, verified by the consumer.

  Decision matrix:
    Workload                    Recommended verification
    ─────────────────────────   ──────────────────────────
    MHR-Byte contract (<10^4)   ZK proof (cheap, fast)
    WASM contract (<10^6)       ZK proof (feasible)
    WASM contract (>10^6)       Redundant execution or TEE
    Small ML model (<1M params) Redundant execution + spot-check
    Large ML model (>1M params) TEE attestation (if available)
    Any model on GPU             TEE (NVIDIA H100 CC) or reputation
    No TEE, large model          Reputation + spot-checking (default)

  Mehr does NOT mandate any single verification approach.
  The consumer chooses based on sensitivity, budget, and available providers.
  The protocol provides the framework; the market determines adoption.
```

Verification is a **consumer-side choice**, not protocol enforcement. Most consumers rely on reputation (the default). High-value or adversarial workloads use redundant execution or spot-checking.

## Contract Use Cases

| Application | Contract Purpose |
|------------|-----------------|
| **Naming** | Community-label-scoped name resolution (`maryam@tehran-mesh` → NodeID) |
| **Forums** | Append-only log management, moderation rules |
| **Marketplace** | Listing validation, escrow logic |
| **Wiki** | CRDT merge rules for collaborative documents |
| **Group messaging** | Symmetric key rotation, member management |
| **Access control** | Permission checks for mutable data objects |

## Resource Limits

Every contract declares its resource bounds upfront:

- **max_memory**: Maximum memory allocation
- **max_cycles**: Maximum CPU cycles before forced termination
- **max_state_size**: Maximum persistent state

These limits are enforced by the runtime. A contract that exceeds its declared limits is terminated immediately. This prevents denial-of-service through runaway computation.

## Private Compute (Optional)

By default, compute delegation has **no input privacy** — the compute node sees your input and produces a result. This is fine for most workloads (contract execution, public data processing, non-sensitive queries). But for sensitive data — medical records, private messages, financial analysis — you need the compute node to process data it cannot read.

Private compute is **opt-in per agreement**. The consumer chooses a privacy tier based on sensitivity and willingness to pay:

```
CapabilityAgreement {
    ...
    privacy: enum {
        None,                   // default — compute node sees input/output
        SplitInference,         // model partitioned, node sees only middle layers
        SecretShared,           // input split across multiple nodes
        TEE,                    // hardware-attested secure enclave
    },
}
```

### Tier 0: No Privacy (Default)

The compute node receives plaintext input, executes, and returns the result. Verification is via [result hash](../marketplace/verification) or redundant execution. This is the cheapest and fastest option.

**Use for**: Public data, non-sensitive queries, contract execution, anything where the input isn't secret.

### Tier 1: Split Inference

For ML/AI workloads. The neural network is partitioned across nodes so no single node sees both the raw input and the final output:

```
Split inference flow:
  1. Consumer runs first 1-3 layers locally (transforms raw input)
  2. Intermediate activations are sent to Inference node
     (optionally with calibrated DP noise for formal privacy guarantees)
  3. Inference node runs the heavy middle layers
  4. Intermediate result sent back to consumer
  5. Consumer runs final 1-2 layers locally (produces final output)

What the Inference node sees:
  ✗ Raw input (transformed by early layers)
  ✗ Final output (produced by consumer's final layers)
  ✓ Intermediate activations (a compressed, transformed representation)
```

**Overhead**: ~1.2–2x latency vs plaintext. Bandwidth for activation transfer at cut points. Consumer needs enough compute for a few neural network layers (Gateway tier or above).

**Privacy strength**: Moderate. Adding differential privacy noise to activations at cut points provides formal (ε, δ)-privacy guarantees at the cost of some accuracy (2–15% depending on privacy budget).

**Use for**: AI inference on personal data — voice transcription, document analysis, image processing — where the compute node shouldn't see the raw content.

### Tier 2: Secret-Shared Computation

Input data is split using Shamir's Secret Sharing into N shares, each sent to a different compute node. No individual node can reconstruct the input.

```
Secret-shared compute flow:
  1. Consumer splits input into 3 shares (2-of-3 threshold)
  2. Each share sent to a different compute node
  3. Each node computes on its share independently
     - Additions and scalar multiplications: free (local computation)
     - Multiplications between secrets: one communication round between nodes
  4. Consumer collects result shares and reconstructs the output

What each compute node sees:
  ✗ Original input (only a random-looking share)
  ✗ Other nodes' shares
  ✗ Final output
  ✓ Its own share (information-theoretically meaningless alone)
```

**Overhead**: 3x bandwidth (3 shares), 3x compute cost (3 nodes). Linear operations are nearly free; non-linear operations require inter-node communication.

**Trust assumption**: At most 1 of 3 nodes may be malicious (honest majority). The consumer selects 3 nodes from different trust neighborhoods to minimize collusion risk.

**Best for**: Linear/affine workloads — aggregation, statistics, linear classifiers, search queries. For neural networks with many non-linear layers, combine with Tier 1: secret-share the input, run the first layers as MPC on shares, then switch to split inference for the deep non-linear layers.

**Use for**: Medical data analysis, private search, financial computation — anything where the input must remain hidden from all compute providers.

### Tier 3: TEE (Hardware-Attested)

Compute runs inside a Trusted Execution Environment (AMD SEV-SNP, NVIDIA H100 Confidential Computing, or ARM CCA). The hardware enforces that even the node operator cannot read the data being processed.

```
TEE compute flow:
  1. Consumer discovers a node advertising TEE capability
  2. Consumer requests and verifies a remote attestation report
     (proves specific code is running inside a genuine TEE)
  3. Consumer sends encrypted input (encrypted to the TEE's ephemeral key)
  4. TEE decrypts, processes, encrypts output for consumer
  5. Consumer decrypts result

What the node operator sees:
  ✗ Input (encrypted for the TEE)
  ✗ Output (encrypted for the consumer)
  ✗ Intermediate state (protected by hardware)
  ✓ That a computation happened, its duration, and data sizes
```

**Overhead**: Under 5% compute overhead. Near-zero bandwidth overhead. Requires server-grade hardware (AMD EPYC, NVIDIA H100).

**Trust assumption**: You trust the hardware vendor (AMD, Intel, NVIDIA) to have correctly implemented the TEE. You do NOT trust the node operator.

**Limitation**: Only available on Inference-tier nodes with server hardware. Not available on ESP32, Raspberry Pi, or consumer hardware.

**Use for**: Highest-sensitivity workloads — end-to-end encrypted AI inference, confidential data processing — where you're willing to trust the hardware vendor but not the node operator.

### Choosing a Privacy Tier

| Tier | Overhead | Privacy | Hardware Required | Cost |
|------|----------|---------|-------------------|------|
| **None** | 1x | None | Any | Cheapest |
| **Split Inference** | 1.2–2x | Moderate (DP-configurable) | Consumer: Gateway+. Provider: any. | Low premium |
| **Secret Shared** | 3x+ | Strong (information-theoretic) | 3 compute nodes | 3x compute cost |
| **TEE** | ~1x | Strong (hardware-attested) | Provider: server-grade with TEE | Slight premium |

The default is **no privacy**. Most compute delegation doesn't need it — you're running a public contract on public data, or the result hash verification is sufficient. Private compute is for when the **input itself** is sensitive.

### Combining Tiers

Tiers can be combined for defense in depth:

- **Split + Secret Shared**: Secret-share the input, run first layers as MPC across 3 nodes, then split inference for deep layers. Maximum software-based privacy.
- **Split + TEE**: Run the heavy middle layers inside a TEE. The TEE never sees raw input (early layers run locally), and you get hardware attestation for the critical computation.

The consumer specifies the desired combination in the capability agreement. The marketplace handles discovery of nodes that support the requested privacy tier.

<!-- faq-start -->

## Frequently Asked Questions

<details className="faq-item">
<summary>What kind of programs can run on MHR-Compute?</summary>

MHR-Compute supports two tiers: MHR-Byte (a minimal 47-opcode bytecode) runs on any device including ESP32 microcontrollers and handles data validation, access control, and simple state transitions. WASM (WebAssembly) runs on more capable hardware and supports complex logic up to 256 MB memory. Neither tier can access the network, filesystem, or clock — all execution is pure and deterministic.

</details>

<details className="faq-item">
<summary>Who pays for computation, and how?</summary>

The node requesting the computation pays. Each opcode has a cycle cost, and compute providers set a price in μMHR per cycle in their capability advertisements. Payment flows through bilateral payment channels — the same mechanism used for all Mehr services. If your node can run the contract locally, there’s no payment to anyone else.

</details>

<details className="faq-item">
<summary>Is this like smart contracts on a blockchain?</summary>

Similar in concept (deterministic, verifiable programs) but fundamentally different in architecture. MHR-Compute contracts don’t require global consensus or a blockchain. They run on individual nodes, state lives in CRDT DataObjects that merge without consensus, and verification is bilateral (between the parties involved). There’s no gas auction, no block confirmation wait, and no global state.

</details>

<details className="faq-item">
<summary>Can GPUs or other accelerators be used for computation?</summary>

Yes, through **opaque compute**. Nodes with GPUs, NPUs, FPGAs, or TPUs advertise specific functions they can run (e.g., Whisper speech-to-text, image generation). These aren’t sandboxed like MHR-Byte/WASM — the protocol treats them as black boxes where input goes in and output comes out. Trust comes from reputation, redundant execution, or TEE attestation rather than deterministic verification.

</details>

<details className="faq-item">
<summary>What if my device is too weak to run a contract?</summary>

Your node automatically delegates to a more capable neighbor via the capability marketplace. The delegation is transparent — you send a request, a nearby node executes the contract, returns the result, and you verify it. You pay the provider’s compute fees. The original requester doesn’t need to know whether execution was local or delegated.

</details>

<!-- faq-end -->

---

### MHR-ID: FAQ
<!-- Source: docs/services/mhr-id/faq.md -->

# MHR-ID: Frequently Asked Questions

<details>
<summary><strong>What is my identity on Mehr?</strong></summary>

Your identity is an Ed25519 key pair — your public key is your identity. No authority issues it, and no authority can revoke it. Everything else — your name, location claims, profile fields, external account links — are **claims** attached to that key, verified by peer vouches.

</details>

<details>
<summary><strong>Can I prove my physical location?</strong></summary>

Yes, via [RadioRangeProof](./verification.md#radiorangeproof). If nearby nodes can hear your LoRa radio beacon, they sign witness attestations proving you're within physical range. Multiple witnesses from known locations triangulate your approximate position. This works at neighborhood level on LoRa (1–15 km range), building level on WiFi, and room level on Bluetooth.

</details>

<details>
<summary><strong>What if I don't have internet access?</strong></summary>

Most MHR-ID features work without internet. Claims, vouches, profiles, visibility controls, and geographic verification all operate over the mesh. The only feature that requires internet is **external identity linking** (linking to GitHub, Twitter, etc.) — and even that can be verified by any peer with internet access, not just dedicated oracles.

</details>

<details>
<summary><strong>Can I link my GitHub/Twitter/Mastodon account?</strong></summary>

Yes. Post a signed challenge string on your external platform profile, then publish an ExternalIdentity claim on Mehr. Verification oracles (gateway nodes with internet) crawl the platform to confirm the link, or you can use OAuth flow. Any peer with internet can also manually verify and vouch for you. See [Identity Linking](./verification.md#identity-linking).

</details>

<details>
<summary><strong>Who can see my profile fields?</strong></summary>

You control visibility per-field. Each profile field is a separate claim with its own visibility setting:
- **Public**: Anyone can read it
- **TrustNetwork**: Your trusted peers and their trusted peers (2 hops)
- **DirectTrust**: Only your directly trusted peers
- **Named**: Only specific nodes you list

Different viewers see different subsets of your profile. See [Visibility Controls](./index.md#visibility-controls).

</details>

<details>
<summary><strong>What happens to my identity if I move cities?</strong></summary>

Your cryptographic identity stays the same — only your geo-scoped claims change. Old location vouches expire naturally (30 epochs), new location claims build via RadioRangeProof and peer attestation. Trust relationships, payment channels, and Topic-scoped communities are all **location-independent** and travel with you. See [Geographic Mobility](./mobility.md#geographic-mobility).

</details>

<details>
<summary><strong>What if my key is compromised?</strong></summary>

Publish a KeyRotation claim signed by **both** the old and new keys. This provides strong cryptographic evidence of legitimate migration. Trusted peers vouch for the rotation, services migrate agreements to the new key, and the old key's reputation transfers. If you've lost the old key entirely, you'll need to rebuild trust from scratch — there's no central recovery mechanism.

</details>

<details>
<summary><strong>How does MHR-ID compare to DID/SSI systems?</strong></summary>

MHR-ID is simpler and mesh-native. Unlike W3C DIDs, there's no DID document resolution, no blockchain anchoring, and no DID method abstraction layer. Your key **is** your identity. Claims replace VCs (Verifiable Credentials) with a lighter-weight, gossip-friendly format. Trust is computed locally from your trust graph, not from a credential issuer hierarchy. See the [comparison table](./mobility.md#comparison-with-other-identity-systems).

</details>

<details>
<summary><strong>Can I be anonymous?</strong></summary>

Yes. You can operate with just a key pair and no claims at all. No display name, no geo presence, no external links. You'll have a low verification level and be treated as untrusted by most nodes, but you can still use the network. For stronger anonymity, generate a new key pair — there's no link between keys unless you explicitly publish a KeyRotation claim.

</details>

---

### MHR-Name
<!-- Source: docs/services/mhr-name.md -->

# MHR-Name: Naming Service

MHR-Name provides human-readable names for the Mehr mesh. There is no global namespace and no central authority — names are **subjective**, resolved from each node's own position in the [trust graph](../economics/trust-neighborhoods). Two nodes may resolve the same name to different targets, and that's by design.

Names can point to three kinds of targets:
- **People and devices** — a NodeID (16-byte destination hash)
- **Content** — a ContentHash in [MHR-Store](mhr-store) (32-byte Blake3)
- **Services** — a running program (NodeID + service type + port)

## Name Format

Names follow the format `name@scope`:

```
maryam@geo:tehran
alice@geo:us/oregon/portland
relay-7@geo:backbone-west
pikachu-fan@topic:gaming/pokemon
my-blog@topic:tech/mesh-networking
```

The scope portion uses the [HierarchicalScope](../economics/trust-neighborhoods#hierarchical-scopes) format. Geographic scopes are prefixed with `geo:`, interest scopes with `topic:`. The `/` separator maps to hierarchy segments (e.g., `geo:us/oregon/portland` is `Geo("us", "oregon", "portland")`).

**Name constraints:**
- Max length: 64 bytes UTF-8
- Allowed characters: Unicode letters, digits, hyphens (`-`), underscores (`_`)
- No whitespace, no control characters, no `@` (reserved as scope delimiter)
- Names are normalized to NFKC Unicode form before registration and lookup

## Why No Global Namespace?

A global namespace requires global consensus on name ownership. Global consensus contradicts Mehr's partition tolerance requirement. If two partitioned networks both register the name "alice," there is no partition-safe way to resolve the conflict.

Scope-based names solve this:
- Names are locally consistent within their scope
- Different communities can have different "alice" users — they are `alice@geo:portland` and `alice@geo:tehran`
- No global coordination needed
- Scopes are self-assigned, not centrally managed

## Name Targets

A name can resolve to one of three target types:

```
NameTarget {
    NodeID(DestinationHash),           // person, device, or service operator (16 bytes)
    ContentHash(Blake3Hash),           // stored content — websites, documents (32 bytes)
    AppManifest(Blake3Hash),           // distributed application (32 bytes)
}
```

| Target Type | Use Case | Example |
|-------------|----------|---------|
| **NodeID** | People, devices, relays, service operators | `alice@geo:portland` resolves to Alice's node |
| **ContentHash** | Websites, documents, media | `my-blog@topic:tech` resolves to a blog stored in MHR-Store |
| **AppManifest** | Distributed applications | `forum-app@topic:apps/forums` resolves to an [AppManifest](mhr-app#appmanifest) |

ContentHash targets are integrity-verified by definition — the hash guarantees the content hasn't been tampered with. AppManifest targets identify [distributed applications](mhr-app#appmanifest) — the manifest binds together contract code, UI, and state schema into a single installable artifact. Live services (APIs, bots) are discovered through the [capability marketplace](../marketplace/discovery), not through naming — naming identifies *what* something is, the marketplace discovers *where* it's running.

## Name Registration

Names are registered by publishing a signed binding:

```
NameBinding {
    name: String,                       // max 64 bytes, NFKC normalized
    scope: HierarchicalScope,           // Geo("portland") or Topic("gaming", "pokemon")
    target: NameTarget,                 // what the name resolves to
    node_id: NodeID,                    // registrant's identity
    registered: u64,                    // epoch when first registered
    expires: u64,                       // expiry epoch (registered + 30 epochs)
    sequence: u32,                      // monotonic counter for updates
    signature: Ed25519Signature,        // signs all fields above
}
```

> **Specification**
NameBindings are signed, scoped, and sequence-numbered. They expire after 30 epochs and propagate via gossip within their matching scope. Updates require strictly increasing sequence numbers. Minimum wire size: 122 bytes.

**Signature**: Covers all fields except the signature itself. Verified using the registrant's Ed25519 public key (obtained via announce or [MHR-ID](mhr-id) claims).

**Sequence number**: Must be strictly greater than any previous binding for the same `(name, scope, node_id)` tuple. This enables updates (changing a name's target) and prevents replay of old bindings.

**Expiry**: Bindings expire after 30 epochs (matching [vouch expiry](mhr-id#vouch-properties) in MHR-ID). Expired bindings are no longer returned in lookups. This prevents stale names from persisting after a node leaves the network.

Name bindings propagate via gossip within the matching scope — a binding for `alice@geo:portland` gossips among nodes with Portland-matching scopes.

## Name Resolution

Resolution is **subjective** — each node resolves names from its own trust graph position. When multiple bindings exist for the same name in the same scope, they are ranked by a 5-level priority:

### Resolution Priority

1. **Petname** (local override) — if the resolver has a local petname matching the query, it wins unconditionally. Petnames are never shared and cannot be overridden by network data.

2. **Trust distance** — scored from the resolver's trust graph. The binding registered by the more-trusted node wins.

   ```
   Trust score calculation:
     Direct trusted peer:        1.0
     Friend-of-friend:           Π(vouch_confidence × trust_decay) along shortest path
     Beyond 2 hops (untrusted):  0.01
   ```

   The trust score follows the same decay model as [transitive credit](../economics/trust-neighborhoods#trust-based-credit): full weight for direct peers, configurable ratio (default 10%) per hop, and a floor of 0.01 for completely untrusted nodes. Higher score wins.

3. **Verification level** — if trust scores are equal, bindings from nodes with stronger [MHR-ID](mhr-id) verification win. A node with vouched GeoPresence claims outranks one with only self-attested claims.

   ```
   Verification tiers:
     Vouched by trusted peers:   highest
     Self-attested claims only:  middle
     No identity claims:         lowest
   ```

4. **Scope specificity** — if verification levels are also equal, a binding registered with a more specific scope ranks higher. `alice@geo:us/oregon/portland` outranks `alice@geo:portland` because it's more precise.

5. **First-seen** (tiebreaker) — if all else is equal, the first binding your node received wins.

### Multi-Result Lookups

Lookups return a **ranked list** of matching bindings, not just a single result. Applications decide how to present results:
- **Auto-pick**: Use the top-ranked result silently (suitable for programmatic lookups)
- **Show list**: Present ranked candidates to the user (suitable for ambiguous names)
- **Prompt**: Ask the user to choose when the top two results are close in score

## Trust-Weighted Propagation

Name bindings propagate through the mesh via gossip, but propagation is shaped by trust:

**Within-scope gossip**: Bindings gossip among nodes whose scopes match the binding's scope. A binding for `Topic("gaming", "pokemon")` propagates among nodes with that topic scope.

**Propagation priority**: Nodes prioritize forwarding bindings from closer trust relationships. A binding from a direct trusted peer is gossiped immediately; a binding from an untrusted node may be delayed or dropped if bandwidth is constrained.

**Gossip filtering**: Nodes only forward bindings that score above a minimum trust threshold (0.01). Bindings from completely unknown, unconnected nodes are not propagated — they must build trust relationships first. This naturally limits Sybil flooding.

**Cross-scope queries**: To resolve a name in a different scope, the query uses **DHT-guided scope routing**:

```
Cross-scope query algorithm:

  Resolver wants to look up "alice@geo:tehran" but is in geo:portland.

  1. SCOPE HASH: Compute scope_key = Blake3("geo:tehran")

  2. DHT LOOKUP: Query MHR-DHT for scope_key.
     Nodes with matching scopes register themselves as scope anchors:
       DHT_PUT(Blake3(scope_string), ScopeAnchor { node_id, scope, timestamp })
     Scope anchors are refreshed every 10 gossip intervals.

  3. ROUTE TO ANCHOR: The DHT lookup returns one or more ScopeAnchor
     entries — nodes whose HierarchicalScope matches "geo:tehran".
     Select the nearest anchor by:
       a. Trust distance (prefer anchors reachable via trusted peers)
       b. If tied: hop count (from CompactPathCost)
       c. If tied: lowest ring distance (XOR distance on DHT ring)

  4. FORWARD QUERY: Send NameLookup message to the selected anchor.
     The anchor resolves "alice" within its local scope using normal
     trust-weighted resolution and returns NameLookupResponse.

  5. CACHE RESULT: Cache the response locally with TTL.
     TTL = 5 × local_gossip_interval
     At 60-second gossip rounds: TTL = 300 seconds (5 minutes).
     At slower transports (e.g., 5-minute LoRa gossip): TTL = 25 minutes.
     TTL is measured in gossip intervals, not wall-clock, so it adapts
     to transport speed automatically.

  6. FALLBACK: If no ScopeAnchor exists in the DHT for the target scope
     (no node in the reachable network has that scope):
       a. Return NAME_NOT_FOUND with reason SCOPE_UNREACHABLE
       b. Cache the negative result with TTL / 2 (to retry sooner)
       c. The resolver may retry after TTL / 2 — the scope may become
          reachable as network topology changes

ScopeAnchor registration:
  Every node registers itself as a scope anchor for each of its scopes:
    For scope in node.scopes:
      DHT_PUT(Blake3(scope.to_string()), ScopeAnchor {
          node_id: self.node_id,
          scope: scope,
          timestamp: current_epoch,
      })
  Refreshed every 10 gossip intervals. Expires after 30 gossip intervals
  without refresh (node left or partitioned).

  Hierarchical registration: A node in geo:us/oregon/portland registers
  anchors for ALL ancestor scopes:
    Blake3("geo:us/oregon/portland")
    Blake3("geo:us/oregon")
    Blake3("geo:us")
  This ensures queries at any specificity level find relevant anchors.
```

"Nearest matching cluster" is defined concretely as the anchor with the shortest trust distance, falling back to hop count and then XOR distance. This maps naturally to Mehr's existing routing — trust distance captures social proximity, hop count captures network proximity, and XOR distance captures DHT ring proximity.

## Collision Handling

Collisions are a natural consequence of decentralized naming. MHR-Name handles them through three mechanisms:

### Display-Side Disambiguation

When a lookup returns multiple results, applications can show additional context from [MHR-ID](mhr-id) to help users distinguish between candidates:

```
Lookup: alice@geo:portland

Results:
  1. alice@geo:portland → 0x3a7f...  (trust: direct, verified: GeoPresence Portland/Hawthorne)
  2. alice@geo:portland → 0x8e2d...  (trust: 2-hop, verified: GeoPresence Portland/Pearl)
```

Verified GeoPresence claims, CommunityMember claims, and ExternalIdentity claims from MHR-ID all provide disambiguation context.

### Voluntary Scope Narrowing

Users can optionally register with a more specific scope to reduce collisions:

```
alice@geo:portland                    → broad scope, may collide
alice@geo:us/oregon/portland          → narrower, less collision risk
alice@geo:us/oregon/portland/pearl    → very specific, unlikely to collide
```

The protocol **never forces** a user to narrow their scope. This is always voluntary — a user in Portland, Oregon and a user in Portland, Maine can both register as `alice@geo:portland` if they choose. The trust-weighted resolution and display-side disambiguation handle the rest.

### Geographic Collision

Two different cities named "portland" (e.g., Portland, Oregon and Portland, Maine) would both match `geo:portland`. Proximity-based resolution handles this naturally — you'll reach whichever Portland is closer in the mesh. To disambiguate explicitly, use a longer scope path: `alice@geo:us/oregon/portland` vs. `alice@geo:us/maine/portland`.

## Name Lifecycle

```
1. REGISTER:  Node publishes NameBinding (signed, gossiped within scope)
2. GOSSIP:    Binding propagates to peers with matching scopes
3. RESOLVE:   Other nodes look up the name using trust-weighted resolution
4. RENEW:     Before expiry, re-publish with sequence+1 (same target = renewal)
5. UPDATE:    Publish with sequence+1 and new target (changes what the name points to)
6. REVOKE:    Publish with sequence+1 and a revocation flag (name is released)
7. EXPIRE:    After 30 epochs with no renewal, binding is removed from caches
```

**Renewal**: Re-publishing a binding with an incremented sequence number and the same target extends the expiry by another 30 epochs.

**Revocation**: A node can explicitly release a name by publishing a binding with the next sequence number and an empty/null target. This signals to the network that the name is no longer claimed.

**Garbage collection**: Nodes remove expired bindings from their local cache. There is no obligation to store bindings for names you don't query.

## Petnames

As a fallback and privacy feature, each user can assign **petnames** — local nicknames stored only on their own device:

```
User's petname mapping (local only, never shared):
  "Mom"       → NodeID(0x3a7f...b2c1)
  "Work"      → NodeID(0x8e2d...f4a9)
  "My Blog"   → ContentHash(0x1b5c...d3e7)
  "Forum"     → AppManifest(0x9f1a...c4d2)
```

Petnames:
- Override **all** network name resolution (highest priority in the resolution hierarchy)
- Are completely private (never shared, never gossiped)
- Can point to any target type (NodeID, ContentHash, AppManifest)
- Cannot be taken, censored, or disputed by anyone
- Are the most censorship-resistant naming possible

## Security Considerations

<details className="security-item">
<summary>Name Squatting</summary>

**Vulnerability:** An attacker registers popular names (e.g., `signal@topic:apps`) early, hoping to intercept traffic.

**Mitigation:** Trust-weighted resolution means squatted names only win for nodes that trust the squatter. For the vast majority of the network, the legitimate service — which has real trust relationships — will outrank the squatter. A name squatter with no trust connections resolves at the 0.01 floor, making their binding nearly invisible.

</details>

<details className="security-item">
<summary>Sybil Name Flooding</summary>

**Vulnerability:** Create many identities, each claiming variations of a target name, to pollute lookup results.

**Mitigation:** Gossip filtering by trust score. Sybil nodes with no trust relationships score 0.01 and their bindings are deprioritized during propagation. Because nodes only forward bindings above the minimum trust threshold, Sybil bindings don't propagate far. The cost of building genuine trust relationships limits the attacker's reach.

</details>

<details className="security-item">
<summary>Partition Poisoning</summary>

**Vulnerability:** During a network partition, inject false name bindings. After merge, these bindings compete with legitimate ones.

**Mitigation:** Sequence numbers prevent rollback — a legitimate node's higher-sequence binding always supersedes lower-sequence poison. Post-merge, trust-weighted resolution naturally favors the binding from the more-connected, more-trusted source.

</details>

<details className="security-item">
<summary>Homoglyph Impersonation</summary>

**Vulnerability:** Register `аlice@geo:portland` using Cyrillic 'а' (U+0430) instead of Latin 'a' (U+0061) to impersonate `alice@geo:portland`.

**Mitigation:** NFKC Unicode normalization on registration collapses many confusable characters. Applications should additionally display script-mixing warnings (e.g., "This name contains mixed Unicode scripts") and optionally reject names that mix scripts within a single label.

</details>

<details className="security-item">
<summary>Key Compromise</summary>

**Vulnerability:** A stolen private key is used to publish new name bindings, hijacking the victim's name.

**Mitigation:** The victim performs [key rotation](mhr-id/mobility#key-rotation) via MHR-ID. The new key publishes a higher-sequence binding for the same name. Trusted peers who vouch for the key rotation accelerate propagation of the legitimate binding. The old key's bindings become superseded.

</details>

<details className="security-item">
<summary>Name-Content Binding Abuse</summary>

**Vulnerability:** A trusted node registers a well-known name pointing to a ContentHash, then later updates it to point to malicious content.

**Mitigation:** ContentHash targets are integrity-verified — the hash itself proves the content is what was originally published. If the name's target changes (new sequence number, different ContentHash), applications should alert the user that the content behind a name has changed, similar to SSH host key warnings.

</details>

<details className="security-item">
<summary>Global vs. Local Authority Conflict</summary>

**Vulnerability:** A globally recognized application (e.g., a popular messaging service) uses the name `signal@topic:apps`. A local node in the same scope also registers `signal@topic:apps`.

**Mitigation:** There is no concept of "global authority" in Mehr — resolution is always subjective. The legitimate service will have trust relationships spanning the network, giving it a high trust score for most resolvers. The local squatter scores high only for nodes that directly trust them. For an attacker to successfully impersonate a well-known service, they would need to build trust relationships rivaling the legitimate service — which requires actually providing reliable service over time.

</details>

## Wire Format

### NameBinding

| Field | Size | Description |
|-------|------|-------------|
| `name_len` | 1 byte | Length of name in bytes (max 64) |
| `name` | variable | UTF-8 name, NFKC normalized |
| `scope` | variable | [HierarchicalScope wire format](../economics/trust-neighborhoods#wire-format) |
| `target_type` | 1 byte | 0x01=NodeID, 0x02=ContentHash, 0x03=AppManifest |
| `target` | variable | Target payload (see below) |
| `node_id` | 16 bytes | Registrant's destination hash |
| `registered` | 8 bytes | Registration epoch (u64 LE) |
| `expires` | 8 bytes | Expiry epoch (u64 LE) |
| `sequence` | 4 bytes | Monotonic counter (u32 LE) |
| `signature` | 64 bytes | Ed25519 signature |

**Target payloads:**

| Type | Tag | Payload Size | Contents |
|------|-----|-------------|----------|
| NodeID | 0x01 | 16 bytes | Destination hash |
| ContentHash | 0x02 | 32 bytes | Blake3 hash |
| AppManifest | 0x03 | 32 bytes | Blake3 hash of [AppManifest](mhr-app#appmanifest) |

Minimum binding size: 1 + 1 + 3 + 1 + 16 + 16 + 8 + 8 + 4 + 64 = **122 bytes** (1-char name, minimal scope, NodeID target). Fits in a single Mehr packet (max 465 bytes data).

### Message Types

Name messages use context byte `0xF7` (social) with sub-types:

| Sub-Type | Name | Description |
|----------|------|-------------|
| `0x08` | NameRegister | Publish or update a name binding |
| `0x09` | NameLookup | Query for a name in a scope |
| `0x0A` | NameLookupResponse | Return matching bindings (ranked list) |

### Backward Compatibility

The old `name@community-label` format maps to the new scope format:

```
alice@portland-mesh    →  alice@geo:portland
relay-7@backbone-west  →  relay-7@geo:backbone-west
```

Old bindings without a `target_type` field default to NodeID using the `node_id` field. Old bindings without `expires` or `sequence` fields are treated as sequence 0 with no expiry (legacy behavior). Nodes running older software continue to use the flat `community_label` field. New nodes check both `scopes` and `community_label` for resolution.

## Integration Points

**[MHR-ID](mhr-id)**: Vouches feed trust scores used in name resolution. GeoPresence claims enable display-side disambiguation. KeyRotation claims enable name recovery after key compromise.

**[MHR-Store](mhr-store)**: ContentHash name targets point to stored content. Name bindings themselves can be stored as DataObjects for persistence beyond gossip TTL.

**[MHR-DHT](mhr-dht)**: Name bindings are replicated via DHT within their scope. Cross-scope lookups use DHT routing to find nodes with matching scopes.

**[Trust Graph](../economics/trust-neighborhoods)**: The trust graph is the foundation of name resolution. Trust distance determines binding priority. The `trust_decay` product along the shortest path yields the trust score.

**[MHR-Pub](mhr-pub)**: Applications can subscribe to name change events via pub/sub. When a name binding is updated (new sequence number), subscribers are notified — useful for contact list sync and service discovery.

<!-- faq-start -->

## Frequently Asked Questions

<details className="faq-item">
<summary>How do names work without DNS?</summary>

MHR-Name uses scoped, trust-weighted naming instead of global DNS. Names follow the format `name@scope` (e.g., `alice@geo:portland`) and resolve from your position in the trust graph. There’s no central registry — bindings propagate via gossip within their matching scope and are ranked by trust distance, verification level, and scope specificity. Different nodes may resolve the same name differently, and that’s by design.

</details>

<details className="faq-item">
<summary>Can someone squat on a popular name?</summary>

Name resolution is trust-weighted, not first-come-first-served globally. If two people register `alice@geo:portland`, the one with stronger trust relationships in your network wins from your perspective. A squatter with no real trust connections will rank below a legitimate user. Names also expire after 30 epochs and must be renewed, preventing indefinite squatting by inactive accounts.

</details>

<details className="faq-item">
<summary>Can constrained devices (like ESP32) resolve names?</summary>

Constrained devices delegate name lookups to a nearby relay, just like DHT lookups. The relay resolves the name and returns the result. The client verifies the response using the binding’s Ed25519 signature — the relay cannot forge a name binding. For extra assurance, the client can query multiple relays and compare results.

</details>

<details className="faq-item">
<summary>What if I want to reach someone in a completely different scope?</summary>

Cross-scope lookups use DHT-guided scope routing. Your node computes the hash of the target scope, queries MHR-DHT for scope anchors (nodes in that scope), and forwards the name query to the nearest anchor. The anchor resolves the name locally and returns the result. The response is cached with a TTL that adapts to your transport speed.

</details>

<!-- faq-end -->

---

## Economics

### MHR Token
<!-- Source: docs/economics/mhr-token.md -->

# MHR Token

MHR is the unit of account for the Mehr network. It is not a speculative asset — it is the internal currency for purchasing capabilities from nodes outside your trust network.

## Properties

```
MHR Properties:
  Smallest unit: 1 μMHR (micro-MHR)
  Initial distribution: Genesis service allocation + demand-backed proof-of-service mining (no ICO)
  Genesis allocation: Disclosed amount to genesis gateway operator (see Genesis below)
  Supply ceiling: 2^64 μMHR (~18.4 × 10^18 μMHR, asymptotic — never reached)
```

### Supply Model

MHR has an **asymptotic supply ceiling** with **decaying emission**:

| Phase | Epoch Range | Emission Per Epoch |
|-------|-------------|-------------------|
| Bootstrap | 0–99,999 | 10^12 μMHR (1,000,000 MHR) |
| Halving 1 | 100,000–199,999 | 5 × 10^11 μMHR |
| Halving 2 | 200,000–299,999 | 2.5 × 10^11 μMHR |
| Halving N | N × 100,000 – (N+1) × 100,000 − 1 | 10^12 × 2^(−N) μMHR |
| Tail | When halved reward is below floor | 0.1% of circulating supply / estimated epochs per year |

```
Emission formula:
  halving_shift = min(e / 100_000, 63)   // clamp to prevent undefined behavior
  epoch_reward(e) = max(
    10^12 >> halving_shift,              // discrete halving (bit-shift)
    circulating_supply * 0.001 / E_year  // tail floor
  )

  E_year = trailing 1,000-epoch moving average of epoch frequency
  Halving is epoch-counted, not wall-clock (partition-safe)
  At ~1 epoch per 10 minutes: 100,000 epochs ≈ 1.9 years

  Implementation note: the shift operand MUST be clamped to 63 (max
  for u64). At epoch 6,400,000 (~year 1218), unclamped shift = 64
  which is undefined behavior on most platforms. Clamping to 63 yields
  0 (10^12 >> 63 = 0), so the tail floor takes over — correct behavior.
```

> **Specification**
Emission halves every 100,000 epochs via bit-shift (`10^12 >> halving_shift`). Tail floor at 0.1% of circulating supply per year ensures perpetual service rewards. Shift operand clamped to 63 for u64 safety.

The theoretical ceiling is 2^64 μMHR, but it is never reached — tail emission asymptotically approaches it. The initial reward of 10^12 μMHR/epoch yields ~1.5% of the supply ceiling minted in the first halving period, providing strong bootstrap incentive. Discrete halving every 100,000 epochs is epoch-counted (no clock synchronization needed) and trivially computable via bit-shift on integer-only hardware.

The tail ensures ongoing proof-of-service rewards exist indefinitely, funding all service operators (relay, storage, compute). In practice, lost keys (estimated 1–2% of supply annually) offset tail emission, keeping effective circulating supply roughly stable after year ~10.

### Typical Costs

| Operation | Cost |
|-----------|------|
| Expected relay cost per packet | ~5 μMHR |
| Relay lottery payout (on win) | ~500 μMHR (5 μMHR ÷ 1/100 win probability) |
| Expected cost: 1 KB message, 5 hops | ~75 μMHR (~3 packets × 5 μMHR × 5 hops) |
| 1 hour of storage (1 MB) | ~50 μMHR |
| 1 minute of compute (contract execution) | ~30–100 μMHR |

The relay lottery pays out infrequently but in larger amounts. Expected value per packet is the same: `500 μMHR × 1/100 = 5 μMHR`. See [Stochastic Relay Rewards](payment-channels) for the full mechanism.

## All-Service Minting

All services — relay, storage, and compute — earn minting rewards. Minting is proportional to real economic activity (channel debits) and capped at a fraction of that activity, making self-dealing structurally unprofitable. A 2% service burn on every funded-channel payment creates a deflationary force that bounds supply even in isolated partitions.

### Why All Services Mint

The anti-gaming defense is **not** service-specific proofs. It is three mechanisms that work uniformly across all services:

1. **Non-deterministic assignment** — the client cannot choose who serves the request
2. **Net-income revenue cap** — total minting cannot exceed 50% of net economic activity
3. **Service burn + active-set scaling** — 2% burn on all funded-channel payments + emission scaled by partition size bounds isolated partition supply growth to at most `E_s` per epoch

Together, these guarantee that self-dealing in a connected network is **never profitable**, and that isolated partition minting is bounded by scaled emission per epoch (convergent over time due to halving). See the [Security Analysis](token-security#security-analysis) for the complete threat model.

### Non-Deterministic Assignment

Each service type has a natural mechanism that prevents the client from choosing the server:

```
Service assignment:

  Relay:    Mesh routing (Kleinberg greedy forwarding)
            Path determined by network topology, not client choice.
            Multi-hop paths include honest relays with high probability.
            Probability of ALL hops being attacker-controlled: X^hops
            (vanishingly small for typical path lengths).

  Storage:  DHT ring assignment
            responsible_node = DHT(hash(content_id || epoch_hash))
            The epoch_hash is unpredictable at request time, preventing
            the client from grinding content IDs to influence assignment.
            Replicas assigned to multiple DHT-ring positions for durability.

  Compute:  DHT ring assignment
            responsible_node = DHT(hash(job_spec || epoch_hash))
            Same mechanism as storage. The compute node is determined by
            the DHT ring, not by client choice.
```

The DHT ring is the same Kleinberg small-world ring used for routing. No new mechanism — storage and compute assignment reuse the existing topology.

### Unified Minting Formula

All service income contributes to one minting pool. The revenue cap uses **net income** (income minus spending per provider), not gross channel debits, to prevent [cycling attacks](token-security#attack-channel-cycling). Emission is scaled by the active set size to limit small-partition minting:

```
Minting formula (all services):

  For each provider P this epoch:
    P_income  = relay_income + storage_income + compute_income  (payments received)
    P_spending = total payments sent across all channels
    P_net     = max(0, P_income - P_spending)

  Active-set-scaled emission:
    active_nodes = number of nodes in epoch's active set
    reference_size = 100  (configurable protocol parameter)
    scale_factor = min(active_nodes, reference_size) / reference_size
    scaled_emission = emission(epoch) × scale_factor
      → 3-node partition: 3/100 × E = 0.03E
      → 100+ node partition: full E

  Revenue cap (net-income based):
    minting_eligible = Σ P_net for all providers P
    epoch_minting = min(scaled_emission, 0.5 × minting_eligible)

  Service burn:
    burn_rate = 0.02  (2% of every funded-channel payment)
    Burned amount is permanently destroyed before minting calculation.
    Provider receives 98% of channel payment; 2% is removed from supply.

  Distribution (gross-income based):
    provider_mint_share = (P_income / Σ all_income) × epoch_minting

  The cap uses net income (prevents cycling). Distribution uses gross
  income (rewards all service provision fairly). A relay earning 1000 μMHR
  and a storage node earning 1000 μMHR get the same distribution share.

  Why net income for the cap:
    In a round-trip cycle (A→B→A), every provider's income = spending → net = 0.
    Cycling produces zero minting, regardless of how many times MHR circulates.
    One-directional spending (real demand) produces positive net income.

  Why active-set scaling:
    Without scaling, a 3-node partition mints the same as a 10,000-node network.
    With scaling, the 3-node partition mints 3% of full emission — proportional
    to its size. The 2% burn provides additional friction (~4% reduction in
    attacker growth rate) and absorbs excess supply after partition merge.
```

### Service-Specific Payment Mechanics

The VRF stochastic lottery remains relay-specific — it is a bandwidth optimization, not a minting mechanism:

```
Payment mechanics by service:

  Relay:    VRF stochastic lottery per-packet (existing)
            Channel debit on lottery win (~1/100 packets)
            High-frequency, low-value: lottery reduces overhead by ~10x

  Storage:  Direct channel debit per-epoch per-agreement
            Low-frequency: one payment per storage agreement per epoch
            No lottery needed — per-event channel updates are affordable

  Compute:  Direct channel debit per-job
            Medium-frequency: one payment per computation
            No lottery needed — per-event channel updates are affordable
```

### Proof: Self-Dealing Is Unprofitable

```
Self-dealing with non-deterministic assignment + net-income cap:

  Attacker has X fraction of network economic capacity.
  Attacker generates Y MHR in fake service demand.

  Non-deterministic assignment routes:
    X × Y  → attacker's own nodes (internal transfer, net cost 0)
    (1-X) × Y → honest nodes (REAL cost to attacker)

  Net income (for revenue cap):
    Honest providers: net = (1-X)Y (received payment, didn't spend back to attacker)
    Attacker providers: net = max(0, XY - Y) = 0 (income < spending for X < 1)
    minting_eligible = (1-X)Y  (only honest providers contribute)

  Minting earned by attacker:
    attacker_share = XY / Y = X  (gross income share)
    attacker_minting = X × min(E, 0.5 × (1-X)Y)

  Attacker's net profit (assuming 0.5(1-X)Y < E):
    -(1-X)Y + X × 0.5 × (1-X)Y = (1-X)Y × (0.5X - 1)

  This is ALWAYS negative for any X < 1:
    X = 10%: net = -0.45Y × 0.9 = -0.855Y  (85.5% loss)
    X = 30%: net = -0.35Y × 0.7 = -0.49Y   (49% loss)
    X = 50%: net = -0.25Y × 0.5 = -0.375Y  (37.5% loss)
    X = 90%: net = -0.05Y × 0.1 = -0.045Y  (4.5% loss)
    X = 99%: net = -0.005Y × 0.01 = -0.005Y (0.5% loss)

  Self-dealing is NEVER profitable in a connected network.
  The net-income cap ensures the attacker's own internal transfers
  don't count toward minting eligibility.
```

Non-deterministic assignment forces the attacker to pay honest nodes. The net-income cap ensures the attacker's internal transfers (paying their own nodes) produce zero minting eligibility. Together, self-dealing in a connected network always loses money — the attacker spends real MHR on honest nodes and earns nothing from their internal circulation.

> **Key Insight**
Self-dealing is **never profitable** for any attacker fraction X < 1. The net-income cap zeroes out internal transfers, while non-deterministic assignment forces irrecoverable payment to honest nodes. Net profit = `(1-X)Y × (0.5X - 1)` — always negative.

> **Design Note**
This "never profitable" result requires that the network is connected and non-deterministic assignment is operational. In an [isolated partition](token-security#attack-isolated-partition) where the attacker controls all nodes, non-deterministic assignment is nullified — but the [trust-gated active set](token-security#trust-gated-active-set) + [merge-time trust audit](token-security#merge-time-trust-audit) (rejects untrusted partition minting on reconnection), active-set-scaled emission, and 2% service burn bound supply growth. See the [Security Analysis](token-security#security-analysis) for the full threat model.

### What We Don't Need

The three-mechanism defense (non-deterministic assignment + net-income cap + burn/scaling) makes several commonly proposed anti-gaming mechanisms unnecessary:

| Mechanism | Why not needed |
|-----------|----------------|
| Numerical trust scores | Binary trust neighborhoods handle free/paid boundary |
| Staking | Channel funding provides implicit Sybil cost |
| Slashing | Attacks are structurally unprofitable — punishment is redundant |
| Service-specific proof protocols | DHT assignment + bilateral verification sufficient |
| Dynamic pool rebalancing | Single pool, proportional distribution, market adjusts prices |

Three mechanisms. One formula. Zero trust assumptions.

### Bootstrap by Service Type

```
Bootstrap sequence:

  Phase 0: FREE TIER
    ├── Relay:   Trusted peers relay for free (works immediately)
    ├── Storage: Trusted peers store each other's data for free
    └── Compute: Nodes run their own contracts locally

  Phase 0.5: GENESIS SERVICE GATEWAY
    ├── Genesis gateway receives transparent MHR allocation
    ├── Gateway offers real services for fiat (relay, storage, compute)
    ├── Consumer fiat → MHR credit extensions → funded channels
    └── Real service demand enters the network for the first time

  Phase 1: DEMAND-BACKED SERVICE MINTING
    ├── Funded-channel traffic triggers relay VRF lottery
    ├── Storage/compute agreements generate direct channel debits
    ├── ALL channel debits (relay + storage + compute) earn minting
    ├── Revenue-capped minting prevents self-dealing (see above)
    └── MHR enters circulation backed by real demand across all services

  Phase 2: MARKET ECONOMY
    ├── Service providers spend MHR on other services
    ├── Competition drives prices toward marginal cost
    └── All service types have mature bilateral payment markets

  Phase 3: MATURE ECONOMY
    ├── Bilateral payments dominate all services
    ├── Minting becomes residual (decaying emission)
    └── Service prices emerge from supply/demand
```

Every service type earns minting from Phase 1. A $30 solar relay earns minting by forwarding packets. A node with spare disk earns minting by storing data. A node with a GPU earns minting by running compute jobs. The minting subsidy is proportional to economic contribution, not service type.

**Storage is a particularly low-barrier entry point.** Any device with spare disk space can offer [cloud storage](../applications/cloud-storage#earning-mhr-through-storage) and earn MHR through both bilateral payments and minting rewards. The marginal cost is near zero (idle disk space), so even modest demand generates income. For users who want to participate in the economy without running relay infrastructure, storage is the simplest starting point.

### Genesis-Anchored Minting

During bootstrap (before the first halving at epoch 100,000), minting eligibility requires a **GenesisAttestation** — a signed proof of recent connectivity to a genesis node. This completely eliminates the [isolated partition attack](token-security#attack-isolated-partition) during the most vulnerable period (high emission, low total supply).

```
GenesisAttestation {
    epoch_number: u64,              // epoch this attestation was issued
    attestor_id: NodeID,            // genesis node or attested peer
    attestor_sig: Ed25519Signature, // signature over (epoch_number || subject_id)
    chain_length: u8,               // 0 = genesis node itself, 1 = direct peer, etc.
    max_chain_length: u8,           // protocol parameter (default: 5)
}

How attestations propagate:
  1. Genesis nodes sign attestations for directly connected peers each epoch
     (chain_length = 1)
  2. Any node with a valid attestation (chain_length < max) can vouch for
     its own direct peers (chain_length + 1)
  3. Attestations propagate one hop per epoch via gossip
  4. TTL: attestations expire after 10 epochs — a node that loses genesis
     connectivity for >10 epochs can no longer mint
  5. Minting eligibility check:
     IF epoch_number < 100,000:  // bootstrap phase
       provider must have a valid GenesisAttestation (not expired, chain verified)
     ELSE:
       GenesisAttestation sunsets — trust-gated active set + merge-time audit take over

Why this works:
  - An isolated partition with no path to a genesis node gets ZERO minting
  - The attacker cannot forge attestations (Ed25519 signatures)
  - The attacker cannot relay attestations without actual connectivity
  - Legitimate isolated communities (natural partitions) also cannot mint
    during bootstrap — this is acceptable because the bootstrap period is
    when the genesis gateway is the primary MHR source anyway
```

**Post-bootstrap defense**: At epoch 100,000 (first halving), GenesisAttestation is retired. Post-bootstrap partition defense is provided by the [trust-gated active set](token-security#trust-gated-active-set) (minting requires ≥1 mutual trust link) and the [merge-time trust audit](token-security#merge-time-trust-audit) (rejects untrusted partition minting on reconnection). No attestation chains, no central authority, no expiring certificates. See [Partition Defense](token-security#partition-defense) for the complete design.

### Service Burn

A **2% burn** is applied to every funded-channel payment, permanently destroying the burned amount. This creates a deflationary force that provides friction in isolated partitions and absorbs excess supply after partition merge.

```
Service burn mechanics:

  On every funded-channel payment (relay lottery win, storage debit, compute debit):
    burn_amount = payment × 0.02
    provider_receives = payment × 0.98
    burn_amount is permanently destroyed (removed from circulating supply)

  Applies to:
    ✓ Relay VRF lottery wins (burn 2% of the payout)
    ✓ Storage per-epoch debits (burn 2% of the payment)
    ✓ Compute per-job debits (burn 2% of the payment)
    ✗ Free-tier trusted traffic (no payment = nothing to burn)
    ✗ Minting rewards (new supply, not a payment)

  Burn tracking:
    Each ServiceDebitSummary includes a burn_total field
    Epoch snapshot includes epoch_burn_total (sum of all burns)
    Burns are reflected in the CRDT ledger as reduced delta_earned
    (provider's delta_earned increases by 98% of payment, not 100%)

Burn role in isolated partitions:
  In an isolated partition, the attacker spends the minimum needed to
  saturate the minting cap (~2.04 × E_s per epoch). The burn on this
  activity is ~0.04 × E_s per epoch — a ~4% reduction in the attacker's
  supply growth rate (see Supply Dynamics Proof in Security Analysis).
  After reconnection, the 2% burn on the entire network's economic
  activity gradually absorbs the excess supply.

Effect on connected networks:
  In the normal (connected) network, burns reduce effective circulating supply.
  Combined with lost keys (~1-2% annual), the deflationary pressure is mild
  but creates a tighter long-term equilibrium. Tail emission (0.1% annual)
  compensates — the steady state is: tail_emission ≈ burns + lost_keys.
```

<!-- faq-start -->

## Frequently Asked Questions

<details className="faq-item">
<summary>How is MHR different from typical cryptocurrencies?</summary>

MHR is not a speculative asset — it is the internal currency for purchasing services (relay, storage, compute) from nodes outside your trust network. There is no ICO, no pre-mine, and no exchange mechanism built into the protocol. Supply is minted only through demand-backed proof-of-service: you earn MHR by providing real services through funded payment channels. The 2% service burn on every payment creates a deflationary counterforce.

</details>

<details className="faq-item">
<summary>What prevents someone from gaming the minting system by creating fake traffic?</summary>

Three mechanisms work together: non-deterministic assignment routes most service requests to honest nodes (the attacker cannot choose the server), the net-income revenue cap ensures internal transfers between the attacker's own nodes produce zero minting eligibility, and the 2% service burn imposes friction on every payment. Self-dealing in a connected network is mathematically proven to be unprofitable for any attacker controlling less than 100% of the network.

</details>

<details className="faq-item">
<summary>How does the emission schedule work and when does supply stop growing?</summary>

Emission starts at 1,000,000 MHR per epoch and halves every 100,000 epochs (approximately every 1.9 years). When the halved reward falls below a floor, tail emission kicks in at 0.1% of circulating supply per year, ensuring service providers always have a minting incentive. The theoretical supply ceiling of 2^64 μMHR is never reached — tail emission approaches it asymptotically. In practice, lost keys (1–2% annually) and the 2% service burn offset tail emission, keeping effective circulating supply stable.

</details>

<details className="faq-item">
<summary>Can storage and compute providers earn minting rewards, or only relay nodes?</summary>

All service types earn minting rewards equally. Relay, storage, and compute income all contribute to a single minting pool. Distribution is proportional to each provider's gross income share — a storage node earning 10% of total income gets 10% of epoch minting. The VRF stochastic lottery is relay-specific (a bandwidth optimization), but storage and compute use direct channel debits that count identically toward minting.

</details>

<details className="faq-item">
<summary>What is the genesis-anchored minting requirement during bootstrap?</summary>

During bootstrap (epoch 0–99,999), minting requires a GenesisAttestation — a signed proof of recent connectivity to a genesis node. These attestations propagate one hop per epoch via gossip, expire after 10 epochs, and form chains up to 5 hops. This completely eliminates isolated partition attacks during the most vulnerable period. After epoch 100,000, genesis attestation sunsets and the trust-gated active set with merge-time audit takes over.

</details>

<!-- faq-end -->

---

### Stochastic Relay Rewards
<!-- Source: docs/economics/payment-channels.md -->

# Stochastic Relay Rewards

Relay nodes are compensated through **probabilistic micropayments** rather than per-packet accounting. This dramatically reduces payment overhead on constrained radio links while providing the same expected income over time.

## Why Not Per-Packet Payment?

> **Key Insight**
Per-packet payment requires a channel state update for every batch of relayed packets. Even batched, this consumes significant bandwidth on LoRa links. The insight: relay rewards don't need to be deterministic — they can be probabilistic, like mining, achieving the same expected value with far less overhead.


## How Stochastic Rewards Work

```mermaid
graph TD
    A["Packet arrives at relay node"] --> B["Forward packet"]
    B --> C["VRF Lottery: VRF(relay_key, packet_hash)"]
    C --> D{"output < target?"}
    D -- "YES (1/100)" --> E["Win! 500 μMHR"]
    D -- "NO (99/100)" --> F["Nothing (no overhead)"]
    E --> G["Channel debit (sender pays)"]
    E --> H["Mining proof (epoch minting)"]
```

Each relayed packet is checked against a **VRF-based lottery**. The relay computes a Verifiable Random Function output over the packet, producing a deterministic but unpredictable result that anyone can verify:

```
Relay reward lottery (VRF-based):
  1. Relay computes: (vrf_output, vrf_proof) = VRF_prove(relay_private_key, packet_hash)
  2. Check: vrf_output < difficulty_target
  3. If win: reward = per_packet_cost × (1 / win_probability)
  4. Expected value per packet = reward × probability = per_packet_cost ✓
  5. Verification: VRF_verify(relay_public_key, packet_hash, vrf_output, vrf_proof)
```

> **Key Insight**
**Why VRF, not a random nonce?** If the relay chose its own nonce, it could grind through values until it found a winner for every packet, extracting the maximum reward every time. The VRF produces exactly **one valid output** per (relay key, packet) pair — the relay cannot influence the lottery outcome. The proof lets any party verify the result without the relay's private key.

The VRF used is **ECVRF-ED25519-SHA512-TAI** ([RFC 9381](https://www.rfc-editor.org/rfc/rfc9381)), which reuses the relay's existing Ed25519 keypair. VRF proof size is 80 bytes, included only in winning lottery claims (not in every packet).


### Example

| Parameter | Value |
|-----------|-------|
| Per-packet relay cost | 5 μMHR |
| Win probability | 1/100 |
| Reward per win | 500 μMHR |
| Expected value per packet | 5 μMHR (same) |
| Channel updates needed | 1 per ~100 packets (vs. every batch) |

A relay handling 10 packets/minute triggers a channel update approximately once every 10 minutes — a **10x reduction** in payment overhead compared to per-minute batching.

### Adaptive Difficulty

The win probability adjusts based on traffic volume. Each relay computes its own difficulty locally based on its observed traffic rate — no global synchronization needed:

```
Difficulty adjustment:
  target_updates_per_minute = 0.1  (one channel update per ~10 minutes)
  observed_packets_per_minute = trailing 5-minute moving average

  win_probability = target_updates_per_minute / observed_packets_per_minute
  win_probability = clamp(win_probability, 1/10000, 1/5)  // bounds

  difficulty_target = MAX_VRF_OUTPUT × win_probability

Traffic tiers (approximate):
  High-traffic links (>100 packets/min):   ~1/1000 probability, larger rewards
  Medium-traffic links (10-100 packets/min): ~1/100 probability
  Low-traffic links (<10 packets/min):     ~1/10 probability, smaller rewards

  Reward on win = per_packet_cost × (1 / win_probability)
  Expected value per packet = per_packet_cost (always, regardless of difficulty)
```

Low-traffic links use higher win probability to reduce variance — a relay handling only a few packets per hour will still receive rewards regularly. The difficulty is computed independently by each relay per-link, so different links on the same node may have different difficulties.

## Bilateral Payment Channels

Rewards are settled through bilateral channels between direct neighbors. Unlike Lightning-style multi-hop payment routing, Mehr uses simple per-hop channels:

- Only two parties need to coordinate
- Both parties are direct neighbors (by definition)
- No global coordination needed

### Channel State

```
ChannelState {
    channel_id: [u8; 16],       // truncated Blake3 hash (16 bytes)
    party_a: [u8; 16],          // destination hash (16 bytes)
    party_b: [u8; 16],          // destination hash (16 bytes)
    balance_a: u64,             // party A's current balance (8 bytes)
    balance_b: u64,             // party B's current balance (8 bytes)
    sequence: u64,              // monotonically increasing (8 bytes)
    sig_a: Ed25519Signature,    // party A's signature (64 bytes)
    sig_b: Ed25519Signature,    // party B's signature (64 bytes)
}
// Total: 16 + 16 + 16 + 8 + 8 + 8 + 64 + 64 = 200 bytes
```

### Channel Lifecycle

1. **Open**: Both parties agree on initial balances. Both sign the opening state (`sequence = 0`).
2. **Update**: On each lottery win, the balance shifts by the reward amount and `sequence` increments by 1. Both parties sign the updated state. Channel updates are infrequent — only triggered by wins.
3. **Settle**: Either party can request settlement. The settlement follows a **two-phase signing protocol** with timeout:

   ```
   Settlement atomicity protocol:

     Phase 1: PROPOSE
       Initiator creates SettlementRecord with current balances and sequence.
       Initiator signs it (sig_a or sig_b, depending on who initiates).
       Initiator sends the half-signed record to the counterparty.

     Phase 2: COUNTERSIGN
       Counterparty verifies: balances match local channel state, sequence matches.
       Counterparty signs the record (adding the second signature).
       Counterparty sends the fully-signed record back to the initiator.
       Both parties gossip the fully-signed record to the network.

     Timeout handling:
       settlement_timeout = 120 gossip rounds (~2 hours at 60-second rounds)

       If counterparty does not countersign within settlement_timeout:
         1. The half-signed record is DISCARDED (not published)
         2. The channel remains open with its current state
         3. The initiator may retry settlement
         4. After 3 failed settlement attempts (3 × timeout = ~6 hours):
            The initiator may file a UNILATERAL SETTLEMENT using the
            last mutually-signed ChannelState:
              UnilateralSettlement {
                  channel_id: [u8; 16],
                  last_state: ChannelState,    // must have both signatures
                  reason: enum { CounterpartyUnresponsive, ChannelAbandonment },
                  filed_by: NodeID,
                  signature: Ed25519Signature,
              }
            The unilateral settlement enters a challenge window (2,880 gossip
            rounds / ~48 hours). The counterparty can respond with a
            higher-sequence state to override.

     Partial (one-signature) state:
       A SettlementRecord with only one signature is NEVER published to the
       network. It is strictly a local, ephemeral negotiation artifact.
       The CRDT ledger only accepts records with both valid signatures.
       This is all-or-nothing — there is no partial settlement state.

     Channel close after repeated failure:
       If settlement fails repeatedly (counterparty consistently offline):
         1. After 4 epochs of no updates: standard abandonment rules apply
            (either party unilaterally closes with last mutually-signed state)
         2. The unilateral close enters the 2,880-round challenge window
         3. If unchallenged: the close is finalized and balances are settled
         4. Remaining balance returns to each party per the last signed state
   ```

   Both sign a `SettlementRecord` whose `final_sequence` matches the current channel `sequence`. The record is gossiped to the network and applied to the [CRDT ledger](crdt-ledger). The channel remains open after settlement — subsequent lottery wins continue from the settled point.
4. **Dispute**: If one party submits an old state, the counterparty can submit a higher-sequence state within a **2,880 gossip round challenge window** (~48 hours at 60-second rounds). The higher sequence always wins.
5. **Abandonment**: If a channel has no updates for **4 epochs**, either party can unilaterally close with the last mutually-signed state. This prevents permanent fund lockup.

### Settlement Timing

Lottery wins accumulate as local channel state updates (balance shifts + sequence increments). Settlements to the CRDT ledger are **not** created per-win — they are created when either party decides to finalize:

```
Settlement triggers:
  - Either party requests cooperative settlement
  - Channel dispute (one party publishes an old state)
  - Channel abandonment (4 epochs of inactivity)
  - Periodic finalization (recommended: once per epoch)

Between settlements, interim balances are NOT gossiped.
Only the two parties track the current ChannelState locally.
```

This preserves the stochastic lottery's bandwidth savings: a relay handling 10 packets/minute triggers ~6 local channel updates per hour, but settlements to the CRDT ledger happen much less frequently.

### Sequence Number Semantics

The `sequence` field is a monotonically increasing version number:

- Each update increments `sequence` by 1; both parties must sign the same sequence
- A `SettlementRecord` references `final_sequence` — the sequence of the state being settled
- After settlement, the channel continues with `sequence > final_sequence`
- Dispute resolution: higher `sequence` always wins, regardless of settlement history
- Replay protection: the CRDT ledger rejects settlements where `final_sequence` is not greater than the last settled sequence for the same `channel_id`

## Multi-Hop Payment

When Alice sends a packet through Bob → Carol → Dave, each relay independently runs the VRF lottery:

```mermaid
graph LR
    Alice --> Bob --> Carol --> Dave
    Bob -. "lottery?" .-> Bob
    Carol -. "lottery?" .-> Carol
```

A lottery win triggers compensation through one or both mechanisms:

1. **Channel debit** (if a channel exists with the upstream sender): Bob's win debits Alice's channel with Bob; Carol's win debits Bob's channel with Carol. A **2% service burn** is applied — the relay receives 98% of the payout, and 2% is permanently destroyed. This is the steady-state mechanism once MHR is circulating.
2. **Mining proof** (demand-backed): The channel debit (post-burn) is recorded as a service proof entitling the relay to a share of the epoch's [minting reward](token-economics#demand-backed-proof-of-service-mining-mhr-genesis) — but only if the packet traversed a funded payment channel. Free-tier trusted traffic does not earn minting rewards. Relay channel debits contribute to the same minting pool as storage and compute debits (see [All-Service Minting](mhr-token#all-service-minting)). Minting is the dominant income source during bootstrap and provides a baseline subsidy that decays over time.

Most packets trigger no channel update at all. Each hop is independent — no end-to-end payment coordination.

## Demand-Backed Minting Eligibility

A relay channel debit is **minting-eligible** only if the packet that triggered it traversed a funded payment channel with the upstream sender. This is one component of the [anti-gaming defense](mhr-token#proof-self-dealing-is-unprofitable):

```
Minting eligibility rule (relay):

  VRF win on packet P at relay R:
    IF upstream channel(sender, R) is funded (balance > 0):
      → Channel debit counts toward R's epoch minting share
    ELSE (free-tier trusted traffic, or no funded channel):
      → Does NOT count toward minting

  Why:
    Without this rule, a Sybil attacker can fabricate traffic between
    colluding nodes and claim minting rewards for zero-cost "work."

    With this rule, generating minting-eligible traffic requires
    spending real MHR through funded channels.
```

**Free-tier trusted traffic**: Trusted peers relay for free — this is unchanged. Free relay is a benefit of the trust network, not a minting mechanism.

**Channel-funded payments (mechanism 1)**: Unaffected. When a relay wins the lottery and has a funded channel with the sender, the channel debit happens regardless of minting eligibility.

**All-service minting**: Relay channel debits contribute to the same minting pool as storage and compute debits. See [All-Service Minting](mhr-token#all-service-minting) for the unified model.

## Revenue-Capped Minting

Even with demand-backed minting eligibility, an attacker could spend MHR on funded channels to generate minting-eligible traffic. The full anti-gaming defense combines **net-income revenue cap**, **non-deterministic service assignment**, and **service burn + active-set scaling** to ensure self-dealing is never profitable in a connected network and that isolated partition supply growth is bounded by scaled emission per epoch:

```
Revenue-capped minting formula (net-income based, with scaling and burn):

  Service burn: 2% of every funded-channel payment is permanently destroyed.
    Provider receives 98% of each channel payment.

  For each provider P this epoch:
    P_income  = total payments received for services (post-burn, 98%)
    P_spending = total payments sent across all channels
    P_net     = max(0, P_income - P_spending)
      → only funded-channel activity counts

  minting_eligible = Σ P_net for all providers P

  Active-set-scaled emission:
    scaled_emission = emission_schedule(epoch) × min(active_set_size, 100) / 100

  effective_minting(epoch) = min(
      scaled_emission,                       // active-set-scaled halving ceiling
      minting_cap × minting_eligible         // 0.5 × net economic activity
  )

  minting_cap = 0.5

  Why net income (not gross debits):
    Cycling MHR back and forth inflates gross debits but produces
    net income = 0 for every participant. Only one-directional flows
    (real demand) produce positive net income.

  Why active-set scaling:
    A 3-node partition gets 3% of full emission (not 100%).
    The 2% burn provides additional friction (~4% reduction in
    attacker growth rate) and absorbs excess supply after merge.
```

See [Revenue-Capped Minting](token-economics#revenue-capped-minting) in the MHR Token spec for the complete self-dealing analysis, and the [Security Analysis](token-security#security-analysis) for all attack vectors including cycling prevention and isolated partition bounds.

**Impact on supply curve:**

- Early network (low traffic): actual minting is well below the emission schedule. Supply grows slowly, tracking real economic activity.
- Growing network: actual minting approaches emission ceiling as service fees increase.
- Mature network: revenue cap is rarely binding (net service activity far exceeds the emission schedule). Supply follows the standard halving schedule.

## Efficiency on Constrained Links

| Metric | Value |
|--------|-------|
| State update size | 200 bytes |
| Average updates per hour (1/100 prob, 10 pkts/min) | ~6 |
| Bandwidth overhead at 1 kbps LoRa | ~0.3% |
| Compared to per-minute batching | **~8x reduction** |

The stochastic model fits within [Tier 2 (economic)](../protocol/network-protocol#bandwidth-budget) of the gossip bandwidth budget even on the most constrained links.

## Trusted Peers: Free Relay

Nodes relay traffic for [trusted peers](trust-neighborhoods) for free — no lottery, no channel updates. The stochastic reward system only activates for traffic between non-trusted nodes. This mirrors the real world: you help your neighbors for free, but charge strangers for using your infrastructure.

<!-- faq-start -->

## Frequently Asked Questions

<details className="faq-item">
<summary>Why use a probabilistic lottery instead of paying per packet?</summary>

Per-packet payment requires a channel state update for every batch of relayed packets, consuming significant bandwidth — especially on constrained LoRa links. The VRF-based stochastic lottery achieves the same expected value (e.g., 5 μMHR per packet) but only triggers a channel update on wins (~1 in 100 packets). This reduces payment overhead by approximately 8–10x while preserving the same average income for relay operators.

</details>

<details className="faq-item">
<summary>How does the VRF prevent a relay from cheating the lottery?</summary>

The VRF (ECVRF-ED25519-SHA512-TAI, RFC 9381) produces exactly one valid output per (relay private key, packet hash) pair. The relay cannot "grind" through nonces to find a winning value — there is only one possible output. Anyone can verify the result using the relay's public key without accessing the private key. Changing the relay key would mean changing the node identity and forfeiting all accumulated reputation.

</details>

<details className="faq-item">
<summary>What happens if the other party in a payment channel goes offline?</summary>

If the counterparty does not respond to a settlement request within 120 gossip rounds (~2 hours), the initiator can retry. After 3 failed attempts (~6 hours), the initiator may file a unilateral settlement using the last mutually-signed channel state. This enters a 48-hour challenge window where the counterparty can respond with a higher-sequence state. If a channel has no updates for 4 full epochs, either party can unilaterally close to prevent permanent fund lockup.

</details>

<details className="faq-item">
<summary>Do payment channels require end-to-end coordination across multiple hops?</summary>

No. Unlike Lightning-style multi-hop payment routing, Mehr uses simple per-hop bilateral channels between direct neighbors. Each relay independently runs the VRF lottery and debits the channel with its upstream sender. There is no end-to-end payment coordination — a packet traversing 5 hops involves 5 independent per-hop lottery checks, each settled on its own bilateral channel.

</details>

<details className="faq-item">
<summary>How does the 2% service burn work with stochastic relay rewards?</summary>

When a relay wins the VRF lottery, 2% of the payout is permanently destroyed (burned) before crediting the relay. For example, on a 500 μMHR lottery win, the relay receives 490 μMHR and 10 μMHR is removed from circulation. This burn applies uniformly to all funded-channel payments — relay wins, storage debits, and compute debits alike. It creates a deflationary counterforce that bounds supply growth and absorbs excess supply after partition merges.

</details>

<!-- faq-end -->

---

### Token Economics
<!-- Source: docs/economics/token-economics.md -->

# Token Economics

## Economic Architecture

Mehr has a simple economic model: **free between friends, paid between strangers.**

> **Key Insight**
The economic layer activates only at trust boundaries. A local mesh where everyone trusts each other has **zero economic overhead** — no tokens, no channels, no settlements needed.

```mermaid
graph LR
    subgraph TRUST["TRUST NETWORK (free)"]
        Alice["Alice"] <-->|"free"| Bob["Bob"]
        Alice <-->|"free"| Dave["Dave"]
    end

    subgraph PAID["PAID ECONOMY (MHR)"]
        Bob -->|"relay"| Carol["Carol"]
        Carol -->|"storage"| StorageNode["Storage Node"]
        Carol -->|"compute"| ComputeNode["Compute Node"]
        Bob -->|"channel debit"| Mint["Minting Pool"]
        Carol -->|"channel debit"| Mint
        StorageNode -->|"channel debit"| Mint
        ComputeNode -->|"channel debit"| Mint
    end
```

### Free Tier (Trust-Based)

- Traffic between [trusted peers](trust-neighborhoods) is **always free**
- No tokens, no channels, no settlements needed
- A local mesh where everyone trusts each other has **zero economic overhead**

### Paid Tier (MHR)

- Services crossing trust boundaries earn through [bilateral payment channels](payment-channels)
- Relay uses [stochastic lottery](payment-channels) for bandwidth efficiency; storage and compute use direct channel debits
- All channel debits contribute to the minting pool (proportional, revenue-capped)
- Settled via [CRDT ledger](crdt-ledger)

## Genesis and Bootstrapping

The bootstrapping problem — needing MHR to use services, but needing to provide services to earn MHR — is solved by separating free-tier operation from the paid economy:

### Free-Tier Operation (No MHR Required)

- **Trusted peer communication is always free** — no tokens needed
- **A local mesh works with zero tokens in circulation**
- The protocol is fully functional without any MHR — just limited to your trust network

### Demand-Backed Proof-of-Service Mining (MHR Genesis)

All services — relay, storage, and compute — earn minting rewards proportional to their channel debits. The **funding source** depends on the economic context:

1. **Minting (subsidy, demand-backed)**: Each epoch, the emission schedule determines the minting ceiling. Actual minting is distributed proportionally to all service providers based on their channel debits during that epoch — but only debits from **funded payment channels** are minting-eligible. Free-tier trusted traffic does not earn minting rewards. This demand-backed requirement ensures minting reflects real economic activity, not fabricated traffic.

2. **Channel debit (market)**: Service providers earn directly from clients through [payment channels](payment-channels). Relay uses a [stochastic lottery](payment-channels) for bandwidth efficiency; storage and compute use direct per-epoch or per-job channel debits. This becomes the dominant income source as MHR enters circulation.

Both mechanisms coexist. As the economy matures, channel-funded service payments naturally replace minting as the primary income source, while the decaying emission schedule ensures the transition is smooth.

```
Service provider compensation per epoch:

  Epoch mint pool: max(10^12 >> (epoch / 100_000), tail_floor)
    → new supply created (not transferred from a pool)
    → halves every 100,000 epochs; floors at 0.1% annual inflation

  Active-set-scaled emission:
    scaled_emission = epoch_mint_pool × min(active_set_size, 100) / 100
    → 3-node partition: 3% of full emission
    → 100+ nodes: full emission

  Service burn (2%):
    Every funded-channel payment burns 2% before crediting the provider.
    Provider receives 98% of the channel payment.
    Burned amount is permanently destroyed (removed from supply).

  Per-provider net income (post-burn):
    P_income  = relay + storage + compute payments received (after burn)
    P_spending = total payments sent across all channels
    P_net     = max(0, P_income - P_spending)
    → only funded-channel activity counts (free-tier excluded)

  Revenue-capped minting (net-income based):
    minting_eligible = Σ P_net for all providers P
    epoch_minting = min(scaled_emission, 0.5 × minting_eligible)

  Provider P's mint share: epoch_minting × (P_income / Σ all_income)
    → distribution uses gross income (rewards all service provision)
    → a storage node earning 10% of total income gets 10% of minting

  Channel revenue: direct payments from clients (separate from minting)
    → relay: VRF lottery wins debited from sender channels (98% after burn)
    → storage: per-epoch debits from storage agreements (98% after burn)
    → compute: per-job debits from compute agreements (98% after burn)

  Total provider income = mint share + channel revenue
```

### Genesis Service Gateway

The bootstrapping problem is solved by a **Genesis Service Gateway** — a known, trusted operator that provides real services for fiat and bootstraps the MHR economy with genuine demand:

> **Trade-off**
The genesis gateway centralizes initial MHR distribution — a deliberate bootstrapping dependency. Genesis attestation sunsets at epoch 100,000 (first halving, ~1.9 years), after which the economy is fully decentralized.

1. **Transparent allocation**: The genesis gateway operator receives a disclosed MHR allocation. No hidden allocation, no ICO — the amount is visible in the ledger from epoch 0.
2. **Competitive fiat pricing**: The gateway offers relay, storage, and compute at market-competitive fiat prices (see [Initial Pricing](#initial-pricing) below).
3. **Funded channels**: Consumer fiat payments are converted to MHR credit extensions, creating funded payment channels. This generates the first real service demand on the network (relay, storage, compute).
4. **Demand-backed minting**: All funded-channel activity — relay traffic, storage agreements, compute jobs — earns minting rewards proportional to channel debits, backed by actual economic activity.
5. **MHR circulation**: Minted MHR enters circulation — all service providers can spend it on other services.
6. **Decentralization**: As more operators join and offer competing services, the genesis gateway becomes one of many providers. The economy transitions from gateway-bootstrapped to fully market-driven.

### Bootstrap Sequence

1. Genesis gateway receives transparent MHR allocation, begins offering fiat-priced services (relay, storage, compute)
2. Genesis nodes begin signing [GenesisAttestations](mhr-token#genesis-anchored-minting) for connected peers — minting eligibility requires attestation during bootstrap
3. Nodes form local meshes (free between trusted peers, no tokens)
4. Consumers pay fiat to genesis gateway → funded channels created for all service types
5. Funded-channel activity triggers demand-backed minting: relay VRF lottery wins + storage/compute direct debits. 2% of each payment is [burned](mhr-token#service-burn).
6. All service providers (relay, storage, compute) with valid genesis attestations earn minting proportional to their channel debits
7. Providers open payment channels and begin spending MHR on other services
8. More operators join, offer competing services across all types, prices fall toward marginal cost
9. At epoch 100,000 (first halving): genesis attestation sunsets — [trust-gated active set](token-security#trust-gated-active-set) + [merge-time trust audit](token-security#merge-time-trust-audit) provide post-bootstrap partition defense
10. Market pricing emerges from supply/demand

### Trust-Based Credit

Trusted peers can [vouch for each other](trust-neighborhoods#trust-based-credit) by extending transitive credit. Each node configures the credit line it extends to its direct trusted peers (e.g., "I'll cover up to 1000 μMHR for Alice"). A friend-of-a-friend gets a configurable ratio (default 10%) of that direct limit — backed by the vouching peer's MHR balance. If a credited node defaults, the voucher absorbs the debt. This provides an on-ramp for new users without needing to earn MHR first.

**Free direct communication works immediately** with no tokens at all. MHR is only needed when your packets traverse untrusted infrastructure.

### Revenue-Capped Minting

The emission schedule sets a ceiling, but actual minting per epoch is capped at a fraction of **net economic activity** across all service types. Emission is further scaled by the partition's active set size, and a 2% [service burn](mhr-token#service-burn) on every funded-channel payment creates a deflationary counterforce:

```
Revenue-capped minting formula:

  For each provider P this epoch:
    P_income  = total payments received for services (relay + storage + compute)
              (post-burn: provider receives 98% of channel payment)
    P_spending = total payments sent across all channels
    P_net     = max(0, P_income - P_spending)
      → only funded-channel activity counts (free-tier excluded)

  minting_eligible = Σ P_net for all providers P

  Active-set-scaled emission:
    scaled_emission = emission_schedule(epoch) × min(active_set_size, 100) / 100

  effective_minting(epoch) = min(
      scaled_emission,                          // active-set-scaled halving ceiling
      minting_cap × minting_eligible            // 0.5 × net economic activity
  )

  minting_cap = 0.5  (minting can never exceed 50% of net service activity)

  Service burn: 2% of every funded-channel payment permanently destroyed.
  This reduces circulating supply and imposes friction on isolated partition
  supply growth (~4% reduction in attacker growth rate per epoch).
```

**Why net income, not gross debits:** Gross debits can be inflated by cycling — two colluding nodes pass the same MHR back and forth, each pass creating new "debits." Net income eliminates this: a round-trip produces income = spending → net = 0 → zero minting. See [Channel Cycling](token-security#attack-channel-cycling) in the Security Analysis for the full defense.

**Why self-dealing is unprofitable — the complete analysis:**

Self-dealing means the attacker controls both the client and the server. The channel debit between them is an internal transfer (net cost 0). The attacker's only gain is minting. The defense has two parts:

1. **Non-deterministic assignment** forces the attacker to pay honest nodes for most of their fake demand
2. **Net-income revenue cap** limits the minting the attacker can capture

```
Self-dealing attack analysis (with non-deterministic assignment):

  Setup:
    Attacker controls X fraction of network economic capacity
    Attacker generates Y MHR in fake service demand

  Non-deterministic assignment:
    Relay: mesh routing sends packets through honest relays (topology-determined)
    Storage: DHT assigns honest nodes for (1-X) fraction of requests
    Compute: DHT assigns honest nodes for (1-X) fraction of requests

  Cost to attacker:
    (1-X) × Y → paid to honest nodes (REAL, irrecoverable cost)
    X × Y → paid to own nodes (internal transfer, net 0)

  Attacker's net income (for minting cap):
    Honest providers' net: (1-X) × Y (they received, didn't spend to attacker)
    Attacker providers' net: 0 (received X×Y from own nodes, internal transfer)
    But attacker also spent (1-X)Y to honest nodes, so:
      Attacker's total spending: Y
      Attacker's total income: X × Y (from own fake demand)
      Attacker's net: max(0, XY - Y) = 0 (since X < 1)

  Revenue for attacker:
    Minting share = (attacker_income / total_income) × epoch_minting
    = (XY / Y) × min(E, 0.5 × minting_eligible)
    = X × min(E, 0.5 × (1-X)Y)    // only honest providers have positive net

  Attacker's profit (assuming 0.5(1-X)Y < E):
    -(1-X)Y + X × 0.5(1-X)Y = (1-X)Y × (0.5X - 1)

  This is ALWAYS negative for X < 1. The net-income cap makes the
  defense STRONGER than the gross-debit analysis (which allowed profit
  at X > 67%). With net income, self-dealing in a connected network is
  NEVER profitable — the attacker's own net income is always 0.
```

**Important**: The "never profitable" result applies to the **connected** network case where non-deterministic assignment routes most demand to honest nodes. In an [isolated partition](token-security#attack-isolated-partition) where the attacker controls all nodes, non-deterministic assignment is nullified — but the [trust-gated active set](token-security#trust-gated-active-set) + [merge-time trust audit](token-security#merge-time-trust-audit) (rejects untrusted minting on reconnection), active-set scaling, and service burn bound supply growth.

**What happens to "unminted" emission:**

- During early bootstrap, net service activity is small, so actual minting is well below the emission schedule
- The difference is NOT minted — it is simply not created (supply grows slower)
- As traffic grows, actual minting approaches the emission schedule ceiling
- In mature economy, the cap is rarely binding (net service activity far exceeds the emission schedule)
- The 2% service burn continuously removes supply, creating a tighter equilibrium than emission alone would suggest

This changes the supply curve: instead of predictable emission, supply growth tracks actual economic activity minus burns. Early supply grows slowly (good — prevents speculation without real usage), mature supply follows the emission schedule minus the burn rate. The steady-state effective supply is where `minting ≈ burns + lost_keys`.

### Initial Pricing

The genesis gateway prices services at or slightly above market competitors. This is deliberate — the goal is fair pricing with operational margin, not undercutting.

```
Initial pricing strategy:

  Principle: Price at market rate with overhead, NOT undercutting.

  The genesis gateway publishes maximum prices (ceilings). These serve as a
  ceiling that competitors can undercut as they join. The gateway can initially
  run on AWS/cloud infrastructure — it needs margin to cover that cost.

  Service             Market Benchmark              Genesis Ceiling
  ────────────────────────────────────────────────────────────────────
  Storage             AWS S3: $0.023/GB/mo          ~$0.02/GB/mo
  Internet gateway    ISP: $30-100/mo               ~$30/mo
  Compute             AWS Lambda: ~$0.20/1M req     At market
  Relay (per-packet)  Bundled in gateway price      ~5 μMHR

  Rationale:
  - Storage: At market, not below — no reason to subsidize
  - Gateway: Match ISP rate; value is privacy/resilience, not cheapness
  - Compute: No reason to undercut cloud pricing initially
  - Relay: Derived from gateway fiat price ÷ expected packet volume
```

**How prices fall over time:**

```
Price evolution:

  Genesis:     Gateway sets ceiling (market rate + overhead)
  Growth:      New providers enter, set prices ≤ ceiling to attract users
  Maturity:    Competition drives prices toward marginal cost
               (Mehr's marginal cost is low — spare bandwidth/disk on existing devices)
```

The genesis gateway doesn't need to be cheapest. It needs to be **trusted, available, and fairly priced**. Price competition comes from the market, not from subsidized undercutting. The gateway's fiat-to-MHR conversion rate becomes the initial exchange rate for MHR.

### Genesis Gateway Discovery

New nodes discover the genesis gateway through DNS:

```
Genesis gateway discovery:

  1. Well-known DNS domain resolves to genesis gateway IP(s)
  2. Hardcoded fallback list in daemon binary (in case DNS is unavailable)
  3. DNS is for initial contact only — once connected, gossip takes over
  4. Multiple DNS records for redundancy (A/AAAA records)

  Note: DNS is used ONLY for initial genesis gateway discovery,
  not for ongoing protocol operation. See roadmap Milestone 1.2.
```

This ties into the existing bootstrap mechanism (Milestone 1.2 in the [roadmap](../development/roadmap#milestone-12-bootstrap--peer-discovery)), elevating DNS from "optional" to the primary method for locating genesis gateways.

## Why One Global Currency

MHR is a single global unit of account, not a per-community token. This is a deliberate design choice.

### The Alternative: Per-Community Currencies

If each isolated community minted its own token, connecting two communities would require a currency exchange — someone to set an exchange rate, provide liquidity, and settle trades. On a mesh network of 50–500 nodes, there is not enough trading volume to sustain a functioning exchange market. The complexity (order books, matching, dispute resolution) vastly exceeds what constrained devices can support.

### How One Currency Works Across Partitions

When two communities operate in isolation:

1. **Internally**: Both communities communicate free between trusted peers — no MHR needed
2. **Independently**: Each community mints MHR via proof-of-service, proportional to actual service activity (relay, storage, compute). The [CRDT ledger](crdt-ledger) tracks balances independently on each side
3. **On reconnection**: The CRDT ledger merges automatically (CRDTs guarantee convergence). Both communities' MHR is valid because it was earned through real work, not printed arbitrarily

MHR derives its value from **labor** (relaying, storage, compute), not from community membership. One hour of relaying in Community A is roughly equivalent to one hour in Community B. Different hardware costs are reflected in **market pricing** — nodes set their own per-byte charges — not in separate currencies.

### Fiat Exchange

MHR has no official fiat exchange rate. The protocol includes no exchange mechanism, no order book, no trading pair. But MHR buys real services — bandwidth, storage, compute, content access — so it has real value. People will trade it for fiat currency, whether through informal markets, OTC trades, or external exchanges.

This is expected and not inherently harmful.

**Why exchange doesn't break the system:**

```mermaid
graph TD
    Operator["Operator earns MHR"] --> Sells["Sells for fiat"]
    Sells --> Pays["Pays electricity bill"]
    Sells --> Buyer["Buyer gets MHR"]
    Buyer --> Spends["Spends on network services"]
    Operator --> RealWork["Network received\nreal work (relay)"]
    Buyer --> RealDemand["Network received\nreal demand (usage)"]
```

1. **Purchased MHR is legitimate.** If someone buys MHR with fiat instead of earning it through relay, the seller earned it through real work. The network benefited from that work. The buyer funds network infrastructure indirectly — identical to buying bus tokens.

2. **MHR derives value from utility.** Its value comes from the services it buys, not from artificial scarcity. If the service economy is healthy, MHR has value regardless of exchange markets.

3. **Hoarding is self-correcting.** Someone who buys MHR and holds it is funding operators (paying fiat for earned MHR) while removing tokens from circulation. Remaining MHR becomes more valuable per service unit, incentivizing earning through service provision. Tail emission (0.1% annual) mildly dilutes idle holdings.

**What could go wrong:**

| Risk | Mitigation |
|------|-----------|
| **Deflationary spiral** (hoarding prevents spending) | Tail emission; free tier ensures basic functionality regardless |
| **Speculation** (price detaches from utility) | Utility value creates a floor; MHR has no use outside the network |
| **Regulatory attention** | Protocol doesn't facilitate exchange; users must understand their jurisdiction |

**Internal price discovery** still works as designed — service prices float based on supply and demand:

```
Abundant relay capacity + low demand → relay prices drop (in μMHR)
Scarce relay capacity + high demand  → relay prices rise (in μMHR)
```

Users don't need to know what 1 μMHR is worth in fiat. They need to know: "Can I afford this service?" — and the answer is usually yes, because they earn MHR by providing services. The economy is circular even if some participants enter through fiat exchange.

### Gateway Operators (Fiat Onramp)

The [Genesis Service Gateway](#genesis-service-gateway) is the first instance of this pattern. The same mechanics — trust extension, credit lines, fiat billing — apply to all subsequent gateway operators. As more gateways join, the economy decentralizes and pricing becomes competitive.

Not everyone wants to run a relay. Pure consumers — people who just want to use the network — should be able to pay with fiat and never think about MHR. **Gateway operators** make this possible.

A gateway operator is a trusted intermediary who bridges fiat payment and MHR economics. The consumer interacts with the gateway; the gateway interacts with the network. This uses existing protocol mechanics — no new wire formats or consensus changes.

```mermaid
graph LR
    subgraph Consumer
        Signup["Signs up\n(fiat payment)"]
        Uses["Uses network\n(messages, content,\nstorage, etc.)"]
        Bill["Monthly fiat bill"]
    end

    subgraph Gateway
        Trust["Adds consumer to trusted_peers\nExtends credit via CreditState"]
        Relay["Gateway relays for free\n(trusted peer = free)"]
        Earns["Gateway earns MHR through\nrelay + receives fiat\nfrom consumers"]
    end

    subgraph Network
        Paid["Paid relay\nto wider network\n(gateway pays MHR)"]
    end

    Signup --> Trust
    Uses --> Relay
    Relay --> Paid
    Bill --> Earns
```

**How it works:**

1. **Sign-up**: Consumer pays the gateway in fiat (monthly subscription, prepaid, pay-as-you-go — the gateway chooses its business model)
2. **Trust extension**: Gateway adds the consumer to `trusted_peers` and extends a credit line via [CreditState](trust-neighborhoods#trust-based-credit). The consumer's traffic through the gateway is free (trusted peer relay)
3. **Network access**: The consumer uses the network normally. Their traffic reaches the gateway for free, and the gateway pays MHR for onward relay to untrusted nodes
4. **Settlement**: The gateway earns MHR through service minting (relay, storage, compute) + charges fiat to consumers. The spread between fiat revenue and MHR costs is the gateway's margin

**The consumer never sees MHR.** From their perspective, they pay a monthly bill and use the network. Like a mobile carrier — you don't think about interconnect fees between networks.

```
Trust-based gateway mechanics:

  Gateway's TrustConfig:
    trusted_peers: { consumer_1, consumer_2, ... }
    cost_overrides: { consumer_1: 0, consumer_2: 0 }  // free for consumers

  Gateway's CreditState per consumer:
    credit_limit: proportional to fiat subscription tier
    rate_limit: prevents abuse (e.g., 10 MB/epoch for basic tier)

  Consumer's view:
    - No MHR wallet needed
    - No payment channels
    - No economic complexity
    - Just "install app, sign up, use"
```

**Why this works without protocol changes:**

| Mechanism | Already Exists |
|-----------|---------------|
| Free relay for trusted peers | [Trust Neighborhoods](trust-neighborhoods#free-local-communication) |
| Credit extension | [CreditState](trust-neighborhoods#trust-based-credit) |
| Rate limiting | Per-epoch credit limits in CreditState |
| Abuse prevention | Gateway revokes trust on non-payment (fiat side) |

**Gateway business models:**

| Model | Description | Consumer Experience |
|-------|-------------|-------------------|
| **Subscription** | Monthly fiat fee for a usage tier | Like a phone plan |
| **Prepaid** | Buy credit in advance, use until depleted | Like a prepaid SIM |
| **Pay-as-you-go** | Fiat bill based on actual usage | Like a metered utility |
| **Freemium** | Free tier (rate-limited) + paid upgrade | Like free WiFi with premium option |

**Gateway incentives:**

- Gateways earn minting rewards across all services they provide (relay, storage, compute)
- Gateways earn fiat from consumer subscriptions
- Gateways with many consumers generate high channel debit volume = proportionally more minting
- Competition between gateways drives prices toward cost (standard market dynamics)

## Security Considerations

<details className="security-item">
<summary>Gateway Failure or Censorship</summary>

**Vulnerability:** A gateway operator goes offline or begins censoring traffic, stranding consumers who depend on it for network access.

**Mitigation:** Consumers can switch gateways at any time — identity is self-certifying and not tied to any gateway. Multiple gateways compete in any area with demand. No lock-in exists; the barrier to switching is zero.

</details>

<details className="security-item">
<summary>Gateway Overcharging</summary>

**Vulnerability:** A gateway operator charges excessive fiat fees for network access, exploiting consumers who don't understand the underlying economics.

**Mitigation:** Market competition between gateways drives prices toward cost. Consumers can compare pricing across providers. Low switching cost means overcharging gateways lose customers.

</details>

<details className="security-item">
<summary>Consumer Abuse of Gateway Credit</summary>

**Vulnerability:** A consumer signs up with a gateway, consumes network resources on credit, then defaults on fiat payment.

**Mitigation:** The gateway revokes trust and cuts off credit immediately. Fiat non-payment is handled off-protocol through standard billing mechanisms. Rate limiting via per-epoch credit caps in CreditState prevents abuse from accumulating before detection.

</details>

## Economic Design Goals

- **Utility-first**: MHR is designed for purchasing services. Fiat exchange may emerge but the protocol's health doesn't depend on it, and the internal economy functions as a closed loop for participants who never touch fiat.
- **Transparent genesis**: Disclosed genesis allocation to the gateway operator, visible in the ledger from epoch 0. No ICO, no hidden allocation, no insider advantage.
- **Demand-backed minting**: Funded payment channels required for minting eligibility across all service types. Non-deterministic assignment + net-income revenue cap guarantee self-dealing is never profitable in connected networks. Isolated partition damage is defended by [trust-gated active set](token-security#trust-gated-active-set) + [merge-time trust audit](token-security#merge-time-trust-audit) + active-set scaling + 2% service burn. See [Security Analysis](token-security#security-analysis).
- **Spend-incentivized**: Tail emission (0.1% annual) mildly dilutes idle holdings. Lost keys (~1–2% annually) permanently remove supply. MHR earns nothing by sitting still — only by being spent on services or lent via trust-based credit.
- **Partition-safe**: The economic layer works correctly during network partitions and converges when they heal
- **Minimal overhead**: [Stochastic rewards](payment-channels) reduce economic bandwidth overhead by ~10x compared to per-packet payment
- **Communities first**: Trusted peer communication is free. The economic layer only activates at trust boundaries.

<!-- faq-start -->

## Frequently Asked Questions

<details className="faq-item">
<summary>Do I need MHR tokens to use the Mehr network?</summary>

No — communication between trusted peers is always free with zero economic overhead. MHR is only needed when your traffic crosses trust boundaries (traverses nodes outside your trust network). A local community mesh where everyone trusts each other operates with no tokens in circulation at all. Gateway operators also offer fiat payment options so consumers never need to interact with MHR directly.

</details>

<details className="faq-item">
<summary>How does the genesis gateway bootstrap the economy without an ICO?</summary>

The genesis gateway operator receives a transparent, disclosed MHR allocation visible in the ledger from epoch 0. The gateway offers real services (relay, storage, compute) at market-competitive fiat prices. Consumer fiat payments create funded payment channels, generating the first real service demand. This demand-backed activity triggers minting for all service providers, putting MHR into circulation backed by actual economic work — not speculation.

</details>

<details className="faq-item">
<summary>What stops gateway operators from overcharging consumers?</summary>

Market competition. The genesis gateway publishes maximum prices (ceilings) that subsequent operators can undercut. Anyone can become a gateway operator using standard trust and credit mechanics — there is no special privilege. As more gateways join, prices fall toward marginal cost. Consumers face low switching costs since identity is self-certifying and not locked to any gateway.

</details>

<details className="faq-item">
<summary>Why use a single global currency instead of per-community tokens?</summary>

Per-community tokens would require currency exchanges between communities — order books, matching engines, and liquidity providers. A mesh of 50–500 nodes lacks the trading volume to sustain functioning exchange markets. MHR derives value from labor (relaying, storage, compute), not community membership. When two isolated communities reconnect, their CRDT ledgers merge automatically with no exchange needed, because both earned MHR through real work.

</details>

<details className="faq-item">
<summary>Can someone buy MHR with fiat and use it on the network?</summary>

Yes, and this is expected. Purchased MHR is legitimate because the seller earned it through real service provision — the network benefited from that work. The buyer funds infrastructure indirectly (like buying bus tokens). MHR derives value from the services it buys, not artificial scarcity. Hoarding is self-correcting: held tokens are removed from circulation, making remaining MHR more valuable and incentivizing service provision. Tail emission of 0.1% annually mildly dilutes idle holdings.

</details>

<!-- faq-end -->

---

### CRDT Ledger
<!-- Source: docs/economics/crdt-ledger.md -->

# CRDT Ledger

The global balance sheet in Mehr is a CRDT-based distributed ledger. Not a blockchain. No consensus protocol. No mining. CRDTs (Conflict-free Replicated Data Types) provide automatic, deterministic convergence without coordination — exactly what a partition-tolerant network requires.

## Why Not a Blockchain?

Blockchains require global consensus: all nodes must agree on the order of transactions. This is fundamentally incompatible with Mehr's partition tolerance requirement. When a village mesh is disconnected from the wider network for days or weeks, it must still process payments internally. CRDTs make this possible.

## Account State

```mermaid
graph TD
    subgraph NodeA["Node A"]
        A_epoch["epoch_bal: 200"]
        A_earned["Δ earned: 300"]
        A_spent["Δ spent: 0"]
        A_balance["balance: 500"]
    end

    subgraph NodeB["Node B"]
        B_epoch["epoch_bal: 200"]
        B_earned["Δ earned: 100"]
        B_spent["Δ spent: 0"]
        B_balance["balance: 300"]
    end

    NodeA --> SR["SettlementRecord<br/>(both signatures)"]
    NodeB --> SR
    SR --> Gossip["Gossiped to network"]
    Gossip --> Merge["Each receiving node<br/>validates & merges<br/><br/>epoch_balance: frozen<br/>delta GCounters: merge<br/>via pointwise max<br/>(no conflicts ever)"]
```

```
AccountState {
    node_id: NodeID,
    epoch_number: u64,            // which epoch this state is relative to
    epoch_balance: u64,           // frozen balance at last epoch compaction
    delta_earned: GCounter,       // post-epoch earnings (per-node entries, merge = pointwise max)
    delta_spent: GCounter,        // post-epoch spending (same structure)
    // Balance = epoch_balance + value(delta_earned) - value(delta_spent)
    settlements: GSet<SettlementHash>,  // dedup set (post-epoch only)
}
```

> **Specification**
Balance is always derived: `epoch_balance + value(delta_earned) - value(delta_spent)`. GCounters use per-node entries with pointwise-max merge — no conflicts possible, regardless of message ordering.

### How GCounters Work

A GCounter (grow-only counter) is a CRDT that can only increase. Each node maintains its own entry, and merging takes the pointwise maximum:

- Node A says "Node X has earned 100" and Node B says "Node X has earned 150"
- Merge result: "Node X has earned 150" (the higher value wins)
- This works regardless of the order updates arrive

### Why Separate epoch_balance from Deltas?

The `epoch_balance` is a frozen scalar from the authoritative epoch snapshot. The `delta_earned` and `delta_spent` GCounters track only post-epoch activity using per-node entries. This separation is critical for partition safety — see [GCounter Rebase](epoch-compaction#gcounter-rebase) for the full analysis.

Balance is always derived: `balance = epoch_balance + value(delta_earned) - value(delta_spent)`. It is never stored directly.

## Settlement Flow

```
SettlementRecord {
    channel_id: [u8; 16],
    party_a: [u8; 16],
    party_b: [u8; 16],
    amount_a_to_b: i64,           // net transfer (negative = B pays A)
    final_sequence: u64,          // channel state sequence at settlement
    sig_a: Ed25519Signature,
    sig_b: Ed25519Signature,
}
// settlement_hash = Blake3(channel_id || party_a || party_b || amount || sequence)
// Signatures are over the settlement_hash (sign-then-hash, not hash-then-sign)

Settlement flow:
1. Alice and Bob settle their payment channel (SettlementRecord signed by both)
2. SettlementRecord is gossiped to the network
3. Each receiving node validates:
   - Both signatures verify against the settlement_hash
   - settlement_hash is not already in the GSet (dedup)
   - Neither party's derived balance goes negative after applying
   - If any check fails: silently drop (do not gossip)
4. If valid and new:
   - Increment party_a's delta_spent / party_b's delta_earned (or vice versa)
   - Add settlement_hash to GSet
   - Gossip forward to neighbors
5. Convergence: O(log N) gossip rounds
```

Settlement validation is performed by **every node** that receives the record. This is cheap (two Ed25519 signature verifications + one Blake3 hash + one GSet lookup) and ensures no node relies on a single validator. Invalid settlements are dropped silently — no penalty, no gossip.

### Gossip Bandwidth

With [stochastic relay rewards](payment-channels), settlements happen far less frequently than under per-packet payment — channel updates only trigger on lottery wins. This dramatically reduces the volume of settlement records the CRDT ledger must gossip.

- Baseline gossip: proportional to settlement frequency (~100-200 bytes per settlement)
- On constrained links (< 10 kbps): batching interval increases, reducing overhead further
- Fits within **Tier 2 (economic)** of the [gossip bandwidth budget](../protocol/network-protocol#bandwidth-budget)

## Double-Spend Prevention

Double-spend prevention is **probabilistic, not perfect**. Perfect prevention requires global consensus, which contradicts partition tolerance. Mehr mitigates double-spending through multiple layers:

> **Key Insight**
Perfect double-spend prevention requires global consensus — incompatible with partition tolerance. Mehr's layered probabilistic defense makes cheating economically irrational: the cost of losing your identity and reputation exceeds any single double-spend gain.

1. **Channel deposits**: Both parties must have visible balance to open a channel
2. **Credit limits**: Based on locally-known balance
3. **Reputation staking**: Long-lived nodes get higher credit limits
4. **Fraud detection**: Overdrafts are flagged network-wide; the offending node is blacklisted
5. **Economic disincentive**: For micropayments, blacklisting makes cheating unprofitable — the cost of losing your identity and accumulated reputation exceeds any single double-spend gain

## Partition Minting and Supply Convergence

When the network is partitioned, each partition independently runs the emission schedule and mints MHR proportional to local service work. Emission is **scaled by the partition's active set size** (`scaled_emission = emission × min(active_set_size, 100) / 100`), and a **2% service burn** on every funded-channel payment creates a deflationary counterforce. On merge, the winning epoch's `epoch_balance` snapshot is adopted and the losing partition's settlements are recovered via settlement proofs (see [Partition-Safe Merge Rules](epoch-compaction#partition-safe-merge-rules)). Individual balance correctness is preserved — no one loses earned MHR.

```
Example (with active-set scaling):
  Epoch 5 emission schedule: 1000 MHR total, reference_size = 100
  Partition A (60 nodes): scaled_emission = 600 MHR, mints up to 600 MHR
  Partition B (40 nodes): scaled_emission = 400 MHR, mints up to 400 MHR
  On merge: total minted in epoch 5 = up to 1000 MHR (no overminting!)

  Compare without scaling:
  Partition A: mints up to 1000 MHR
  Partition B: mints up to 1000 MHR
  On merge: total = up to 2000 MHR (2x overminting)
```

Active-set scaling eliminates overminting when the partition sizes sum to the reference size or less. When the total active set exceeds the reference size, some overminting can still occur (each large partition mints at full emission), but this is bounded and further reduced by the 2% service burn within each partition.

The remaining overminting bounds:

1. **Proportional to scale factors**: Two partitions with N₁ + N₂ ≤ reference_size produce no overminting at all. Larger networks may produce up to Kx (K = partition count), but this is offset by burns.
2. **Reduced by service burn**: 2% of economic activity is permanently destroyed each epoch, creating a deflationary counterforce that partially offsets any overminting.
3. **Self-correcting over time**: The emission schedule decays geometrically. Partition supply shocks become negligible as emission decreases.
4. **Offset by lost keys**: The estimated 1-2% annual key loss rate further reduces effective supply.

The protocol does not attempt to "claw back" overminted supply. The cost of the mechanism (requiring consensus) exceeds the cost of the problem (minor temporary supply inflation during rare partitions).

## Service Compensation Tracking

Minting rewards are computed during epoch finalization. All service types — relay, storage, and compute — contribute to a unified minting pool. Emission is scaled by the active set size (`min(active_set_size, 100) / 100`). A 2% service burn is applied to every funded-channel payment before crediting the provider. The revenue cap uses **net income** (income minus spending per provider) to prevent [cycling attacks](token-security#attack-channel-cycling), while distribution uses gross income to reward all service provision fairly. See [All-Service Minting](mhr-token#all-service-minting).

For relay specifically, VRF lottery win proofs are accumulated as service proofs:

```
ServiceDebitSummary {
    provider_id: NodeID,
    relay_income: u64,                  // total relay payments received this epoch (μMHR, post-burn)
    storage_income: u64,                // total storage payments received this epoch (μMHR, post-burn)
    compute_income: u64,                // total compute payments received this epoch (μMHR, post-burn)
    total_spending: u64,                // total payments sent across all channels (μMHR)
    burn_total: u64,                    // total μMHR burned this epoch (2% of all funded-channel payments)
    relay_sample_proofs: Vec<VRFProof>, // subset of relay VRF proofs (up to 10) for spot-checking
    income_hash: Blake3Hash,            // Blake3 of all income/spending proofs (verifiable if challenged)
}

Derived fields:
  gross_income = relay_income + storage_income + compute_income
  net_income   = max(0, gross_income - total_spending)
  // Note: income fields are post-burn (provider receives 98% of channel payment)
  // burn_total tracks the 2% that was permanently destroyed
```

The epoch proposer aggregates debit summaries from gossip and includes totals in the epoch snapshot. The revenue cap is `epoch_minting = min(scaled_emission, 0.5 × Σ net_income)`, where `scaled_emission = emission × min(active_set_size, 100) / 100`. This prevents cycling (round-trip payments produce net income = 0) and limits small-partition minting. The `epoch_burn_total` in the Epoch struct tracks total burns for the epoch. Distribution uses gross income: each provider's mint share is `epoch_minting × (provider_gross_income / Σ all_gross_income)`. Full proof sets are not gossiped (too large) — only summaries with spot-check samples. Any node can challenge a provider's income/spending/burn claims during the 4-epoch grace period by requesting the full proof set. Fraudulent claims result in the provider's minting share being redistributed and the provider's reputation being penalized.

<!-- faq-start -->

## Frequently Asked Questions

<details className="faq-item">
<summary>Why use a CRDT ledger instead of a blockchain?</summary>

Blockchains require global consensus — all nodes must agree on transaction order. This is fundamentally incompatible with partition tolerance. When a village mesh is disconnected for days or weeks, it must still process payments internally. CRDTs (Conflict-free Replicated Data Types) provide automatic, deterministic convergence without coordination. Two partitions operating independently will produce identical merged state regardless of the order updates arrive.

</details>

<details className="faq-item">
<summary>How does the CRDT ledger prevent double-spending?</summary>

Double-spend prevention is probabilistic, not perfect — perfect prevention requires global consensus, which contradicts partition tolerance. Mehr uses layered defenses: channel deposits require visible balance, credit limits are based on locally-known balance, reputation staking gives long-lived nodes higher credit limits, and overdrafts are flagged network-wide with the offending node blacklisted. For micropayments, the cost of losing your identity and accumulated reputation exceeds any single double-spend gain.

</details>

<details className="faq-item">
<summary>What happens to account balances when two network partitions reconnect?</summary>

The CRDT merge handles this automatically. If both partitions share the same epoch number, GCounters merge via pointwise maximum and settlement GSets take the union. If epoch numbers differ, the higher epoch wins and the lower partition's settlements are recovered via settlement proofs during a verification window. Individual balance correctness is preserved — no one loses earned MHR. Active-set-scaled emission and the 2% service burn limit any supply overshoot.

</details>

<details className="faq-item">
<summary>How does settlement validation work without a central authority?</summary>

Every node that receives a SettlementRecord validates it independently: both Ed25519 signatures are verified against the settlement hash, the settlement hash is checked for uniqueness in the GSet, and derived balances are confirmed non-negative after applying the settlement. Invalid settlements are silently dropped. This distributed validation is cheap (two signature verifications + one hash + one lookup) and ensures no single validator has authority.

</details>

<!-- faq-end -->

---

### Token Security
<!-- Source: docs/economics/token-security.md -->

# Token Security

## Partition Tolerance

The economic layer is designed to operate correctly during network partitions and converge automatically when they heal. This section describes how all-service minting interacts with partitions.

### Per-Partition Minting

Each partition operates as a self-contained economy with its own minting. Emission is **scaled by the partition's active set size** and reduced by the **2% service burn**:

```
Partition minting (with active-set scaling and burn):

  Partition A (60 nodes):
    scaled_emission_A = emission(epoch) × min(60, 100) / 100 = 0.6E
    local_debits_A = relay + storage + compute debits within partition A
    local_minting_A = min(scaled_emission_A, 0.5 × net_income_A)
    burns_A = 0.02 × total_funded_payments_A

  Partition B (40 nodes):
    scaled_emission_B = emission(epoch) × min(40, 100) / 100 = 0.4E
    local_debits_B = relay + storage + compute debits within partition B
    local_minting_B = min(scaled_emission_B, 0.5 × net_income_B)
    burns_B = 0.02 × total_funded_payments_B

  Each partition independently applies scaled emission, revenue cap, and burns.
  No cross-partition knowledge needed — each side sees only local activity.
```

On merge, total minted supply may exceed what a single-network emission would have produced. However, with active-set scaling, the overminting is **bounded by the sum of scale factors** (e.g., 60-node + 40-node = 0.6E + 0.4E = 1.0E, no overminting at all when the two partitions together equal the reference size). The 2% burn during partition operation further reduces the net supply impact. This is the same [partition minting tradeoff](crdt-ledger#partition-minting-and-supply-convergence) — the alternative (coordinated minting) requires global consensus, which contradicts partition tolerance.

### DHT Assignment During Partitions

Storage and compute use DHT ring assignment. During a partition:

```
DHT assignment in partitioned network:

  Before partition:
    Full ring: nodes A, B, C, D, E, F, G, H
    storage_key = hash(content || epoch_hash) → assigned to node D

  During partition (A,B,C,D | E,F,G,H):
    Left partition ring: A, B, C, D
    Right partition ring: E, F, G, H

    New storage requests in left partition use left-ring DHT
    New storage requests in right partition use right-ring DHT
    Existing agreements continue on whichever side their node is in

  After merge:
    Full ring restored
    New requests use full ring
    Existing agreements are unaffected (provider stays the same)
```

Existing storage agreements survive partitions because the payment channel persists between client and provider. If the provider is on the other side of the partition, the client cannot verify or pay — the agreement is effectively paused. On merge, the channel resumes (CRDT convergence restores both parties' balances).

### Revenue Cap During Partitions

Each partition's revenue cap uses only local net income, scaled by its active set size:

- No cross-partition knowledge needed
- Each partition independently caps minting at `min(scaled_emission, 0.5 × net_income)`
- Active-set scaling limits small partitions: 3-node partition gets 3% of full emission
- 2% service burn creates deflationary counterforce within each partition
- The net-income cap prevents [cycling attacks](#attack-channel-cycling) even within isolated partitions
- On merge, the CRDT ledger handles balance convergence
- During bootstrap (epoch < 100,000): [genesis-anchored minting](mhr-token#genesis-anchored-minting) prevents isolated partitions from minting at all
- Post-bootstrap: [trust-gated active set](#trust-gated-active-set) requires mutual trust links for minting eligibility; [merge-time trust audit](#merge-time-trust-audit) rejects untrusted minting on reconnection

For detailed analysis of attacks that exploit partitions — including fully-controlled partitions, cycling, and compounding — see the [Security Analysis](#security-analysis).

## Security Analysis

This section catalogs all known economic attack vectors and their defenses. The economic layer relies on five mechanisms — **non-deterministic assignment**, **net-income revenue cap**, **service burn + active-set scaling**, **[trust-gated active set](#trust-gated-active-set)**, and **[merge-time trust audit](#merge-time-trust-audit)** — to defend against minting abuse. During bootstrap, [genesis-anchored minting](mhr-token#genesis-anchored-minting) provides the initial layer by requiring provable connectivity to genesis nodes. Post-bootstrap, the trust-gated active set requires mutual trust links for minting eligibility, and the merge-time trust audit validates partition minting on reconnection. No staking, slashing, or hardware attestation is required.

### Attack: Self-Dealing (Connected Network)

**Description**: Attacker controls X fraction of network economic capacity. Generates Y MHR in fake service demand through funded channels to earn minting rewards.

**Defense**: Non-deterministic assignment routes (1-X)×Y to honest nodes (real, irrecoverable cost). The net-income revenue cap ensures the attacker's own internal transfers produce zero minting eligibility. Result: self-dealing in a connected network is **never profitable** for any X < 1.

**Residual risk**: None in connected networks. See [Proof: Self-Dealing Is Unprofitable](mhr-token#proof-self-dealing-is-unprofitable) for the full analysis.

### Attack: Channel Cycling

**Description**: Two colluding nodes cycle M MHR back and forth on the same channel K times per epoch. With gross debits, each round-trip adds 2M to total debits — after K cycles, total debits = 2KM. The attacker reaches the emission ceiling immediately, regardless of actual economic activity. This also works across channels (triangle cycling: A→B→C→A) and through settlement-mediated cycling (settle, refund, repeat).

**Defense**: The revenue cap uses **net income per provider** (income minus spending), not gross channel debits.

```
Cycling prevention (net-income cap):

  Same-channel cycling: A→B then B→A, repeated K times
    A: income = KM, spending = KM → net = 0
    B: income = KM, spending = KM → net = 0
    minting_eligible = 0 → epoch_minting = 0 ✓

  Cross-channel cycling (triangle): A→B→C→A, repeated K times
    A: income = KM (from C), spending = KM (to B) → net = 0
    B: income = KM (from A), spending = KM (to C) → net = 0
    C: income = KM (from B), spending = KM (to A) → net = 0
    minting_eligible = 0 → epoch_minting = 0 ✓

  Settlement-mediated cycling: settle channel, refund, repeat
    Same result — net income is tracked per-provider across ALL
    channels and settlements within the epoch. A provider who
    receives M via settlement and spends M via new channel debits
    has net income = 0 regardless of settlement timing.

  Key property: any CLOSED CYCLE produces zero net income for
  every participant, because every provider's income equals their
  spending. Only one-directional flows (real demand) produce
  positive net income.
```

**Residual risk**: None. Cycling is completely neutralized by the net-income cap.

### Attack: Sybil DHT Positioning

**Description**: Attacker creates many node identities to occupy more DHT ring space. With more ring positions, a larger fraction of storage/compute assignments are directed to attacker nodes, increasing their minting share.

**Defense**: This is equivalent to increasing X (attacker's network fraction) in the self-dealing proof. More identities capture more assignments, but:

```
Sybil analysis:

  Attacker creates N identities, captures N/total_nodes of DHT ring.
  Equivalent to having X = N/total_nodes network fraction.

  In connected network: self-dealing proof applies.
    Attacker's net income = 0 (internal transfers don't count).
    Never profitable for any X < 1.

  Additional defenses:
    - DHT assignment uses hash(content_id || epoch_hash)
      → epoch_hash changes every epoch, so ring positions are ephemeral
      → attacker can't grind permanent strategic positions
    - Each identity needs funded channels (real MHR) to earn minting
    - Creating identities is cheap but funding channels requires capital
```

**Residual risk**: Same as self-dealing — none in connected networks.

### Attack: Content/Job ID Grinding

**Description**: Client generates content IDs or job specifications designed so that `hash(content_id || epoch_hash)` maps to an attacker-controlled node on the DHT ring.

**Defense**: The `epoch_hash` is determined at epoch proposal time and is unpredictable at content creation time. To pre-grind assignments, the attacker would need to predict future epoch hashes — computationally infeasible (hash space is 2^256). Even grinding after the epoch hash is known is impractical: the client must use the content_id it actually wants to store, not an arbitrary one.

**Residual risk**: None (computationally infeasible).

### Attack: Relay Without Forwarding

**Description**: Relay node claims VRF lottery wins without actually forwarding packets, collecting payment for non-service.

**Defense**: The VRF lottery requires the actual packet hash as input — the relay must have received the real packet to compute a valid VRF proof. If the relay doesn't forward the packet:

- The sender detects non-delivery (no acknowledgment from destination)
- The sender routes around the dishonest relay in future
- Persistent non-forwarding is detectable via delivery rate monitoring

**Residual risk**: Individual packet drops are hard to attribute (could be normal link loss). But the economic impact is bounded to individual lottery wins, and persistent dishonesty leads to route abandonment.

### Attack: Storage/Compute Fabrication

**Description**: Provider claims to store data or execute computations without actually doing so, collecting channel payments for non-service.

**Defense**: Bilateral verification by the client:

- **Storage**: Client issues periodic challenge-response queries on stored data (e.g., "return bytes 1024-2048 of block X"). Failure to respond correctly means the data is not stored.
- **Compute**: Client verifies computation results against expected output or spot-checks. Incorrect results are immediately detectable.

No protocol-level proof mechanism is needed. The economic incentive (continued payment) ensures honest service. Dishonest providers lose the client's business immediately.

**Residual risk**: Brief period of undetected non-service before the client verifies. Bounded by a single epoch's payment for storage, or a single job's payment for compute.

### Attack: Isolated Partition

**Description**: Attacker creates a network partition they fully or majority control. Within this partition, the attacker controls enough of the economic capacity to profit from self-dealing. At 100% control, non-deterministic assignment is nullified — all service requests go to attacker nodes. Creating an isolated partition is trivial — a few VMs on a laptop suffice.

This is the most significant economic attack vector. Seven defense layers bound the damage to a finite, predictable amount:

**Defense layers**:

1. **Genesis-anchored minting (bootstrap defense)**: During bootstrap (epoch < 100,000), minting requires a valid [GenesisAttestation](mhr-token#genesis-anchored-minting) — a signed proof of recent connectivity to a genesis node. An isolated partition with no path to a genesis node gets **zero minting**. This completely eliminates the attack during the most vulnerable period (high emission, low total supply).

2. **[Trust-gated active set](#trust-gated-active-set) (post-bootstrap identity defense)**: After bootstrap, minting requires ≥1 mutual trust link with another active-set member. This prevents nodes with zero social ties from entering the minting-eligible set in connected networks. During isolation, attacker nodes trivially satisfy this (they trust each other) — the defense activates at merge time.

3. **[Merge-time trust audit](#merge-time-trust-audit) (reconnection defense)**: When a partition reconnects, minting from untrusted nodes is rejected. Cross-partition trust scoring ensures that only minting from nodes with real trust relationships is accepted into the main network supply. Fresh-identity attacks → 0% dilution on merge.

4. **Active-set-scaled emission (size defense)**: Emission is scaled by the partition's active set size: `scaled_emission = emission × min(active_nodes, 100) / 100`. A 3-node partition gets 3% of full emission. This eliminates the linear scaling advantage of small partitions.

5. **Service burn (friction defense)**: 2% of every funded-channel payment is permanently destroyed. This imposes ongoing friction on the attacker and, after reconnection, the burn on the entire network's economic activity gradually absorbs excess supply.

6. **Cycling prevention**: The net-income cap prevents the attacker from inflating debits by cycling MHR between their own nodes. Only net one-directional flows count toward minting eligibility.

7. **Self-correcting on merge**: Excess supply dilutes ALL holders equally, including the attacker's own holdings. The emission schedule decays geometrically, so any supply shock becomes negligible over time.

#### Supply Dynamics Proof

The attacker's supply growth per epoch depends on their spending strategy. Let `S_k` = supply at epoch k, `E_s` = scaled emission, `b` = burn rate (0.02), and `A_k` = economic activity (total one-directional payments):

```
Supply recurrence:
  S_{k+1} = S_k - b × A_k + min(E_s, 0.5 × 0.98 × A_k)

  Minting requires positive net income. A rational attacker structures
  one-directional payments (no cycling within an epoch) to maximize
  net income. Each epoch, the sender alternates (A→B in epoch k,
  B→A in epoch k+1) to avoid same-epoch cycling.

  The attacker chooses A_k to maximize S_{k+1}:
    If 0.49 × A_k < E_s (revenue cap binds):
      gain = 0.49 × A_k - 0.02 × A_k = 0.47 × A_k
      → maximized by A_k = S_k (spend everything)
    If 0.49 × A_k ≥ E_s (emission cap binds):
      gain = E_s - 0.02 × A_k
      → maximized by A_k = E_s / 0.49 ≈ 2.04 × E_s (spend minimum)

  Phase 1 (S_k < 2.04 × E_s): attacker spends everything
    S_{k+1} = S_k + 0.47 × S_k = 1.47 × S_k  (exponential growth)

  Phase 2 (S_k ≥ 2.04 × E_s): attacker spends only 2.04 × E_s
    S_{k+1} = S_k + E_s - 0.02 × 2.04 × E_s = S_k + 0.959 × E_s
    (linear growth at ~0.96 × E_s per epoch, no equilibrium)
```

The attacker reaches Phase 2 quickly (about `log(2.04 × E_s / M_0) / log(1.47)` epochs from initial capital `M_0`). After that, supply grows linearly:

```
Worst-case supply bound (optimal attacker, post-Phase 1):
  S_K ≈ 2.04 × E_s + 0.959 × E_s × K  (after K epochs in Phase 2)

  Simpler upper bound (per epoch):
    Supply growth ≤ E_s per epoch  (emission is the hard ceiling)

  Example (3-node partition, after first halving, 1000 epochs ≈ 1 week):
    E_s = 15,000 MHR/epoch
    Max excess after 1000 epochs: ~15,000 × 1000 = 15M MHR
    Total network supply at epoch 100,000: ~10^11 MHR
    Impact: 0.015% of supply  → negligible

  Example (3-node partition, after 5 halvings, 1000 epochs):
    E_s = 937.5 MHR/epoch
    Max excess after 1000 epochs: ~937,500 MHR  → negligible

  Total lifetime excess (infinite-duration partition, all halvings):
    Σ E_s per halving period = (N/100) × Σ_{h=1}^{∞} E_h × 100,000
    = (N/100) × 10^11 MHR (convergent geometric sum)
    For N=3: 3 × 10^9 MHR
    Total actual supply: ~2 × 10^11 MHR
    Dilution: 3 × 10^9 / 2 × 10^11 = 1.5%
    → Even an infinitely long 3-node partition produces ~1.5% dilution
    → For realistic durations (weeks-months), dilution is < 0.1%
```

> **Key Insight**
Worst-case supply growth is bounded at ≤ E_s (scaled emission) per epoch. A 3-node partition running indefinitely produces ~1.5% lifetime dilution. With merge-time trust audit, fresh-identity attacks produce **0% dilution** on reconnection.

**Note on the `E_s / burn_rate` formula**: Under 100% money velocity (attacker circulates ALL supply every epoch), a true equilibrium exists at `S* = E_s / burn_rate`. This is because `S_{k+1} = 0.98 × S_k + E_s`, which converges to `E_s / 0.02`. However, a rational attacker avoids this by spending only the minimum needed to saturate the minting cap, keeping a reserve that is never burned. The per-epoch growth bound (`≤ E_s`) and the convergent halving sum are the correct worst-case bounds.

#### Attacker Economics: Cost vs. Damage

The isolated partition attack requires running node processes and maintaining them. The attacker's return on investment determines whether the attack is practical at scale.

**Critical note on hardware costs**: A "node" in Mehr is an Ed25519 keypair plus a lightweight process. Active set membership requires only that a node appears in a `SettlementRecord` within the last 2 epochs — there is no hardware attestation, proof-of-work, or unique device requirement. An attacker can run 100 node identities as 100 processes on a single machine (localhost). The processes settle with each other over loopback, generating the required `SettlementRecord` entries for active set membership. This means the real hardware cost for a 100-node partition is **one machine** (~$60/year for a cheap VPS), not 100 separate VMs.

```
Cost-damage analysis (post-bootstrap, first halving period):

  N = virtual attacker nodes (Ed25519 identities, all on one machine)
  E_s = (N/100) × 500,000 MHR/epoch  (capped at 500,000 for N ≥ 100)
  Annual excess = E_s × 52,600 epochs/year
  Total supply at epoch 100,000: ~10^11 MHR
  Hardware cost: ONE machine for any N (processes on localhost)

  N     E_s/epoch   Annual excess    Annual dilution   Lifetime dilution   Real cost/year
  ---   ---------   ------------     ---------------   -----------------   ---------------
    3      15,000       789M MHR     0.8% of supply          1.5%              ~$60
   10      50,000     2,630M MHR     2.6% of supply          5.0%              ~$60
   50     250,000    13,150M MHR    13.2% of supply         25.0%              ~$60
  100     500,000    26,300M MHR    26.3% of supply         50.0%              ~$60
  200     500,000    26,300M MHR    26.3% of supply         50.0%              ~$60

  Notes:
    - "Annual dilution" is the first year only; subsequent years are halved
    - "Lifetime dilution" assumes infinite duration (convergent halving sum)
    - Active-set cap at 100 means nodes beyond 100 add no damage
    - All N identities run on one machine: cost is FLAT, not per-node
    - These are upper bounds: actual dilution decreases as emission halves
```

#### Localhost Attack: Why Hardware Cost Is Not the Defense {#localhost-attack}

The previous cost table assumed $5/month per VM per node. This is wrong. Nothing in the protocol prevents running N identities on one machine:

```
Localhost attack setup:
  1. Generate 100 Ed25519 keypairs                    (free, instant)
  2. Start 100 processes on one $5/month VPS           (~$60/year)
  3. Open funded channels between them on loopback     (needs initial MHR)
  4. Settle channels → 100 nodes appear in active set  (100/100 = full scaling)
  5. Self-deal: one-directional payments between nodes  (net income > 0)
  6. Mint at full emission rate                         (500,000 MHR/epoch)

Bootstrap from minimal capital:
  Phase 1: exponential growth at 1.47x per epoch (spend everything)
  Phase 2: linear growth at ~0.96 × E_s per epoch (saturate minting cap)

  Starting capital    Epochs to Phase 2    Wall-clock time
  ----------------    -----------------    ---------------
  1 MHR                     ~36 epochs       ~6 hours
  100 MHR                   ~24 epochs       ~4 hours
  10,000 MHR                ~12 epochs       ~2 hours

  Even 1 MHR of initial capital reaches full emission rate in ~6 hours.
  Initial capital is a speed bump, not a wall.

Per-MACHINE return comparison (the correct metric):
  An attacker's "nodes" are lightweight processes on one machine.
  An honest node is a real device. The correct comparison is per-machine.

  Network size    Honest return/machine    Attack return/machine    Ratio
  ------------    ---------------------    ---------------------    -----
       100         5,000 MHR/epoch          500,000 MHR/epoch       100x
     1,000           500 MHR/epoch          500,000 MHR/epoch     1,000x
    10,000            50 MHR/epoch          500,000 MHR/epoch    10,000x

  The per-NODE comparison ("honest participation earns the same") is
  misleading because it equates a virtual process with a physical device.
```

**What the real defenses are** (honest assessment):

| Defense layer | Strength | Why |
|---|---|---|
| **GenesisAttestation** (bootstrap, epoch < 100,000) | **Complete** | No connectivity to genesis node → zero minting. ~1.9 years of total protection. |
| **Halving schedule** | **Strong** | Annual dilution halves every ~1.9 years: 26.3% → 13.2% → 6.6% → ... |
| **Active-set cap at 100** | **Strong** | No benefit from running > 100 identities |
| **Lifetime dilution cap** | **Strong** | 50% maximum (convergent halving sum) regardless of attacker persistence |
| Hardware cost | **Weak** | ~$60/year for one machine, not $6,000/year. Not a deterrent. |
| Per-node return parity | **Misleading** | Per-machine, attacker wins by 100x–10,000x |
| Token value destruction | **Partial** | Attacker's minted tokens lose value from dilution, but this is a circular argument and doesn't prevent the attack |

**Key observations**:

1. **The attack is cheap post-bootstrap.** A 100-node localhost partition costs ~$60/year for a $5/month VPS and produces 26.3% annual dilution (first year). Hardware cost is not the defense — mathematical bounds are.

2. **Attack yields dilution, not theft.** Minted MHR dilutes ALL holders including the attacker. If the attacker holds fraction F of pre-attack supply, they lose F × dilution from their existing holdings. The net gain is `minted_amount - F × dilution × total_supply`, which decreases as the attacker's share of the network grows. An attacker who already holds significant MHR damages their own position by inflating supply.

3. **Repeated attacks offer no compounding advantage.** An attacker who merges and re-partitions continues at the same emission rate (epoch-counted, not wall-clock). The total damage over T epochs is ≤ E_s × T regardless of how many merge/split cycles occur. There is no "compound interest" — the attack is strictly linear per epoch, and the halving schedule steadily reduces E_s.

4. **CRDT merge is permissionless — but minting is audited.** CRDT merge rules adopt data automatically (settlements, counters, bloom filters converge). However, the [merge-time trust audit](#merge-time-trust-audit) validates the *minting component* separately — rejecting minting from partition nodes that lack cross-partition trust. Fresh-identity attacks produce 0% dilution; pre-planned attacks are discounted proportional to untrusted nodes.

5. **The protocol can distinguish trusted from untrusted nodes — not localhost from real devices.** 100 localhost processes are indistinguishable from 100 real devices during isolation. But on reconnection, the merge-time trust audit exposes them: zero cross-partition trust → 100% minting rejection. The defense works at merge time, not during isolation.

**Why these layers matter**: Each defense targets a different phase of the attack:
- Genesis attestation prevents the attack during bootstrap (when damage is maximal) — this is the only **complete** defense
- Active-set scaling limits emission rate regardless of economic activity — this is the primary quantitative defense
- Service burn imposes ~4% friction during isolation and, more importantly, absorbs excess supply after merge via ongoing 2% deflation on the entire network's economic activity

#### Solution: Trust-Gated Active Set + Merge-Time Trust Audit {#partition-defense}

The localhost attack exposes a gap: post-bootstrap, there is no mechanism that makes running virtual nodes on one machine more expensive than running one honest node. Two defense layers close this gap without introducing any centralized dependency.

##### Layer 1: Trust-Gated Active Set {#trust-gated-active-set}

Post-bootstrap, minting eligibility requires **mutual trust**: a node must have at least one **mutual trust link** with another active-set member to be minting-eligible. "Mutual" means both nodes have each other in their `trusted_peers` configuration.

```
Trust-gated minting eligibility:

  A node N is minting-eligible in epoch E if ALL of:
    1. N appears in ≥1 SettlementRecord in the last 2 epochs (active set)
    2. ∃ at least one node M such that:
       - M is also in the active set
       - N is in M.trusted_peers
       - M is in N.trusted_peers
       (i.e., N and M have a mutual trust link)

  Emission scaling (updated):
    trust_gated_active = number of active set nodes WITH ≥1 mutual trust link
    scaled_emission = emission(epoch) × min(trust_gated_active, 100) / 100

    Nodes without mutual trust links can still transact (channels,
    settlements) but do NOT contribute to the minting-eligible active set.

  Why mutual trust is expensive:
    Adding a node to your trusted_peers means:
      - You absorb their debts if they default
      - You relay their traffic for free
      - Your reputation is linked to theirs
    This economic cost makes mass trust fabrication expensive.
    An attacker needs real people to willingly vouch for fake nodes —
    each vouch exposes the voucher to economic loss.
```

**Why the trust gate does NOT prevent partition attacks during isolation:**

The trust gate is not the partition defense — the merge-time trust audit (Layer 2) is. Here's why:

```
During isolation:
  Attacker's 100 localhost nodes all trust each other.
  They satisfy the mutual-trust requirement trivially.
  They generate SettlementRecords among themselves.
  → Trust gate does NOT block minting during the partition.

The trust gate's value is structural:
  - It establishes trust relationships as a protocol-level concept
  - These relationships become the basis for merge-time auditing
  - It prevents nodes with zero social ties from entering the active set
    in the connected network (e.g., drive-by Sybil nodes)
```

**Impact on legitimate communities:**

| Scenario | Outcome |
|---|---|
| Village mesh, connected | Mutual trust between neighbors — mints normally |
| Village mesh, isolated | Mutual trust still valid — mints normally for any duration |
| New node joining | Gets mutual trust link with any existing trusted peer; can mint immediately |
| Nomadic node (no local trust) | Cannot mint until establishing mutual trust; can still transact via channels |

Unlike time-limited attestations, the trust gate never expires. Legitimate isolated communities mint indefinitely — the defense activates only at merge time.

##### Layer 2: Merge-Time Trust Audit {#merge-time-trust-audit}

The trust gate establishes trust relationships; the merge-time audit **uses those relationships to validate minting on reconnection**. This is the primary partition defense.

The key insight: **separate CRDT convergence from economic validation**. The CRDT merge is automatic and conflict-free (settlements, counters, bloom filters converge as designed). The *minting component* is audited separately.

```
Merge-time trust audit:

  When partition P reconnects to main network M:

  Step 1: CRDT merge (unchanged, automatic)
    Settlements, GCounters, epoch snapshots merge per existing rules.
    This is non-negotiable — CRDT convergence is preserved.

  Step 2: Identify divergent epoch range
    E_split = last common epoch between P and M
    divergent_epochs = P's epochs after E_split

  Step 3: Cross-partition trust scoring
    For each node N in P's active set during divergent epochs:
      cross_trust(N) = number of nodes in M's active set (at E_split)
                       that have N in their trusted_peers

    partition_trust_score = Σ min(1, cross_trust(N)) / |P.active_set|
      → 1.0 if every partition node is trusted by someone in the main network
      → 0.0 if no partition node has any external trust

  Step 4: Minting discount
    For each divergent epoch E in P's chain:
      accepted_minting(E) = P.epoch_minting(E) × partition_trust_score
      rejected_minting(E) = P.epoch_minting(E) × (1 - partition_trust_score)

  Step 5: Balance rebase
    For each node in P's active set:
      epoch_balance is adjusted to reflect only accepted minting
      This is applied as a rebase during the merge verification window
      (extends the existing 4-epoch grace period to 8 epochs for
      cross-partition merges)

  Step 6: Quarantine window
    Rejected minting enters a Q = 10 epoch quarantine.
    During quarantine, partition nodes can submit trust proofs:
      - Signed trust configs from M-side nodes that trust P-side nodes
      - Pre-partition channel histories showing real economic relationships
    If proofs are validated, quarantined minting is released.
    After Q epochs with no proofs: minting is permanently rejected
    (balance rebase becomes final).

Attack outcomes (with trust gate + merge audit combined):

  Fresh localhost (0 cross-trust):
    During isolation: mints freely (nodes trust each other)
    On merge: partition_trust_score = 0.0 → 100% rejected → 0% dilution ✓

  Pre-planned, 1 real trust link out of 100 nodes:
    During isolation: mints freely
    On merge: 1/100 nodes trusted → partition_trust_score = 0.01
    99% rejected → ~0.26% dilution per cycle ✓

  Pre-planned, deep infiltration (50 of 100 nodes trusted):
    During isolation: mints freely
    On merge: 50/100 trusted → partition_trust_score = 0.50
    50% accepted → ~33.3% dilution (one-shot, flagged after merge)
    Requires 50 real people to vouch for attacker nodes ⚠

  Legitimate village (all nodes trusted by external peers):
    partition_trust_score = 1.0 → 0% rejection → full minting accepted ✓
```

**Why this doesn't break CRDTs**: The CRDT merge (Step 1) is unconditional — data converges. The trust audit (Steps 3-6) operates on the *economic layer on top of the CRDT*. It adjusts `epoch_balance` during the verification window, which is an existing mechanism (settlement proofs already modify balances during the grace period). The audit extends this same mechanism to minting validation.

##### Combined Defense Summary

> **Specification**
Five defense layers — trust-gated active set, merge-time trust audit, active-set-scaled emission, 2% service burn, and halving schedule — combine to bound isolated partition damage. Fresh-identity attacks achieve 0% dilution on merge.

| Attack variant | Trust gate alone | Trust gate + merge audit | Notes |
|---|---|---|---|
| Fresh localhost (100 virtual nodes) | Mints during isolation | **0% dilution on merge** | 100% minting rejected — no cross-partition trust |
| Pre-planned, no trust infiltration | Mints during isolation | **~0% dilution** | All nodes untrusted → 100% rejection |
| Pre-planned, 1 trust link | Mints during isolation | **~0.26% dilution** | 99 untrusted → 99% rejection |
| Pre-planned, deep infiltration (50%) | Mints during isolation | **~33.3% dilution** | One-shot; requires 50 real vouchers |
| Legitimate village | Full minting | Full minting | All nodes trusted → 0% rejection |

**Note on neighborhood-scoped minting**: An alternative structural defense — each trust neighborhood minting its own denomination with market-set exchange rates — would achieve 0% dilution even against deep infiltration. However, it introduces ~10x implementation complexity, destroys the single global currency, creates new attack surfaces (exchange rate manipulation), and hurts legitimate nomadic users. See [Partition Defense Comparison](../development/partition-defense-comparison) for the full analysis.

#### Can This Attack Ruin the Network?

No. With the [trust-gated active set](#trust-gated-active-set) + [merge-time trust audit](#merge-time-trust-audit), the most common attack variants are eliminated entirely:

1. **Fresh-identity attacks produce zero dilution.** 100 virtual nodes on localhost with no external trust connections → merge-time trust audit rejects 100% of their minting on reconnection (zero cross-partition trust). Result: 0% dilution.

2. **Pre-planned attacks with minimal infiltration produce negligible dilution.** Attacker pre-positions nodes, establishes 1 real trust link, then isolates. On reconnection, merge-time trust audit rejects ~99% of minting. With 1 out of 100 nodes trusted → ~0.26% dilution per cycle.

3. **Damage rate decreases over time.** Each halving period (≈ 1.9 years), the attacker's per-epoch minting is halved. Combined with merge audit rejection, effective damage is a small fraction of an already-decaying emission schedule.

4. **Network value grows faster than attacker damage.** A healthy network's total economic value (real services transacted) grows with adoption, while the attacker's dilution rate shrinks with each halving and each audit rejection. The ratio of attack damage to network value decreases over time.

The fundamental tradeoff: Mehr chooses **partition tolerance over inflation resistance**. A globally-consistent ledger (blockchain) can prevent this attack entirely, but at the cost of requiring global consensus — which fails during partitions. Mehr accepts bounded, audited, decreasing inflation from isolated partitions in exchange for partition tolerance. The trust-gated active set + merge-time trust audit make this tradeoff far more favorable than pure mathematical bounds alone.

**Residual risk**: The remaining exposure is **deep trust infiltration** — an attacker who gets ≥50% of their partition nodes trusted by main-network peers before isolating. In this worst case, ~33.3% dilution passes the merge audit (one-shot). This scenario requires sustained social engineering — convincing 50 real people to add attacker nodes to their `trusted_peers`, each person accepting economic liability for the attacker's debts. This is visible, one-shot (flagged after the first merge), and bounded by the halving schedule. See the [Partition Defense Comparison](../development/partition-defense-comparison) for the design rationale and analysis of neighborhood-scoped minting as an alternative.

### Attack: Artificial Partition Creation

**Description**: Attacker deliberately creates multiple isolated partitions they control, maximizing total minting across all partitions.

**Defense**:

```
Multi-partition attack economics (with active-set scaling + burn):

  K partitions, each with N_k nodes fully controlled by attacker
  Total attacker nodes: Σ N_k

  Per-partition max growth: (N_k / 100) × E per epoch
  Per-partition max over T epochs: (N_k / 100) × E × T

  Total attacker max growth: Σ (N_k / 100) × E per epoch
                            = (Σ N_k / 100) × E per epoch

  Key insight: splitting nodes across K partitions gives the SAME total
  growth rate as one partition with Σ N_k nodes. There is no advantage
  to fragmenting into multiple partitions.

  During bootstrap: genesis-anchored minting prevents all isolated
  partitions from minting regardless of K.

  Cost:
    - K sets of hardware (real physical devices)
    - Initial MHR capital in each partition
    - No scaling advantage over a single partition of the same total size
```

**Residual risk**: No advantage over single-partition attack. Same per-epoch growth bound applies. During bootstrap, completely prevented by genesis attestation.

### Attack: Channel Balance Inflation

**Description**: Attacker creates payment channels with inflated balances not backed by actual MHR holdings on the CRDT ledger.

**Defense**: Settlement validation is performed by **every receiving node**, which checks that neither party's derived balance goes negative after applying the settlement. Derived balance = `epoch_balance + delta_earned - delta_spent`, which is deterministic from the CRDT state. A node cannot claim more MHR than the ledger attributes to it.

Channel opening requires both parties to sign the initial state. The balances must be backed by ledger holdings. Creating MHR from nothing requires forging the CRDT state, which requires forging settlement records (dual Ed25519 signatures) or corrupting epoch snapshots (67% acknowledgment threshold).

**Residual risk**: None under normal operation. In a fully attacker-controlled partition, the attacker can corrupt the local CRDT state — but this reduces to the [Isolated Partition](#attack-isolated-partition) attack, defended by [trust-gated active set + merge-time trust audit](#partition-defense).

### Attack: Double-Spend via Old Channel State

**Description**: One party publishes an old channel state (lower sequence number, more favorable balance) to claim funds already spent.

**Defense**: The dispute resolution window (2,880 gossip rounds, ~48 hours) allows the counterparty to submit a higher-sequence state, which always wins. After the window, the latest submitted state is final. Channel abandonment (4 epochs of inactivity) allows unilateral close with the last mutually-signed state. See [Bilateral Payment Channels](payment-channels#channel-lifecycle).

**Residual risk**: If the honest counterparty is offline for >48 hours during a dispute, they cannot submit the newer state. Mitigated by the probabilistic double-spend detection in the CRDT ledger and by economic disincentive (blacklisting makes the one-time gain smaller than the cost of losing network identity).

### Attack: Settlement Forgery

**Description**: Forge settlement records to credit attacker with unearned balance on the CRDT ledger.

**Defense**: Settlement records require Ed25519 signatures from BOTH parties. Forging requires compromising both private keys. Every receiving node validates both signatures independently against the settlement hash. Invalid settlements are silently dropped and not gossiped.

**Residual risk**: None without private key compromise.

### Attack: CRDT Counter Manipulation

**Description**: Inflate GCounter entries in the CRDT ledger to increase balance without corresponding economic activity.

**Defense**: GCounter entries are per-node — each processing node writes only to its own entry. Merge takes pointwise maximum. Increases must correspond to valid settlement records, which require dual Ed25519 signatures. A node that inflates its own entry without corresponding settlements is detectable by any peer that compares the claimed delta against available settlement records during the epoch verification window.

**Residual risk**: In a partition where all verifiers are attacker nodes, inflation goes undetected locally. On merge, this reduces to the [Isolated Partition](#attack-isolated-partition) attack — defended by [trust-gated active set + merge-time trust audit](#partition-defense), and corrected during epoch reconciliation.

### Attack: VRF Lottery Manipulation

**Description**: Relay node tries to influence the VRF output to win the lottery more frequently.

**Defense**: The VRF (ECVRF-ED25519-SHA512-TAI, RFC 9381) produces exactly **one valid output** per (private_key, packet_hash) pair. The relay cannot "grind" through values — there is only one valid output for each packet. The VRF proof is verifiable by any party using the relay's public key. Changing the relay key changes the node identity (and forfeits accumulated reputation).

**Residual risk**: None (cryptographic guarantee).

### Attack: Trust/Credit Exploitation

**Description**: Attacker gains trust from peers, extracts maximum credit, then defaults on the debt.

**Defense**: Credit limits are per-peer, per-epoch, configurable by the vouching node. The voucher absorbs the debt (economic skin in the game — you only trust people you'd lend to). Transitive credit decays (10% per hop, max 2 hops). Trust is revocable at any time. See [Trust & Neighborhoods](trust-neighborhoods).

**Residual risk**: One-time loss up to the extended credit limit. Mitigated by conservative credit settings and the social cost of defaulting (loss of all trust relationships in the network).

### Defense Summary

| Attack Vector | Primary Defense | Residual Risk |
|---|---|---|
| Self-dealing (connected) | Non-deterministic assignment + net-income cap | None |
| Channel cycling | Net-income revenue cap | None |
| Sybil DHT positioning | Reduces to self-dealing proof | None (connected) |
| Content/Job ID grinding | Unpredictable epoch_hash | None |
| Relay non-forwarding | VRF requires real packet + sender detection | Individual packet drops |
| Storage/compute fabrication | Bilateral client verification | Brief undetected period |
| **Isolated partition** | **Trust-gated active set + merge-time trust audit + burn + scaled emission** | **Fresh IDs: 0% dilution on merge. Pre-planned: audit-discounted on merge** |
| Artificial partition creation | Same as isolated partition; no advantage to splitting | Same per-epoch growth bound |
| Channel balance inflation | CRDT ledger validation | None (connected) |
| Double-spend (old state) | 48-hour dispute window | Offline counterparty |
| Settlement forgery | Dual Ed25519 signatures | None |
| CRDT counter manipulation | Per-node entries + settlement proof | Reduces to partition attack |
| VRF lottery manipulation | Cryptographic (one output per input) | None |
| Trust/credit exploitation | Credit limits + voucher absorbs debt | One-time credit loss |

**All attack vectors are bounded.** The isolated partition — the only vector with material residual risk — is defended by five layers:

1. **Trust-gated active set** ([details](#trust-gated-active-set)): Post-bootstrap, minting requires ≥1 mutual trust link. Prevents zero-social-tie nodes from minting in connected networks. During isolation, attacker nodes satisfy this trivially — the defense activates at merge time.
2. **Merge-time trust audit** ([details](#merge-time-trust-audit)): On reconnection, minting from untrusted partition nodes is rejected. Fresh-identity attacks → 0% dilution. Pre-planned with minimal infiltration → ~0.26% dilution.
3. **Active-set-scaled emission**: Limits small-partition minting rate — the primary quantitative defense.
4. **2% service burn**: Provides ~4% friction during isolation and absorbs excess supply after merge.
5. **Halving schedule**: Cumulative excess converges — the exponentially decaying emission bounds lifetime dilution.

**Hardware cost is not a meaningful defense** — 100 virtual nodes can run on a single $5/month machine ([localhost attack](#localhost-attack)), ~$60/year. The defense rests on merge-time trust audit (rejects minting from untrusted nodes on reconnection) and mathematical bounds (halving, active-set cap). See [Attacker Economics](#attacker-economics-cost-vs-damage), [Localhost Attack](#localhost-attack), and [Partition Defense](#partition-defense) for the full analysis.

For the worst-case residual scenario (attacker deeply infiltrates the trust graph with 50% of partition nodes trusted by main-network peers), dilution is ~33.3% one-shot — still bounded by halving and flagged after the first merge. See the [Partition Defense Comparison](../development/partition-defense-comparison) for the design rationale and analysis of neighborhood-scoped minting as an alternative.

## Long-Term Sustainability

Does MHR stay functional for 100 years?

### Economic Equilibrium

```
Supply dynamics over time:

  Year 0-10:   High minting emission, rapid supply growth
               2% service burn provides continuous deflationary pressure
               Lost keys: ~1-2% annually (negligible vs. emission)
               Economy bootstraps; genesis attestation prevents partition exploits

  Year 10-30:  Minting decays significantly (many halvings)
               Burns + lost keys accumulate (~10-40% of early supply permanently gone)
               Effective circulating supply stabilizes faster than without burns
               Genesis attestation sunsets at first halving; burns + scaling take over

  Year 30+:    Tail emission ≈ burns + lost keys
               Tighter equilibrium than lost keys alone
               All income is from bilateral payments + residual minting
```

The tail emission exists specifically for this: it ensures service providers always have a minting incentive, even centuries from now. Lost keys, service burns, and tail emission create a tight equilibrium — new supply enters through service, old supply exits through burns and lost keys. Neither grows without bound. The 2% service burn accelerates the approach to equilibrium compared to relying on lost keys alone.

### Technology Evolution

| Challenge | Mehr's Answer |
|-----------|--------------|
| **New radio technologies** | Transport-agnostic — any medium that moves bytes works |
| **Post-quantum cryptography** | [KeyRotation](../services/mhr-id) claims enable key migration; new algorithms plug into existing identity framework |
| **Hardware evolution** | Capability marketplace adapts — nodes advertise what they can do, not what they are |
| **Protocol upgrades** | [MEP governance](../protocol/versioning#governance) — trust-weighted version signaling via announces, ≥67% acceptance threshold. Communities can fork; gateway bridges maintain connectivity across versions |

### What Doesn't Change

The fundamental economic model — free between trusted peers, paid between strangers — is as old as human commerce. It doesn't depend on any specific technology, cryptographic primitive, or hardware platform. As long as people want to communicate and are willing to help their neighbors, the model works.

<!-- faq-start -->

## Frequently Asked Questions

<details className="faq-item">
<summary>Can an attacker inflate the token supply by running virtual nodes on a single machine?</summary>

During bootstrap (first ~1.9 years), genesis-anchored minting completely prevents this — isolated partitions without genesis connectivity get zero minting. Post-bootstrap, an attacker can mint during isolation, but the merge-time trust audit rejects 100% of minting from nodes with zero cross-partition trust on reconnection. Fresh-identity localhost attacks produce 0% dilution. Even deep infiltration (50% of nodes trusted) is bounded to ~33.3% one-shot dilution, flagged after the first merge.

</details>

<details className="faq-item">
<summary>What is the worst-case supply dilution from a partition attack?</summary>

Lifetime dilution from a 3-node isolated partition running indefinitely is approximately 1.5% of total supply — and this assumes 100% minting acceptance on merge, ignoring the trust audit. With the merge-time trust audit, fresh-identity attacks produce 0% dilution. Active-set-scaled emission limits a 3-node partition to just 3% of full emission per epoch. The halving schedule ensures cumulative excess converges geometrically, and the 2% service burn provides additional friction.

</details>

<details className="faq-item">
<summary>How does the channel cycling attack work and why is it fully neutralized?</summary>

In a cycling attack, two colluding nodes pass MHR back and forth to inflate gross channel debits and reach the minting ceiling. The defense is the net-income revenue cap: minting eligibility uses net income (income minus spending per provider), not gross debits. In any closed cycle — same-channel, cross-channel triangle, or settlement-mediated — every participant's income equals their spending, producing net income of zero. Only one-directional flows (real demand) generate positive net income.

</details>

<details className="faq-item">
<summary>What prevents an attacker from forging settlement records or VRF lottery outcomes?</summary>

Settlement records require valid Ed25519 signatures from both parties — forging requires compromising both private keys. Every receiving node independently validates both signatures before accepting. For VRF lottery manipulation, the ECVRF-ED25519-SHA512-TAI function produces exactly one valid output per (relay key, packet hash) pair. The relay cannot grind through values since there is only one valid output per packet — this is a cryptographic guarantee.

</details>

<details className="faq-item">
<summary>How does the trust-gated active set differ from traditional staking?</summary>

Unlike staking (which requires locking up tokens as collateral), the trust-gated active set requires mutual trust relationships — another active-set member must have you in their trusted_peers, and you must have them in yours. Adding someone to your trusted_peers means absorbing their debts if they default and relaying their traffic for free, making mass trust fabrication economically expensive. This social-economic cost replaces financial collateral without requiring locked capital.

</details>

<!-- faq-end -->

---

### Epoch Compaction
<!-- Source: docs/economics/epoch-compaction.md -->

# Epoch Compaction

The settlement GSet grows without bound. The Epoch Checkpoint Protocol solves this by periodically snapshotting the ledger state.

```
Epoch {
    epoch_number: u64,
    timestamp: u64,

    // Frozen account balances at this epoch (see GCounter Rebase)
    account_snapshot: Map<NodeID, epoch_balance>,

    // Bloom filter of ALL settlement hashes included
    included_settlements: BloomFilter,

    // Active set: defines the 67% threshold denominator
    active_set_hash: Blake3Hash,    // hash of sorted NodeIDs active in last 2 epochs
    active_set_size: u32,           // number of nodes in the active set

    // Genesis-anchored minting (bootstrap only, before epoch 100,000)
    genesis_attestations: GSet<GenesisAttestationHash>,  // valid attestations this epoch

    // Service burn tracking
    epoch_burn_total: u64,          // total μMHR burned this epoch across all providers

    // Acknowledgment tracking
    ack_count: u32,
    ack_threshold: u32,             // 67% of active_set_size
    status: enum { Proposed, Active, Finalized, Archived },
}
```

> **Specification**
The Epoch struct snapshots frozen account balances, a bloom filter of included settlements, the active set hash defining the 67% threshold, and cumulative burn totals. Constrained devices discard individual settlement records after epoch activation.

## Epoch Triggers

An epoch is triggered when **any** of these conditions is met:

| Trigger | Threshold | Purpose |
|---------|-----------|---------|
| **Settlement count** | ≥ 10,000 batches | Standard trigger for large meshes |
| **GSet memory** | ≥ 500 KB | Protects constrained devices (ESP32 has ~520 KB usable RAM) |
| **Small partition** | ≥ max(200, active_set_size × 10) settlements AND ≥ 1,000 gossip rounds since last epoch | Prevents stagnation in small partitions |

The small-partition trigger ensures a 20-node village doesn't wait months for an epoch. At 200 settlements (the minimum), the GSet is ~6.4 KB — well within ESP32 capacity. The 1,000 gossip round floor (roughly 17 hours at 60-second intervals) prevents epochs from firing too rapidly in tiny partitions with bursty activity.

## Epoch Proposer Selection

Eligibility requirements adapt to partition size:

1. The node has processed ≥ min(10,000, current epoch trigger threshold) settlement batches since the last epoch
2. The node has direct links to ≥ min(3, active_set_size / 2) active nodes
3. No other epoch proposal for this `epoch_number` has been seen

In a 20-node partition, a node needs only 3 direct links (not 10) and 200 processed settlements (not 10,000) to propose.

**Conflict resolution**: If multiple proposals for the same `epoch_number` arrive, nodes ACK the one with the **highest settlement count** (most complete state). Ties broken by lowest proposer `destination_hash`.

**Active set divergence** (post-partition): Two partitions may propose epochs with different `active_set_hash` values because they've seen different settlement participants. Resolution:

```
Active set conflict handling:
  1. If your local settlement count is within 5% of the proposal's count:
     ACK the proposal's active_set_hash (defer to proposer — close enough)
  2. If your local settlement count exceeds the proposal's by >5%:
     NAK the proposal. Wait 3 gossip rounds for further convergence,
     then propose your own epoch if no better proposal arrives
  3. After partition merge: the epoch with the highest settlement count
     is accepted by all nodes. The losing partition's active set members
     that were missing from the winning proposal are included in the
     NEXT epoch's active set (no settlements are lost — they are applied
     on top of the winning snapshot during the verification window)
```

Epoch proposals are rate-limited to one per node per epoch period. Proposals that don't meet eligibility are silently ignored.

## Epoch Lifecycle

1. **Propose**: An eligible node proposes a new epoch with a snapshot of current state. The proposal includes an `active_set_hash` — a Blake3 hash of the sorted list of NodeIDs in the active set, as observed by the proposer. This fixes the denominator for the 67% threshold.

**Active set definition**: A node is in the active set if it appears as `party_a` or `party_b` in at least one `SettlementRecord` within the last 2 epochs. Relay-only nodes (that relay packets but never settle channels) are not in the active set — they participate in the economy via mining proofs, not via epoch consensus. This keeps the active set small and the 67% threshold meaningful.
2. **Acknowledge**: Nodes compare against their local state. If they've seen the same or more settlements, they ACK. If they have unseen settlements, they gossip those first. A node ACKs the proposal's `active_set_hash` — even if its own view differs slightly, it agrees to use the proposer's set as the threshold denominator for this epoch.
3. **Activate**: At 67% acknowledgment (of the active set defined in the proposal), the epoch becomes active. Nodes can discard individual settlement records and use only the bloom filter for dedup. If a significant fraction of nodes reject the active set (NAK), the proposer must re-propose with an updated set after further gossip convergence.
4. **Verification window**: During the grace period (4 epochs after activation), any node can submit a **settlement proof** — the full `SettlementRecord` — for any settlement it believes was missed. If the settlement is valid (signatures check) and NOT in the epoch's bloom filter, it is applied on top of the snapshot.
5. **Finalize**: After the grace period, previous epoch data is fully discarded. The bloom filter is the final word.

## GCounter Rebase

GCounter `delta_earned` and `delta_spent` grow monotonically between epochs. Over very long timescales (centuries), high-throughput nodes could approach the u64 maximum (1.84 × 10^19) due to money velocity: the same tokens are earned, spent, earned again, each cycle growing both delta counters.

Epoch compaction solves this. At each epoch, the snapshot freezes the balance and resets the deltas:

```
GCounter rebase at epoch compaction:

  Before epoch:
    Alice: epoch_balance = 200,000    delta_earned = {Y: 3,000,000, Z: 1,800,000}
    delta_spent = {W: 4,600,000, V: 200,000}
    Balance = 200,000 + 4,800,000 - 4,800,000 = 200,000

  After epoch snapshot (rebased):
    Alice: epoch_balance = 200,000    epoch_number incremented
    delta_earned = {}  (zeroed)
    delta_spent = {}   (zeroed)
    Balance = 200,000 (unchanged)

  Post-epoch settlements apply on top:
    Alice earns 50,000 (processed by node Y) → delta_earned = {Y: 50,000}
    Alice spends 30,000 (processed by node Z) → delta_spent = {Z: 30,000}
    Balance = 200,000 + 50,000 - 30,000 = 220,000 ✓
```

Without rebase, a node processing 10^10 μMHR/epoch of throughput would overflow u64 after ~1.84 × 10^9 epochs (~35,000 years). With rebase, delta counters never exceed one epoch's worth of activity — the protocol runs indefinitely.

## Partition-Safe Merge Rules

The separation of `epoch_balance` from delta GCounters is critical for correctness during partition merges. When two copies of the same account are merged:

```
CASE 1: Same epoch_number, same epoch_balance (normal operation)
  Standard CRDT merge:
    epoch_balance: unchanged (identical on both sides)
    delta_earned: GCounter pointwise max
    delta_spent:  GCounter pointwise max
    settlements:  GSet union

  This is the common case — both nodes are in the same partition
  or have received the same epoch snapshot.

CASE 2: Same epoch_number, DIFFERENT epoch_balance (concurrent partition compaction)
  Two partitions independently compacted to the same epoch number
  but processed different pre-rebase settlements, producing different
  epoch_balance values.

  Resolution:
    1. The epoch with the higher settlement count wins (existing rule)
    2. Winning epoch's account_snapshot provides epoch_balance for ALL accounts
    3. Winning partition's delta GCounters are kept as-is
    4. Losing partition's delta GCounters are discarded
    5. Losing partition's post-epoch settlements that are NOT in the winning
       epoch's bloom filter are submitted as settlement proofs during the
       verification window, which re-applies them to the delta GCounters
    6. Losing partition's PRE-epoch settlements that are NOT in the winning
       epoch's bloom filter are ALSO submitted as settlement proofs —
       these add the amounts that were absorbed into the losing partition's
       epoch_balance but lost when the winning partition's higher/lower
       epoch_balance was adopted

CASE 3: DIFFERENT epoch_numbers (one partition is ahead)
  The higher epoch_number wins entirely.
    epoch_number: higher value
    epoch_balance: from the higher-epoch partition
    delta_earned: from the higher-epoch partition
    delta_spent:  from the higher-epoch partition
  The lower-epoch partition's settlements are recovered via
  settlement proofs against the winning epoch's bloom filter.
```

**Why this is safe**: The delta GCounters use per-node entries (each processing node writes only to its own entry). Within a single partition, standard CRDT merge (pointwise max) is always correct. Across partitions with conflicting epochs, the settlement proof mechanism — which checks against the winning epoch's bloom filter, NOT the GSet — recovers any settlements that were lost during epoch_balance adoption. The bloom filter check is critical: a settlement may be in the merged GSet (from the losing partition's contribution) but NOT reflected in the winning epoch's delta GCounters, so the GSet must not be used for dedup during settlement proof processing.

**Settlement proof dedup rule**: During the verification window, settlement proofs are checked against the **winning epoch's bloom filter only**. The live GSet is NOT consulted. After successful re-application, the settlement hash is added to the GSet to prevent future re-processing during normal (non-verification-window) operation.

## Merge-Time Supply Audit {#merge-time-supply-audit}

When two partitions reconnect, the CRDT merge (above) handles data convergence automatically. The **supply audit** is an additional economic layer that validates minting produced during the partition. This prevents an isolated attacker from injecting unbounded supply into the main network.

**Key principle**: CRDT convergence is unconditional — settlements, GCounters, epoch snapshots, and bloom filters all merge per the rules above. The supply audit operates *on top of* the merged data, adjusting only the minting component.

> **Key Insight**
CRDT convergence and economic validation are cleanly separated. The merge is unconditional (data always converges); the supply audit adjusts only minting via cross-partition trust scoring — preserving CRDT guarantees while validating economic integrity.

```
Merge-time supply audit:

  Trigger: a node receives epoch data from a partition it has been
  disconnected from for ≥ 1 epoch

  Step 1: Identify divergent epoch range
    E_split = last common epoch number between the two partitions
    divergent_range = epochs in the reconnecting partition after E_split

  Step 2: Cross-partition trust scoring
    For each node N that was in the reconnecting partition's active set
    during the divergent range:

      cross_trust(N) = |{ M_node ∈ main_active_set(E_split) :
                          N ∈ M_node.trusted_peers }|

    This counts how many main-network active nodes had N in their
    trusted_peers at the time of the split. The trust graph is a
    CRDT (GSet of signed trust configs), so both sides have the
    pre-split state.

    partition_trust_score = Σ min(1, cross_trust(N)) / |partition_active_set|
      → 1.0 if every partition node was trusted by ≥1 main-network node
      → 0.0 if no partition node had any cross-partition trust

  Step 3: Minting discount
    For each divergent epoch E in the reconnecting partition:
      accepted_minting(E) = partition_epoch_minting(E) × partition_trust_score
      rejected_minting(E) = partition_epoch_minting(E) × (1 - partition_trust_score)

    accepted_minting merges into the main supply normally.
    rejected_minting enters quarantine (Step 4).

  Step 4: Quarantine window (Q = 10 epochs)
    Rejected minting is held in a pending state for Q = 10 epochs.

    During quarantine, partition nodes can submit trust proofs:
      - Signed trust configs from main-network nodes dated after E_split
        that trust partition nodes (proves trust was established during,
        not before, the partition — weaker but still counts)
      - Pre-partition settlement records showing real bilateral economic
        activity between partition nodes and main-network nodes
      - Channel close proofs from before the split showing funded channels

    Trust proofs increase partition_trust_score retroactively.
    Recalculated score applies to all divergent epochs.

    After Q epochs with no proofs: rejected minting is permanently
    discarded. Affected nodes' epoch_balances are rebased:
      rebased_balance = epoch_balance - (rejected_minting / partition_size)

    This rebase uses the same mechanism as the existing settlement proof
    verification window — it adjusts epoch_balance during the grace period.

  Step 5: Normal operation resumes
    After quarantine closes, all nodes (main + reconnected partition)
    have a consistent view. CRDT state has converged. Supply reflects
    only accepted minting.
```

**Why this doesn't break CRDTs**: The CRDT merge (Cases 1-3 above) is unconditional — `epoch_balance`, `delta_earned`, `delta_spent` GCounters, and the settlement GSet converge without modification. The supply audit adjusts `epoch_balance` *during* the verification window through the same mechanism that settlement proofs already use (re-applying missed settlements adjusts deltas, which affects derived balances). The quarantine and rebase are *economic policy* applied on top of convergent data, not modifications to convergence itself.

**Interaction with settlement proofs**: During the quarantine window, settlement proofs and trust proofs are processed in parallel. Settlement proofs recover lost transactions (existing mechanism); trust proofs validate minting (new mechanism). Both modify the same `epoch_balance` through their respective adjustments. The extended verification window (8 epochs for cross-partition merges, up from 4) ensures both processes complete before epoch finalization.

**Extended verification window**: Cross-partition merges extend the standard 4-epoch verification window to 8 epochs. The first 4 epochs handle settlement proof recovery (existing). Epochs 5-8 overlap with the quarantine window for trust proof submission. The supply audit quarantine (Q = 10 epochs) extends beyond the verification window — any balance rebase from rejected minting in epochs 9-10 is applied as a separate adjustment to the already-finalized epoch, similar to how late-arriving settlement proofs are handled after the standard window closes.

For the full economic analysis and attack outcomes, see [Partition Defense](token-security#partition-defense) in the token economics specification.

## Late Arrivals After Compaction

When a node reconnects after an epoch has been compacted, it checks its unprocessed settlements against the epoch's bloom filter:
- **Present in filter**: Already counted in epoch_balance, discard
- **Absent from filter**: New settlement, apply to delta GCounters on top of epoch_balance. If within the verification window, submit as a settlement proof.

**Important**: During the verification window after a partition merge, settlement proofs are checked against the **winning epoch's bloom filter only** — NOT the merged GSet. This is because a settlement may exist in the merged GSet (contributed by the losing partition) but not be reflected in the delta GCounters (because the losing partition's deltas were discarded during conflict resolution). The bloom-filter-only check ensures such settlements are correctly re-applied.

## Bloom Filter Sizing

| Data | Size |
|------|------|
| 1M settlement hashes (raw) | ~32 MB |
| Bloom filter (0.01% false positive rate) | ~2.4 MB |
| Target epoch frequency | ~10,000 settlement batches |
| Per-node storage target | Under 5 MB |

The false positive rate is set to **0.01% (1 in 10,000)** rather than 1%, because false positives cause legitimate settlements to be silently treated as duplicates. At 0.01%, the expected loss is negligible (~1 settlement per 10,000), and the verification window provides a recovery mechanism for any that are caught.

**Construction**: The bloom filter uses `k = 13` hash functions derived from Blake3:

```
Bloom filter hash construction:
  For each settlement_hash and index i in [0, k):
    h_i = Blake3(settlement_hash || i as u8) truncated to 32 bits
    bit_position = h_i mod m  (where m = total bits in filter)

  Bits per element: m/n = -ln(p) / (ln2)² ≈ 19.2 bits at p = 0.0001
  k = -log₂(p) ≈ 13.3, rounded to 13

  For 10,000 settlements: m = 192,000 bits = 24 KB
  For 1M settlements: m = 19.2M bits ≈ 2.4 MB
```

The Merkle tree over the account snapshot also uses Blake3 (consistent with all content hashing in Mehr). Leaf nodes are `Blake3(NodeID || epoch_balance)`, and internal nodes are `Blake3(left_child || right_child)`.

**Critical retention rule**: Both parties to a settlement **must retain the full `SettlementRecord`** until the epoch's verification window closes (4 epochs after activation). If both parties discard the record after epoch activation (believing it was included) and a bloom filter false positive caused it to be missed, the settlement would be permanently lost. During the verification window, each party independently checks that its settlements are reflected in the snapshot; if any are missing, it submits a settlement proof. Only after the window closes may the full record be discarded.

## Snapshot Scaling

At 1M+ nodes, the flat `account_snapshot` is ~32 MB — too large for constrained devices. The solution is a **Merkle-tree snapshot** with sparse views.

**Full snapshot** (backbone/gateway nodes only): The account snapshot is stored as a sorted Merkle tree keyed by NodeID. Only nodes that participate in epoch consensus need the full tree. At 1M nodes and 24 bytes per entry (16-byte NodeID + 8-byte epoch_balance), this is ~24 MB — feasible for nodes with SSDs.

**Sparse snapshot** (everyone else): Constrained devices store only:
- Their own balance
- Balances of direct channel partners
- Balances of trust graph neighbors (Ring 0-2)
- The Merkle root of the full snapshot

For a typical node with ~50 relevant accounts: 50 × 24 bytes = 1.2 KB.

**On-demand balance verification**: When a constrained node needs a balance it doesn't have locally (e.g., to extend credit to a new node), it requests a Merkle proof from any capable peer:

```
BalanceProof {
    node_id: NodeID,
    epoch_balance: u64,
    merkle_siblings: Vec<Blake3Hash>,  // path from leaf to root
    epoch_number: u64,
}
// Size: ~640 bytes for 1M nodes (20 tree levels × 32-byte hashes)
```

The constrained node verifies the proof against the Merkle root it already has. This proves the balance is in the snapshot without storing the full 32 MB.

## Constrained Node Epoch Summary

LoRa nodes and other constrained devices don't participate in epoch consensus. They receive a compact summary from their nearest capable peer:

```
EpochSummary {
    epoch_number: u64,
    merkle_root: Blake3Hash,               // root of full account snapshot
    proposer_id: NodeID,                   // who proposed this epoch
    proposer_sig: Ed25519Signature,        // signature over (epoch_number || merkle_root)
    my_epoch_balance: u64,                     // frozen balance at this epoch
    partner_epoch_balances: Vec<(NodeID, u64)>, // channel partners + trust neighbors
    bloom_segment: BloomFilter,            // relevant portion of settlement bloom
}
```

Typical size: under 5 KB for a node with 20-30 channel partners.

## Merkle Root Trust

The `merkle_root` is the anchor for all balance verification on constrained nodes. To prevent a malicious relay from feeding a fake root:

```
Merkle root acceptance:
  - If the source is a trusted peer (in trust graph): accept immediately
    (trusted peers have economic skin in the game)
  - If the source is untrusted: verify proposer_sig against proposer_id,
    then confirm with at least 1 additional independent peer in Ring 0/1
    (2-source quorum, same as DHT mutable object verification)
  - Cold start (no prior epoch): query 2+ peers and accept majority agreement
  - Retention: keep roots for the last 4 epochs (grace period for balance proofs)
```

The proposer's signature prevents trivial forgery — an attacker must either compromise the proposer's key or control the majority of a node's Ring 0 peers.

<!-- faq-start -->

## Frequently Asked Questions

<details className="faq-item">
<summary>Why is epoch compaction necessary for the CRDT ledger?</summary>

The settlement GSet (grow-only set) grows without bound as more settlements are processed. Without compaction, memory usage would eventually exceed device capacity — especially on constrained devices like ESP32 with ~520 KB usable RAM. Epoch compaction periodically snapshots the ledger state, freezes account balances, and replaces the full settlement history with a compact bloom filter. GCounter deltas are rebased to zero, preventing overflow even over centuries of operation.

</details>

<details className="faq-item">
<summary>How are epochs triggered in small mesh networks?</summary>

Small partitions use an adaptive trigger: an epoch fires after at least max(200, active_set_size × 10) settlements AND 1,000 gossip rounds (~17 hours) since the last epoch. This prevents a 20-node village from waiting months for the standard 10,000-settlement threshold. The 1,000-round floor also prevents epochs from firing too rapidly during bursty activity. Proposer eligibility requirements similarly adapt — a node needs only 3 direct links (not 10) in a small partition.

</details>

<details className="faq-item">
<summary>What happens if a settlement is missed during epoch compaction?</summary>

The verification window (4 epochs after activation) provides a recovery mechanism. Any node can submit a settlement proof — the full SettlementRecord — for any settlement not in the epoch's bloom filter. The bloom filter uses a 0.01% false positive rate (1 in 10,000) to minimize silent losses. Both parties to a settlement must retain the full record until the verification window closes. For cross-partition merges, the window extends to 8 epochs.

</details>

<details className="faq-item">
<summary>How do constrained devices like ESP32 nodes handle epoch data?</summary>

Constrained devices do not participate in epoch consensus. Instead, they receive a compact EpochSummary (typically under 5 KB) from their nearest capable peer, containing the Merkle root, their own frozen balance, channel partner balances, and a relevant bloom filter segment. When they need a balance they don't have locally, they request a Merkle proof (~640 bytes for 1M nodes) and verify it against the stored Merkle root.

</details>

<details className="faq-item">
<summary>How does the merge-time supply audit prevent inflation from partition attacks?</summary>

When partitions reconnect, the CRDT data merges unconditionally, but the minting component is audited separately. A cross-partition trust score is computed: each partition node is checked for trust relationships with main-network nodes at the time of the split. Minting from untrusted nodes is discounted or rejected entirely. Rejected minting enters a 10-epoch quarantine where partition nodes can submit trust proofs. Unclaimed minting is permanently discarded and balances are rebased.

</details>

<!-- faq-end -->

---

### Trust & Neighborhoods
<!-- Source: docs/economics/trust-neighborhoods.md -->

# Trust & Neighborhoods

Communities in Mehr are **emergent, not declared**. There are no admin-created "zones" to join, no governance to negotiate, no artificial boundaries between groups. Instead, communities form naturally from a trust graph — just like in the real world.

## The Trust Graph

Each node maintains a set of trusted peers:

> **Specification**
`TrustConfig` is the core social primitive: trusted peers get free relay, non-trusted traffic is priced per-byte, and transitive credit extends to friends-of-friends at a configurable ratio (default 10%, max 50%). Scopes are self-assigned hierarchical namespaces (max 1 Geo + 3 Topic, ≤ 1 KB total).

```
TrustConfig {
    // Peers I trust — relay their traffic for free
    trusted_peers: Set<NodeID>,

    // What I charge non-trusted traffic
    default_cost_per_byte: u64,

    // Per-peer cost overrides (discount for friends-of-friends, etc.)
    cost_overrides: Map<NodeID, u64>,

    // Transitive credit ratio for friends-of-friends (default 0.10)
    transitive_credit_ratio: f32,           // range [0.0, 0.50], default 0.10
    transitive_ratio_overrides: Map<NodeID, f32>,  // per-peer override

    // Self-assigned scopes — geographic and interest (purely informational)
    scopes: Vec<HierarchicalScope>,     // max 1 Geo + up to 3 Topic; total ≤ 1 KB

    // DEPRECATED: use scopes instead. Kept for backward compatibility.
    community_label: Option<String>,    // e.g., "portland-mesh"
}
```

**Adding a trusted peer is the only social action in Mehr.** Everything else — free local communication, community identity, credit lines — emerges from the trust graph.

Trust relationships are **asymmetric and revocable at any time**. Removing a node from `trusted_peers` immediately ends free relay for that node and downgrades any stored data from "trusted peer" to normal priority in the [garbage collection policy](../services/mhr-store#garbage-collection). Cost overrides are unidirectional — they apply to outbound traffic pricing from the configuring node only.

## How Communities Emerge

When a cluster of nodes all trust each other, a **neighborhood** forms:

```mermaid
graph LR
    Alice["Alice"] <-->|trust| Bob["Bob"]
    Bob <-->|trust| Carol["Carol"]
    Alice <-->|trust| Dave["Dave"]
    Bob <-->|trust| Dave
    Carol <-->|trust| Dave
```

Alice, Bob, Carol, and Dave are a neighborhood. No one "created" it. No one "joined" it. It exists because they trust each other.

### Properties

- **No admin**: Nobody runs the neighborhood. It has no keys, no governance, no admission policy.
- **No fragmentation**: The trust graph is continuous. There are no hard boundaries between communities — neighborhoods overlap naturally when people have friends in multiple clusters.
- **No UX burden**: You just mark people as trusted. The same action you'd take when adding a contact.
- **Fully decentralized**: There is nothing to attack, take over, or censor.

## Free Local Communication

Traffic between trusted peers is **always free**. A relay node checks its trust list:

```
Relay decision:
  if sender is trusted AND destination is trusted:
    relay for free (no lottery, no channel update)
  else if sender is trusted:
    relay for free (helping a friend send outbound)
  else:
    relay with stochastic reward lottery
```

Note the asymmetry: a relay helps its trusted peers **send** traffic for free, but does not relay free traffic from strangers just because the destination is trusted. Without this rule, an untrusted node could route unlimited free traffic through you to any of your trusted peers, shifting relay costs onto you without compensation.

This means a village mesh where everyone trusts each other operates with **zero economic overhead** — no tokens, no channels, no settlements. The economic layer only activates for traffic crossing trust boundaries.

## Trust-Based Credit

When a node needs MHR (e.g., to reach beyond its trusted neighborhood), its trusted peers can vouch for it:

```
Transitive credit:
  Direct trust:           full credit line (set by trusting peer)
  Friend-of-friend (2 hops): configurable ratio of direct credit line (default 10%, max 50%)
  3+ hops of trust:        no credit (too diluted)

  If a credited node defaults, the vouching peer absorbs the debt.
  This makes trust economically meaningful — you only trust people
  you'd lend to.
```
> **Key Insight**
Trust in Mehr has real economic cost: vouching for a peer means absorbing their potential debts. This makes the trust graph a natural Sybil defense — creating fake identities is free, but getting anyone to trust (and financially back) them is not.

The credit line is **rate-limited** for safety:

| Trust Distance | Max Credit | Rate Limit |
|---------------|-----------|-----------|
| Direct trusted peer | Configurable by trusting node | Per-epoch (configurable) |
| Friend-of-friend | Configurable % of direct limit (default 10%, max 50%) | Per-epoch, per friend-of-friend |
| Beyond 2 hops | None | N/A |

```
Credit accounting:
  Each trusting node tracks outstanding credit per grantee:

  CreditState {
      grantee: NodeID,
      credit_limit: u64,              // max outstanding μMHR
      outstanding: u64,               // currently extended
      granted_this_epoch: u64,        // epoch-scoped rate limit
      last_grant_epoch: u64,          // for epoch-boundary reset
  }

  Rules:
    - Direct peers: each gets a separate credit_limit (set in TrustConfig)
    - Friend-of-friend: each gets transitive_credit_ratio × vouching peer's direct limit,
      tracked independently per grantee
    - granted_this_epoch resets to 0 at each epoch boundary
    - Outstanding credit that exceeds limit: no new grants until repaid
    - Default handling: vouching peer's outstanding balance increases by
      the defaulted amount (absorbs debt); grantee is flagged
```

## Hierarchical Scopes

Nodes self-assign **scopes** — hierarchical namespaces that describe where they are and what they care about. Scopes replace the flat `community_label` with a structured system that supports both place-based communities and interest communities.

```
HierarchicalScope {
    scope_type: enum {
        Geo,    // place hierarchy (physical or virtual)
        Topic,  // interest/community hierarchy
    },
    segments: Vec<String>,    // hierarchical path, max 8 levels, max 32 chars each
}

Scope constraints:
  Per node (TrustConfig):    max 1 Geo + up to 3 Topic (4 total)
  Per content (PostEnvelope): max 1 Geo + up to 3 Topic (4 total)
  Total scope bytes:          ≤ 1,024 bytes (UI shows remaining space)

Rationale:
  - You are physically in one place at a time
  - Voting is geo-scoped — multiple geo scopes would enable double-voting
  - RadioRangeProof verifies one location
  - Interests are multi-dimensional — multiple topics are natural
  - Hierarchical prefix matching means a single deep tag covers multiple query levels
  - 3 topics is expressive enough: most users have 1-2 primary interests
  - 1 KB cap keeps scope data small for constrained devices (ESP32)
```

Scopes are **hierarchical namespaces**, similar to URLs. The `segments` are arbitrary strings — they can describe physical locations, virtual spaces, organizations, or anything else. The `scope_type` signals **propagation intent**, not physicality: `Geo` means "this is a place where members are dense and nearby each other" (whether physically or virtually), while `Topic` means "this is an interest that spans across places."

### Geo Scopes

Geo scopes describe **places** — physical locations, virtual spaces, or any community with dense, place-like membership:

```
Geo scope examples:

  Physical locations:                    Virtual places:

  north-america                          cyberspace
  └── us                                 └── guild-wars
      ├── oregon                         │   ├── server-42
      │   ├── portland                   │   └── server-7
      │   │   ├── hawthorne ◀── Alice    │
      │   │   └── pearl     ◀── Bob     └── discord
      │   ├── eugene                         └── mehr-dev ◀── Dave
      │   └── bend
      └── california                     organizations
          └── ...                        └── university
                                             └── mit
  asia                                       └── csail ◀── Eve
  └── iran
      └── tehran
          └── district-6   ◀── Carol
```

```
Physical:  Geo("north-america", "us", "oregon", "portland", "hawthorne")
Virtual:   Geo("cyberspace", "guild-wars", "server-42")
Org:       Geo("organizations", "university", "mit", "csail")
```

The hierarchy is **bottom-up** — neighbors form the base, cities emerge from connected neighborhoods, regions from connected cities. This applies equally to physical and virtual places: you join a game server first, then discover the broader game community, then the gaming umbrella. The pattern is the same — local connections aggregate into larger structures.

### Interest Scopes

Interest scopes describe communities of shared interest that span geography:

```
Dave sets:   Topic("gaming", "pokemon", "competitive")
Eve sets:    Topic("science", "physics", "quantum")
Frank sets:  Topic("music", "jazz")
```

Interest communities are **sparse** — not everyone in Portland cares about Pokemon. A Pokemon community might span Portland, Tokyo, and Berlin with nothing in between. This is the opposite of geographic scopes, which are **dense** (most people are physically somewhere).

### Scope Matching

Subscriptions and queries can match at any level of the hierarchy:

| Pattern | Matches |
|---------|---------|
| `Geo("north-america", "us", "oregon", "portland")` exact | Only Portland |
| `Geo("north-america", "us", "oregon")` prefix | Portland, Eugene, Bend, and everything in Oregon |
| `Geo("north-america", "us")` prefix | All US scopes |
| `Topic("gaming")` prefix | Pokemon, Minecraft, and all gaming sub-topics |
| `Topic("gaming", "pokemon")` exact | Only Pokemon, not gaming broadly |

### Properties

Scopes retain all the properties of `community_label`:

- **Self-assigned** — no one approves your scope claims, no authority enforces them
- **Not authoritative** — scopes carry no protocol-level privileges (cannot grant access, waive fees, or modify trust)
- **Not unique** — multiple disjoint clusters can use the same scope
- **Free-form strings** — all segments are arbitrary strings. Communities converge on naming through social consensus (e.g., "portland" not "pdx"), the same way they do today. No ISO codes, no standardized taxonomy, no gatekeeping.
- **Used by services** — [MHR-Name](../services/mhr-name) scopes names by geographic scope, [MHR-Pub](../services/mhr-pub) supports `Scope(match)` subscriptions, [MHR-DHT](../services/mhr-dht#neighborhood-scoped-dht) uses scopes for content propagation boundaries, and the [Social](../applications/social) layer uses scopes for geographic and interest feeds

### Geo Scope Verification

Geo scopes can optionally be **verified** — see [MHR-ID](../services/mhr-id) for the full verification protocol. Verification methods vary by the kind of place:

| Place Type | Verification Method | Precision |
|-----------|-------------------|-----------|
| **Physical neighborhood** | [RadioRangeProof](../services/mhr-id/verification#radiorangeproof) — if you can hear a node's LoRa beacon, you're within physical range | ~1–15 km |
| **Physical city/region** | Bottom-up aggregation of verified neighborhood claims | Aggregated |
| **Virtual space** | Application-specific (e.g., server-signed attestation, invite-chain, admin vouch) | Varies |
| **Organization** | Peer attestation from existing verified members | Social |

Interest scopes are **never verified** — anyone can declare interest in Pokemon. Verification matters for geo scopes that carry governance weight (see [Voting](../applications/voting)) — a node cannot vote on Portland issues without a verified Portland-area presence claim. Unverified geo scopes are still useful for content routing and feed subscriptions; they just don't carry voting rights.

### Wire Format

Designed for constrained devices:

| Field | Size | Description |
|-------|------|-------------|
| `scope_type` | 1 byte | 0 = Geo, 1 = Topic |
| `segment_count` | 1 byte | Number of path segments (max 8) |
| `segments` | variable | Length-prefixed UTF-8 (1-byte length + content per segment) |

Maximum size per scope: 2 + 8 × 33 = 266 bytes. Total scope data is capped at **1,024 bytes** (1 KB). With max 4 scopes (1 Geo + 3 Topic), typical usage is well under the cap — e.g., `Geo("us", "oregon", "portland")` (15 bytes) + `Topic("gaming", "pokemon")` (18 bytes) + `Topic("music", "jazz")` (16 bytes) = 49 bytes. The cap only binds when using deep hierarchies (8 segments × 32 chars) across multiple scopes.

### Backward Compatibility

The `community_label` field is retained for backward compatibility. New nodes populate both:

```
Migration:
  community_label: "portland-mesh"
    → scopes: [Geo("portland")]

  Old nodes: read community_label, ignore scopes
  New nodes: read scopes, fall back to community_label if scopes is empty
```

### Geo vs. Topic: Two Dimensions of Community

| | Geo Scopes (Places) | Topic Scopes (Interests) |
|---|---|---|
| **Density** | Dense — members are nearby each other | Sparse — members are scattered |
| **Propagation** | Bottom-up through proximity | Wide, across places |
| **Verification** | RadioRangeProof (physical), attestation (virtual), or none | None needed (self-declared) |
| **Content cost** | Cheap locally, expensive globally | Depends on relay distance |
| **Voting** | Enables scoped voting (if verified) | No voting implications |
| **Physical examples** | `Geo("north-america", "us", "oregon", "portland")` | `Topic("gaming", "pokemon")` |
| **Virtual examples** | `Geo("cyberspace", "guild-wars", "server-42")` | `Topic("science", "physics")` |
| **Multiplicity** | One per node/content | Up to 3 (within 1 KB total) |

A single post can have **one** geographic scope and **multiple** interest scopes. A post tagged `Geo("portland") + Topic("gaming", "pokemon")` appears in both the Portland local feed and the global Pokemon feed. Intersection queries ("Portland Pokemon") are resolved client-side by filtering on both scopes.

## Comparison: Explicit Zones vs. Trust Neighborhoods

| Aspect | Explicit Zones | Trust Neighborhoods |
|--------|---------------|---------------------|
| Creation | Someone creates a zone | Emerges from mutual trust |
| Joining | Request + approval | Mark someone as trusted |
| Governance | Admin keys, voting | None needed |
| Boundaries | Hard, declared | Soft, overlapping |
| Free communication | Within zone boundary | Between any trusted peers |
| Naming | `alice@zone-name` | `alice@geo:portland` (hierarchical scopes) |
| Sybil resistance | Admission policy | Trust is social and economic (you absorb their debts) |
| UX complexity | Create, join, configure | Add contacts |

## Security Considerations

<details className="security-item">
<summary>Sybil Identity Flooding</summary>

**Vulnerability:** An attacker creates many fake identities to gain disproportionate influence or free relay in the network.

**Mitigation:** The trust graph provides natural Sybil resistance through four layers:
1. **Trust has economic cost** — vouching for a node means absorbing its potential debts. Sybil identities with no real relationships get no credit.
2. **Rate limiting** — even if a malicious node gains one trust relationship, transitive credit is capped by configurable ratio (default 10%, max 50%) per hop.
3. **Reputation dilution** — creating many identities dilutes reputation rather than concentrating it. A node's usefulness as a relay/service provider is what earns trust over time.
4. **Local detection** — a node's trust graph is visible to its peers. A node trusting an unusual number of new, unproven identities is itself suspicious.

</details>

## Real-World Parallels

| Real World | Mehr Trust |
|-----------|------------|
| Talk to your neighbor for free | Free relay between trusted peers |
| Lend money to a friend | Transitive credit via trust |
| Wouldn't lend to a stranger | No credit without trust chain |
| Communities aren't corporations | Neighborhoods have no admin |
| You belong to multiple groups | Trust graph is continuous, not partitioned |
| Reputation builds over time | Trust earned through reliable service |

<!-- faq-start -->

## Frequently Asked Questions

<details className="faq-item">
<summary>How do communities actually form if there’s no “create community” button?</summary>

You simply mark other people as trusted peers in your node’s configuration. When a cluster of people mutually trust each other, a neighborhood emerges organically — no one needs to create, name, or administrate it. It’s the same way friend groups form in real life: you start trusting individuals, and the group structure follows.

</details>

<details className="faq-item">
<summary>What if I’m new and don’t trust anyone yet?</summary>

You can still participate — you’ll just pay standard relay fees in MHR for traffic that crosses trust boundaries. As you interact with neighbors and build relationships, you’ll naturally add trusted peers. Gateway operators can also onboard new users by adding them as trusted peers, giving immediate free access within the gateway’s neighborhood.

</details>

<details className="faq-item">
<summary>Can I revoke trust once I’ve granted it?</summary>

Yes, instantly. Removing a node from your `trusted_peers` list takes effect immediately. That node loses free relay through you, and any stored data from them drops to normal priority in garbage collection. Trust is asymmetric and unilateral — you control your own trust list without needing the other party’s permission.

</details>

<details className="faq-item">
<summary>Are my trust relationships visible to everyone on the network?</summary>

Your trust graph is visible to your direct peers (they can see who you relay for free), but it is not broadcast globally. Neighbors observe trust relationships through relay behavior — if you relay someone’s traffic for free, nearby nodes can infer a trust link. There is no public trust directory that lists all relationships.

</details>

<!-- faq-end -->

---

### Real-World Economics
<!-- Source: docs/economics/real-world-impact.md -->

# Real-World Economics

Mehr doesn't exist in a vacuum. It interacts with the existing internet economy — ISPs, cloud providers, and the people who pay for connectivity today. This page examines what happens when a mesh network meets existing infrastructure economics.

## The Apartment Building Scenario

Consider a typical apartment building in Denmark:

```mermaid
graph TD
    subgraph current["Current Model"]
        ISP1["ISPs"]
        A1["Apt 1\n200 kr/month"]
        A2["Apt 2\n200 kr/month"]
        A3["..."]
        A50["Apt 50\n200 kr/month"]
        ISP1 -->|"own router,\nown subscription"| A1
        ISP1 --> A2
        ISP1 --> A3
        ISP1 --> A50
        TOTAL1["50 × 200 kr = 10,000 kr/month\nUtilization per connection: < 5%"]
    end

    subgraph mehr["Mehr Model"]
        ISP2["ISPs"]
        GW1["Gateway Node 1\n200 kr/month"]
        GW2["Gateway Node 2\n200 kr/month"]
        R1["Apt 1\npays MHR"]
        R2["Apt 2\npays MHR"]
        R3["..."]
        R47["Apt 47\npays MHR"]
        ISP2 --> GW1
        ISP2 --> GW2
        GW1 -->|"WiFi mesh"| R1
        GW1 --> R2
        GW2 --> R3
        GW2 --> R47
        TOTAL2["2-3 gateways = 400-600 kr/month\nISP revenue drops ~94%"]
    end
```

### Why This Works

Residential internet connections are massively over-provisioned. A 1 Gbps connection serving one household averages under 50 Mbps actual usage, and most of that is concentrated in evening hours. The infrastructure exists to handle peak load, but sits idle the vast majority of the time.

With Mehr, 2-3 well-placed gateway nodes with good internet connections can serve an entire building. The gateway operators earn MHR from the other residents — effectively becoming micro-ISPs within their building.

### What Happens to ISPs?

**Mehr doesn't kill ISPs. It restructures them.**

| Today | With Mehr |
|-------|-----------|
| ISPs sell per-household subscriptions | ISPs sell per-building or per-community connections |
| Revenue depends on subscriber count | Revenue depends on bandwidth sold |
| Last-mile infrastructure to every apartment | Last-mile to building entry point; mesh handles internal distribution |
| ISPs handle per-customer support | Gateway operators handle local support |
| ISPs own the customer relationship | The community owns its own network |

The key insight: **ISPs already don't want to be last-mile providers.** Last-mile infrastructure (running cable to every apartment) is their most expensive, lowest-margin business. Mehr handles last-mile distribution through the mesh, letting ISPs focus on what they're actually good at — backbone transit and peering.

ISPs would likely respond by:
1. Offering **building connections** — one fat pipe per building at a higher bandwidth tier
2. Pricing by **bandwidth consumed**, not by connection count
3. Becoming **backbone providers** to Mehr gateway operators
4. Running their own **Mehr backbone nodes** to earn routing fees

### The Math for Gateway Operators

```
Gateway operator costs:
  Internet subscription: 200 kr/month
  Hardware (Pi 4 + LoRa + modem): ~300 kr one-time (~25 kr/month amortized over 1 year)
  Total: ~225 kr/month

Gateway operator revenue:
  ~47 apartments paying for shared internet
  If each pays 50 kr/month equivalent in MHR: 2,350 kr/month
  After subtracting costs: ~2,125 kr/month profit

Resident savings:
  Was paying: 200 kr/month
  Now paying: ~50 kr/month in MHR
  Saving: 150 kr/month (75% reduction)
```

Both sides win. Gateway operators earn significant income from hardware they'd have anyway. Residents save money. The only loser is the ISP's per-household billing model — which was always an artifact of last-mile economics, not actual cost.
> **Key Insight**
Mehr doesn’t kill ISPs — it restructures them. Last-mile distribution moves to the mesh, letting ISPs focus on backbone transit. A 50-unit apartment building drops from 50 subscriptions to 2–3 shared gateways, saving residents ~75% while creating income for gateway operators.

## How You Earn on Mehr

Every node earns proportionally to the value it provides:

### Relay Earnings

The simplest way to earn. Any node that forwards packets for non-trusted traffic participates in the [stochastic relay lottery](payment-channels). More traffic through your node = more lottery wins = more MHR.

```
Relay earnings estimate (at ~5 μMHR expected reward per packet):
  Minimal relay (ESP32 + LoRa): ~5,000–50,000 μMHR/month
    → ~30-300 packets/day, zero operating cost (solar powered)

  Community bridge (Pi Zero + WiFi): ~50,000–500,000 μMHR/month
    → Bridges LoRa to WiFi, moderate traffic

  Gateway (Pi 4 + cellular): ~500,000–5,000,000 μMHR/month
    → Internet uplink, high-value traffic

  Backbone (mini PC + directional WiFi): 5,000,000+ μMHR/month
    → High-throughput transit between mesh segments
```

### Storage Earnings

Nodes with disk space earn by storing data for the network via [MHR-Store](../services/mhr-store):

- Store popular content that others request frequently
- Host replicated data for availability
- Cache content for faster local access

### Compute Earnings

Nodes with CPUs or GPUs earn by executing contracts and offering inference via [MHR-Compute](../services/mhr-compute):

- Run MHR-Byte contracts for constrained nodes
- Offer WASM execution for heavier workloads
- Provide ML inference (speech-to-text, translation, image generation)

### Gateway Earnings

The highest-value service. Internet gateway operators earn from:

- HTTP proxy services
- DNS relay
- Bridge traffic between mesh and internet
- All of the above, plus relay/storage/compute earnings

### What Makes a Node Valuable?

The marketplace naturally prices capabilities based on scarcity and utility:

| Factor | Effect on Earnings |
|--------|-------------------|
| **Connectivity** | More links = more routing traffic = more relay earnings |
| **Location** | Strategic position (bridge between clusters) = higher routing value |
| **Uptime** | 24/7 availability = more agreements, better reputation |
| **Storage capacity** | More disk = more storage contracts |
| **Compute power** | GPU = high-value inference contracts |
| **Internet access** | Gateway capability = premium pricing |
| **Trust network size** | More trusted peers = higher credit lines, more routing |

## Broader Economic Implications

### For Developing Regions

In areas with no ISP at all, Mehr enables community networks from scratch:

1. One satellite or cellular connection serves an entire village via mesh
2. The gateway operator earns from the community
3. Community members earn by relaying for each other and for outsiders
4. Economic activity within the mesh is free (trusted peers)
5. External connectivity costs are shared, not per-household

### For Urban Areas

In cities where internet is available but expensive:

1. Shared internet connections reduce per-household costs by 50-75%
2. Local services (storage, compute, messaging) run on the mesh with no cloud dependency
3. Community infrastructure becomes an income source, not a cost center

### For Censorship-Resistant Communication

When governments control the internet:

1. The mesh operates independently of ISP infrastructure
2. Even if internet gateways are shut down, local communication continues
3. Gateway nodes with satellite uplinks or VPN tunnels become high-value — and the market prices them accordingly

## The Concentration of Compute

### The Problem

The world's compute infrastructure is concentrating at an unprecedented rate. Hyperscale data centers — owned by a handful of companies — now account for 44% of global data center capacity (Q1 2025, Synergy Research), up from under 30% five years ago. Three cloud providers (AWS, Azure, Google Cloud) control 63% of the cloud infrastructure market. By 2030, hyperscalers are projected to command 61% of all global data center capacity, with on-premise facilities shrinking to just 22%.

AI has accelerated this concentration dramatically. A single AI training run in 2025 requires 15,000–25,000 advanced GPUs at a cost of $120M–$450M. NVIDIA controls 80–92% of the AI accelerator market, with its entire Blackwell chip production run sold out before launch. The supply chain is even more concentrated than the compute itself: three manufacturers (SK Hynix, Samsung, Micron) control 100% of HBM (High Bandwidth Memory) production — the specialized memory required for AI accelerators. SK Hynix alone holds ~60% market share and has sold out its entire 2026 supply. HBM is not available to consumers at all — it is soldered onto GPU packages during manufacturing, never sold as a separate component.

This is industry gatekeeping at the hardware level. The ability to train and run AI models is increasingly restricted to organizations that can secure allocation from a single chip vendor and a three-company memory oligopoly. Microsoft alone announced $80B in AI infrastructure spending for 2025. The gap between those who can access compute and those who cannot is widening faster than at any point in the history of computing.

### Why Crypto Doesn't Fix This

Decentralized networks were supposed to distribute power. In practice, the dominant consensus mechanisms reproduce existing inequality:

**Proof of Work** concentrates around cheap electricity and specialized hardware. Six mining pools mine over 95% of all Bitcoin blocks. The top two pools (Foundry and AntPool) control ~50% of total network hashrate. Individual miners with consumer hardware cannot compete.

**Proof of Stake** concentrates around existing capital. Ethereum whale wallets hold 57% of supply (December 2024). Two staking entities control ~60% of the staking market. PoS has a built-in compounding effect: larger stakers earn proportionally more rewards, making wealth concentration self-reinforcing. The entity-clustered Gini coefficient for Bitcoin is approximately 0.83 — worse than any national economy's wealth distribution.

Decentralized compute projects (Akash, Render, Golem) have attempted to build distributed alternatives to cloud computing. The results are mixed. Akash has ~1,000 GPUs across 65+ data centers, but its growth strategy involves onboarding 7,200 enterprise-grade GPUs operated by vetted "Nodekeepers" — re-centralizing by another name. Render Network's top 100 wallets hold 45% of circulating supply. These projects improve on hyperscaler monopolies, but their economic models still favor large, well-capitalized operators over distributed participants.

The pattern is consistent: **when all compute is valued equally regardless of location, capital concentration wins.** Whoever can deploy the most hardware in the cheapest facility captures the most value.

### Why Mehr Is Structurally Different

Mehr breaks this pattern because **not all compute, storage, and bandwidth are valued equally.** The protocol's economics are designed around a principle that distinguishes it from every other decentralized network: proximity to demand determines value.

#### Relay Economics: Every Hop Earns Independently

In Mehr, every relay node on a multi-hop path runs its own [stochastic reward lottery](payment-channels). A message crossing 8 hops generates 8 independent earning opportunities — one for each relay. This means:

- **Distributed relay networks earn more in aggregate** than a single concentrated hub handling the same traffic
- **A node positioned between two communities** (a bridge node) earns on every packet crossing the boundary — regardless of how powerful it is
- **An ESP32 on a rooftop** earning relay fees on a busy LoRa link can out-earn a rack server in a data center that nobody routes through

The value of a relay node is determined by its position in the network topology, not its hardware specifications.

#### Discovery: Local-First by Design

[Service discovery](../marketplace/discovery) uses concentric rings. Ring 0 (direct neighbors) exchanges full capability information for free every 60 seconds. Ring 1 (2–3 hops) shares summarized capabilities every few minutes. Ring 2+ provides only coarse hints, on demand.

The consequence: **most service requests resolve locally.** A storage request finds a nearby provider before it ever discovers a distant data center. A compute job delegates to a neighbor's GPU before querying the wider network. The further out a query goes, the higher the latency and cost — which naturally incentivizes local provision of common capabilities.

#### Cost-Weighted Routing: Distance Costs Money

Mehr's routing scores each path using a weighted combination of XOR distance, [cumulative cost](../protocol/network-protocol#routing), and latency. Under the default "Cheapest" routing policy (β=0.8 cost weight), cumulative cost dominates path selection. Every hop adds cost. A compute provider 2 hops away will almost always be selected over an equivalent provider 10 hops away — because the 10-hop path costs 5× more in relay fees.

This is the opposite of cloud economics, where a centralized data center amortizes distance through backbone networks. In Mehr, **the network charges for distance.** A GPU in your neighbor's garage is cheaper to use than a GPU farm across the continent, even if the farm has 1000× more hardware.

#### Trust Neighborhoods: Free at the Edge, Paid at the Core

[Trust neighborhoods](trust-neighborhoods) create zero-cost zones. Trusted peers relay for each other for free — no tokens, no channels, no overhead. A village mesh where everyone trusts each other operates with zero economic friction.

This means a small community with distributed hardware (a few Raspberry Pis, some storage, maybe a GPU) operates as a self-sufficient compute cluster at zero cost. The economic layer only activates when requests cross trust boundaries. Concentrated data centers, by definition, exist outside most users' trust neighborhoods — they always incur economic costs that local providers don't.

#### Anti-Concentration Mechanisms

Three protocol-level mechanisms prevent capital from capturing the network:

1. **Non-deterministic service assignment**: Clients cannot choose which node serves their request. [DHT ring assignment](../services/mhr-dht) uses `hash(content_id || epoch_hash)` — the epoch hash is unpredictable, preventing a large operator from grinding assignments toward their own nodes.

2. **Net-income revenue cap**: [Minting eligibility](token-economics#revenue-capped-minting) is based on net income (income minus spending), not gross revenue. An operator cycling MHR between their own nodes generates net income of zero — no minting. Self-dealing is structurally unprofitable in any connected network.

3. **Active-set scaled emission**: A 3-node partition (or a 3-node cluster operated by one entity) receives only 3% of full emission. You cannot earn more by concentrating — you earn proportionally to the network's active set size.

Together, these mechanisms ensure that **a single entity operating 1,000 nodes in a data center earns less per node than 1,000 independent operators each running one node in different locations.** The concentrated operator faces zero net income from internal transfers, while the distributed operators each earn genuine net income from serving distinct local demand.

### What This Means in Practice

| Scenario | Cloud / Traditional Crypto | Mehr |
|----------|--------------------------|------|
| **Who earns from relay?** | Backbone operators with the most bandwidth | Every relay node on every path, proportional to traffic |
| **Who earns from storage?** | Whoever operates the most disks | Nodes closest to where data is requested (DHT + cost weighting) |
| **Who earns from compute?** | Whoever deploys the most GPUs | Nodes nearest to the requester (discovery rings + routing cost) |
| **Can a whale dominate?** | Yes — more capital → more mining/staking → more revenue | No — net-income cap makes self-dealing unprofitable; non-deterministic assignment prevents selection |
| **Does hardware concentration help?** | Yes — economies of scale in data centers | Limited — proximity advantage outweighs scale advantage for most workloads |
| **Does geographic distribution help?** | No — it increases latency and management cost | Yes — distributed nodes are closer to more users, earning more from proximity |

### Honest Limitations

Mehr does not solve the concentration of compute. It restructures incentives so that distributed participation is valued, but it does not conjure GPUs out of thin air:

- **AI training** requires massive parallel compute that a distributed mesh cannot provide. Training a frontier model needs 15,000+ GPUs with fast interconnects. Mehr serves inference and edge workloads, not training.
- **HBM scarcity** is a physical supply chain problem. Mehr can't create memory that doesn't exist. What it can do is make efficient use of the consumer hardware that does exist — especially for inference, where a single GPU with consumer VRAM can serve many requests.
- **Economies of scale still exist** for raw hardware procurement. A data center buying 10,000 GPUs gets better per-unit pricing than an individual buying one. Mehr's advantage is in the *operational* economics — proximity reduces relay costs, trust reduces overhead, and the revenue cap prevents monopoly pricing.
- **Not all workloads benefit equally.** Latency-sensitive edge workloads (inference, real-time processing, content delivery) benefit most from proximity-based pricing. Batch processing workloads with no latency requirements may still be cheaper in centralized facilities.

The claim is not that Mehr eliminates inequality. The claim is that Mehr's economic structure — uniquely among decentralized networks — **values where you are, not how much you have.** A solar-powered relay on a rooftop in Lagos serving its neighborhood earns based on the traffic it carries, not the capital behind it. That is a structural difference from every proof-of-work, proof-of-stake, and existing decentralized compute market.
> **Key Insight**
Mehr’s anti-concentration mechanisms (non-deterministic assignment, net-income revenue cap, active-set scaled emission) ensure that 1,000 independent operators each running one node in different locations earn more per node than a single entity running 1,000 nodes in a data center. Proximity to demand — not capital — determines value.


---

### Content Propagation
<!-- Source: docs/economics/propagation.md -->

# Content Propagation

Content on Mehr doesn't spread uniformly — it follows economic gravity. Local content is cheap and fast. Global content costs more but reaches more people. Popular content funds its own propagation. Unpopular content stays local or expires. No algorithm decides what lives and what dies — economics does.

## Geographic Propagation

Geographic content follows a **bottom-up, demand-driven** model. Content starts local and bubbles upward as demand increases.

### Cost by Scope Level

| Scope Level | Relay Cost | Cache Density | Typical Latency |
|-------------|-----------|---------------|----------------|
| Neighborhood | ~free (trusted peers) | Very high | Seconds |
| City | Low (short paths, many caches) | High | Seconds to minutes |
| Region | Moderate | Moderate | Minutes |
| Country | Higher | Sparse | Minutes to hours |
| Continent/Global | Highest | Very sparse | Hours (LoRa) to minutes (internet gateway) |

The cost gradient is **natural** — it costs more relay hops to reach distant nodes, so the price is higher. There is no artificial pricing; relay fees accumulate per hop.

### Upward Propagation

Content climbs the scope hierarchy through three mechanisms:

**1. Demand-driven caching**: When readers at a broader scope request content, intermediate nodes cache it. A Portland post requested by 10 Oregon readers gets cached at relay nodes between Portland and the rest of Oregon. Those caches then serve future Oregon requests cheaply.

```
Portland post → requested by Eugene reader
  → relay nodes between Portland and Eugene cache the post
  → next Eugene reader gets it from the cache (cheaper, faster)
  → after N requests, the post is effectively "Oregon-scoped"
```

**2. Speculative caching**: Storage nodes observe demand signals (retrieval frequency) and speculatively cache popular content. A storage node in Seattle might cache a popular Portland post because serving it to Seattle readers is profitable.

```
Storage node decision:
  observed_demand = retrievals_per_epoch for this content
  hosting_cost = storage_cost_per_epoch
  expected_revenue = observed_demand × average_retrieval_fee × (255 - kickback_rate) / 255

  if expected_revenue > hosting_cost:
    cache the content (profitable to serve)
```

**3. Author-funded promotion**: An author can explicitly pay for wider scope by creating storage agreements with providers in broader geographic areas. This is the equivalent of "promoting" a post — paying for reach.

### The Self-Funding Loop

When a post earns more in [kickback](../services/mhr-store#revenue-sharing-kickback) than it costs to store, it becomes **self-sustaining**:

> **Key Insight**
Popular content funds its own propagation. When kickback revenue exceeds storage cost, content becomes self-sustaining and climbs scope levels automatically — no algorithm, no promotion, just economics. Unpopular content dies naturally when funding stops.

```mermaid
graph TD
    A[Author pays initial cost] --> B[Storage - N epochs]
    B --> C[Readers pay to access]
    C --> D[Kickback to author]
    D --> E[Reinvest]
    E --> F[Extend storage / wider scope]
    F --> B
```

```
Self-funding threshold:
  kickback_per_epoch > cost_per_epoch → content is immortal
  kickback_per_epoch < cost_per_epoch → author subsidizes or content expires
```

**Popular content climbs automatically:**

| Phase | Scope | Funding |
|-------|-------|---------|
| 1. Published | Neighborhood | Author pays |
| 2. Local hit | City | Kickback covers neighborhood; surplus pays for city scope |
| 3. Regional hit | Region | City kickback funds regional storage |
| 4. Viral | Country/Global | Self-funding at all levels; may exceed author's cost |

**Unpopular content follows the opposite trajectory:** kickback doesn't cover costs, author stops paying, storage agreements expire, content is garbage-collected. No moderation needed — economics handles content lifecycle.

## Interest Propagation

Interest communities are **sparse** — they span geography. A `Topic("gaming", "pokemon")` post from Portland might interest readers in Tokyo, Berlin, and Buenos Aires, with nothing in between.

```mermaid
graph TD
    subgraph Geographic["Geographic (dense, local-first)"]
        direction BT
        G_Author[Author] --> G_Nbhd1[Nbhd]
        G_Nbhd1 --> G_City1[City]
        G_Nbhd2[Nbhd] --> G_City1
        G_City1 --> G_Region1[Region]
        G_City2[City] --> G_Region1
        G_City3[City] --> G_Region2[Region]
        G_Region1 --> G_Country[Country]
        G_Region2 --> G_Country
    end

    subgraph Interest["Interest (sparse, validated-then-global)"]
        direction BT
        I_Author[Author] --> I_Validation[Local validation]
        I_Validation --> I_Relay1[Interest relay node]
        I_Validation --> I_Relay2[Interest relay node]
        I_Validation --> I_Relay3[Interest relay node]
        I_Relay1 --> I_Portland[Portland]
        I_Relay2 --> I_Tokyo[Tokyo]
        I_Relay3 --> I_Berlin[Berlin]
    end
```

> **Geographic** bubbles UP with demand. **Interest** spreads OUT after local validation.

### Interest Relay Nodes

Interest communities naturally develop **interest relay nodes** — nodes that subscribe to a topic and bridge between geographic clusters:

```
Portland ──LoRa──▶ [Portland Pokemon fan] ──internet gateway──▶ [Tokyo Pokemon fan] ──LoRa──▶ Tokyo mesh
                   (interest relay)                              (interest relay)
```

Interest relay nodes earn relay fees for bridging communities. They subscribe to a topic via [MHR-Pub Scope subscriptions](../services/mhr-pub#interest-feeds) and forward content to other interested nodes.

### Local-First Interest Propagation

Interest content follows the same demand-driven model as geographic content — **start local, propagate outward only when validated**. An envelope tagged `Topic("gaming", "pokemon")` does not immediately reach every Pokemon subscriber worldwide. Instead:

```
Interest propagation stages:

  1. PUBLISH: Envelope propagates to the author's local geographic cluster
     (same as any post — reaches nearby nodes first)

  2. LOCAL VALIDATION: Interest relay nodes in the author's cluster observe:
     - Did anyone in my cluster boost this envelope?
     - Did N distinct nodes fetch the full post? (default N = 3)
     - Did a curator include it in a feed?
     If none of these: envelope stays local.

  3. OUTBOUND: Once locally validated, interest relay nodes forward the
     envelope to peer interest relay nodes in other geographic clusters.

  4. REMOTE CLUSTERS: Each receiving cluster applies the same validation
     before forwarding further. Content spreads through the interest graph
     one cluster at a time, gated by local traction at each hop.
```

**Why this matters**: Without local validation, a garbage post tagged with a popular topic would reach every subscriber globally — the envelope is free, so there's no economic brake. Local-first propagation ensures that only content validated by the author's community reaches distant clusters. The author's neighbors are the first quality gate.

**Validation thresholds** (configurable per interest relay node):

| Signal | Default Threshold | Rationale |
|--------|------------------|-----------|
| Boost from trusted node | 1 boost | Someone in the local cluster endorsed it |
| Full post retrievals | 3 distinct nodes | Multiple people found it worth paying for |
| Curator inclusion | 1 curator | A human with reputation selected it |

An interest relay node can set its own thresholds — a high-traffic relay might require more validation before forwarding, while a small community relay might forward after a single boost. The thresholds are a local policy, not a protocol constant.

### Interest vs. Geographic Cost

| | Geographic | Interest |
|---|---|---|
| **Propagation pattern** | Dense, local-first | Sparse, local-first then global |
| **Cost driver** | Relay distance (hops) | Relay distance (may cross continents) |
| **Caching** | Many nearby caches (dense) | Few caches, widely spaced (sparse) |
| **Self-funding** | Easy (many local readers) | Harder (fewer readers, higher relay costs) |
| **Typical delivery** | Push/Digest (short paths) | Digest/PullHint (long paths, constrained links) |
| **Quality gate** | Retrieval demand drives scope promotion | Local validation gates outbound relay |

Interest content is generally more expensive to propagate because it crosses more trust boundaries and relay hops. But interest communities can compensate by having dedicated relay infrastructure — nodes that specialize in bridging a specific topic.

> **Trade-off**
Interest content is inherently more expensive than geographic content — it crosses more trust boundaries and relay hops to reach a sparse global audience. Self-funding is harder because fewer readers are nearby. Dedicated interest relay nodes help but add infrastructure cost.

## Intersection: Geographic + Interest

A post tagged with both geographic and interest scopes propagates through **both** channels:

```
Post: "Pokemon tournament in Portland this Saturday"
Scopes: Geo("north-america", "us", "oregon", "portland") + Topic("gaming", "pokemon")

Propagation:
  Geographic: Portland neighborhood → Portland city (high priority, local event)
  Interest: Portland → global Pokemon community (relevant to traveling players)

Readers see it via:
  Portland feed subscribers (geographic)
  Pokemon feed subscribers (interest)
  Portland Pokemon intersection (both)
```

The economics naturally prioritize the right audience: local Portland readers get it cheaply (geographic proximity), global Pokemon fans get it at higher cost (interest relay), and Portland Pokemon fans get it through whichever channel is cheaper.

## Content Lifecycle

Every piece of content on Mehr follows an economic lifecycle. Content is born local and either grows through demand or expires through indifference:

```mermaid
graph LR
    Birth["BIRTH\nAuthor pays\nNeighborhood scope"] --> Growth["GROWTH\nKickback rises\nCaches multiply"]
    Growth --> Peak["PEAK\nSelf-funding\nWide caching"]
    Peak --> Decline["DECLINE\nDemand drops\nCaches evict"]
    Decline --> Expiry["EXPIRY\nGC'd or\nreduced scope"]
```

```
Phase 1: BIRTH
  Author creates post, pays for initial storage
  Content exists at neighborhood scope

Phase 2: GROWTH (if popular)
  Readers pay to access → kickback accumulates
  Speculative caching expands effective scope
  Storage nodes compete to host (profitable)

Phase 3: PEAK
  Content widely cached, cheaply accessible
  Kickback may exceed storage cost (self-funding)
  Maximum reach for its popularity level

Phase 4: DECLINE
  Readership drops → kickback drops
  Speculative caches evict (no longer profitable)
  Scope contracts back toward origin

Phase 5: EXPIRY (if not self-funding)
  Kickback below storage cost
  Author stops paying → grace period → garbage collection
  Or: remains self-funding indefinitely at reduced scope
```

### Comparison with Other Models

| | Centralized (Twitter) | Polycentric | Mehr |
|---|---|---|---|
| **What lives** | Platform decides | Server operator decides | Economics decides |
| **Content cost** | Free to post | Free to post | Author pays |
| **Popular content** | Algorithmically boosted | Same treatment as unpopular | Self-funds, propagates wider |
| **Unpopular content** | Shadow-banned or deprioritized | Lives until server drops it | Expires when funding stops |
| **Content lifespan** | Platform's discretion | Until all servers remove it | Until demand or author funding stops |
| **Spam** | ML moderation (arms race) | ML moderation | Economically irrational |

## Protocol Constants

| Constant | Value | Description |
|----------|-------|-------------|
| Kickback rate range | 0–255 | Author's share of retrieval fees (rate/255) |
| Default kickback rate | 128 (~50%) | Balanced split for social posts |
| Speculative cache threshold | Configurable | Retrievals/epoch before speculative caching triggers |
| Scope promotion threshold | Configurable | Kickback surplus that triggers scope expansion |

---

### Content Governance
<!-- Source: docs/economics/content-governance.md -->

# Content Governance

Mehr has no content moderation, no terms of service, and no central authority that decides what is allowed. Content governance is distributed: every node makes its own decisions about what to store, relay, and display. Censorship resistance and content governance are the same mechanism — **node sovereignty**.

## Node Autonomy

Every node has absolute control over three decisions:

```mermaid
flowchart TD
    Node["YOUR NODE"]
    Store["1. What to STORE<br/>Refuse any StorageAgreement"]
    Relay["2. What to RELAY<br/>Refuse to forward any envelope"]
    Display["3. What to DISPLAY<br/>Filter, block, hide anything"]
    Local["These are LOCAL decisions.<br/>The protocol does not enforce them network-wide.<br/>No node can override another node's choices."]

    Node --> Store
    Node --> Relay
    Node --> Display
    Store --> Local
    Relay --> Local
    Display --> Local
```

**Storing**: A storage node can refuse any [StorageAgreement](../services/mhr-store#storage-agreements) for any reason. No node is compelled to host content it objects to. The agreement is bilateral — both parties must consent.

**Relaying**: A relay node can inspect [PostEnvelope](../applications/social#postenvelope-free-layer) metadata and refuse to forward specific content. This is a local routing policy, not protocol enforcement. The node still relays other traffic normally.

**Displaying**: Client software can filter, block, or hide content based on any criteria — author blocklists, content-type filters, community norms, or user preferences. This happens entirely on the reader's device.

## Content Filtering

### Envelope-Based Filtering

PostEnvelopes contain metadata that nodes can use for filtering without fetching paid content:

| Envelope Field | Filtering Use |
|---------------|--------------|
| `scopes` | Block specific topics or geographic areas |
| `author` | Block specific authors (local blocklist) |
| `media_hints.content_type` | Block specific media types (e.g., refuse video) |
| `content_size` | Refuse to relay objects above a size threshold |
| `references` | Block content referencing known-bad posts |

A relay node's filtering policy is expressed as local configuration, not as a protocol message. Other nodes don't know what you filter — they only notice that you don't relay certain content.

### Trust-Based Enforcement

The most powerful governance mechanism is the [trust graph](trust-neighborhoods):

```mermaid
flowchart TD
    Discover["1. Alice discovers Bob is<br/>producing objectionable content"]
    Remove["2. Alice removes Bob<br/>from trusted_peers"]

    Discover --> Remove

    Remove --> LoseRelay["Loses free relay"]
    Remove --> LoseStorage["Loses free storage"]
    Remove --> LoseCredit["Credit line drops to zero"]
    Remove --> LoseRep["Trust distance increases"]

    Social["3. Carol and Dave also<br/>remove Bob (social consensus)"]

    LoseRelay --> Social
    LoseStorage --> Social
    LoseCredit --> Social
    LoseRep --> Social

    Social --> Isolated["Isolated from<br/>neighborhood free tier"]
    Social --> Paying["Paying full relay<br/>and storage costs"]
    Social --> NoCredit["Unable to get credit<br/>(no one vouches)"]
    Social --> MarketOnly["Visible only to nodes<br/>willing to serve at market rate"]
```

This mirrors real-world community enforcement. If your neighbors find your behavior unacceptable, they stop helping you. Nobody passes a law — social consequences are sufficient. The economic impact of trust revocation is real: lost credit lines, lost free relay, lost free storage.

### Community Norms

Communities can establish shared content policies through social consensus:

- "Our trust neighborhood doesn't relay content tagged with X"
- "Nodes that relay Y will be removed from trusted peers"
- "Our curators don't include Z in their feeds"

These are enforced through trust relationships, not protocol rules. A node that violates community norms faces trust revocation — which has economic consequences. No formal governance or voting is needed for communities to establish and enforce norms.

> **Key Insight**
Community moderation in Mehr mirrors real-world social enforcement: trust revocation costs the offender free relay, free storage, and credit lines. No formal governance is needed — a cluster of nodes independently removing someone from `trusted_peers` is economically equivalent to community expulsion.

## Why Illicit Content Is Harder on Mehr

| Factor | Regular Internet | Mehr |
|--------|-----------------|------|
| **Cost to publish** | Free (anonymous upload) | Author pays storage per epoch |
| **Author identity** | Anonymous (VPN, Tor, throwaway accounts) | NodeID attached; author known to local cluster |
| **Hosting** | Bulletproof hosting services exist | Every storage node can independently refuse |
| **Distribution** | Global CDN, unlimited free copies | [Local-first propagation](propagation#local-first-interest-propagation); global reach requires demand or payment |
| **Amplification** | Algorithms promote engagement | No algorithm — only human curators and boosts |
| **Social consequences** | None (anonymous) | Trust revocation, credit loss, neighborhood isolation |
| **Takedown** | Centralized (platform compliance) | Distributed (each node independently decides) |

### What Economics Limits

```
Economic barriers to abuse:

  Spam:
    Cost to post: N μMHR per epoch
    Revenue from spam: ~0 (nobody reads it)
    Result: economically irrational

  Low-demand illicit content:
    Cost to store: N μMHR per epoch (ongoing)
    Readership: small (niche audience)
    Kickback: below self-funding threshold
    Result: author pays indefinitely or content expires

  Automated distribution (bots):
    Bot needs MHR → must earn through service OR purchase
    Each bot post costs storage
    Local-first propagation means no automatic global reach
    Result: expensive and slow compared to internet-scale abuse

  Mass distribution:
    Local validation gates interest propagation
    Content needs real engagement (boosts, retrievals, curation)
    before reaching distant clusters
    Result: garbage can't go viral without genuine demand
```

### Formal Economic Model

**Spam Unprofitability Proof:**

```mermaid
flowchart LR
    Spammer["Spammer sends<br/>N messages"]
    Cost["Cost = N × C_relay<br/>+ N × C_store × T"]
    Revenue["Revenue = N × R_spam"]
    Result["Cost > Revenue<br/>ALWAYS"]

    Spammer --> Cost
    Spammer --> Revenue
    Cost --> Result
    Revenue --> Result
```

```
Spam profitability:
  Cost_spam = N × C_relay + N × C_store × T
  Revenue_spam = N × R_spam

  Profitable only if: R_spam > C_relay + C_store × T

  In Mehr:
    R_spam → 0 (client-side filtering, no algorithmic amplification)
    C_relay > 0 (paid relay for non-trusted traffic)
    C_store > 0 (ongoing per-epoch storage cost)

  Therefore: Cost > Revenue always. Spam is unprofitable at any scale.
```

**Trust-Flow Sybil Resistance:**

```mermaid
graph LR
    subgraph Honest["Honest Network"]
        A["Alice<br/>100 MHR credit"] <-->|"full trust"| B["Bob<br/>100 MHR credit"]
        B <-->|"full trust"| C["Carol<br/>100 MHR credit"]
        A <-->|"full trust"| C
    end

    subgraph Sybil["Sybil Cluster"]
        S1["Sybil-1<br/>0 MHR credit"]
        S2["Sybil-2<br/>0 MHR credit"]
        S3["Sybil-3<br/>0 MHR credit"]
        S1 <-->|"fake trust"| S2
        S2 <-->|"fake trust"| S3
        S1 <-->|"fake trust"| S3
    end

    B -->|"1 trust edge<br/>10 MHR max credit"| S1

    style Sybil fill:#fdd,stroke:#c00
    style Honest fill:#dfd,stroke:#0a0
```

The honest network has full mutual credit. The Sybil cluster has zero internal credit (no real economic backing). Even with one trust edge from Bob, Sybil-1 gets at most `transitive_credit_ratio × Bob's limit` — and Sybil-2 and Sybil-3 get nothing (3+ hops = no credit). Creating more Sybils doesn't increase available credit.

**Content Lifecycle Economic Model:**

```mermaid
flowchart LR
    Post["Author publishes post<br/>(pays storage)"]
    Propagate["Propagation cost<br/>(relay fees)"]
    Demand["Demand signal<br/>(retrievals + boosts)"]
    Threshold{"Kickback ≥<br/>storage cost?"}
    Sustain["Self-funding:<br/>content persists"]
    Expire["Below threshold:<br/>author pays or<br/>content expires"]

    Post --> Propagate
    Propagate --> Demand
    Demand --> Threshold
    Threshold -->|"Yes"| Sustain
    Threshold -->|"No"| Expire
```

### What Economics Doesn't Limit

A determined, well-funded group willing to pay high storage and relay costs can distribute content through the paid economy. The protocol does not prevent this.

The defense layers for this scenario:

```mermaid
flowchart TD
    Content["Well-funded bad actor<br/>publishes content"]
    L1["Layer 1: NODE REFUSAL<br/>Every storage and relay node<br/>can independently refuse"]
    L2["Layer 2: TRUST REVOCATION<br/>Local community discovers content,<br/>trust revocation isolates author"]
    L3["Layer 3: CURATOR EXCLUSION<br/>No curator includes content →<br/>no organic amplification"]
    L4["Layer 4: LOCAL VALIDATION<br/>Interest relays gate propagation<br/>on local engagement"]
    L5["Layer 5: ECONOMIC ATTRITION<br/>Ongoing storage costs,<br/>no self-funding threshold,<br/>no ad revenue"]

    Content --> L1
    L1 -->|"passes"| L2
    L2 -->|"passes"| L3
    L3 -->|"passes"| L4
    L4 -->|"passes"| L5
    L5 --> Result["Content bleeds MHR<br/>indefinitely with<br/>minimal reach"]
```

This is the same tradeoff every free society makes: some bad actors will misuse freedom. The alternative — giving any central authority the power to decide what's allowed — creates a worse problem. Mehr is more resistant than centralized platforms to censorship, but also more resistant to centralized takedown. Both properties come from the same design: node sovereignty.
> **Trade-off**
A well-funded bad actor can pay full market rates to distribute content — the protocol cannot prevent this. The five defense layers (node refusal, trust revocation, curator exclusion, local validation, economic attrition) limit reach and make it expensive, but they do not guarantee removal. This is the inherent tradeoff of node sovereignty.

## Not Censorship

Refusing to store or relay content is **not censorship** — it is individual autonomy.

```
Censorship:
  A central authority prevents content from existing or
  being accessed ANYWHERE on the network.
  → Impossible on Mehr (no central authority exists)

Node autonomy:
  An individual node chooses not to participate in
  distributing specific content.
  → Always possible on Mehr (every node is sovereign)
```

On Mehr, no single node can prevent content from existing. It can only refuse to help distribute it. If even one node in the network is willing to store and relay the content, it remains accessible to anyone willing to pay the relay costs to reach it. The author's freedom to publish and the node's freedom to refuse coexist without contradiction.

## Comparison With Other Models

| | Centralized Platform | Polycentric (ActivityPub) | Mehr |
|---|---|---|---|
| **Who decides** | Platform moderation team | Server administrator | Each node independently |
| **Enforcement** | Account ban, content removal | Server defederation | Trust revocation, storage/relay refusal |
| **Scope** | Global (platform-wide) | Per-server | Per-node |
| **Appeal** | Platform's discretion | Server operator's discretion | None needed (find a willing node) |
| **Collateral damage** | Affects all users on platform | Affects all users on defederated server | Affects only the specific node |
| **Can content survive?** | No (platform removes it) | Maybe (other servers may host) | Yes (if any node is willing) |
| **Can node refuse?** | N/A (users don't run nodes) | Yes (server operator decides) | Yes (every node operator decides) |

---

## Hardware

### Reference Designs
<!-- Source: docs/hardware/reference-designs.md -->

# Hardware Reference Designs

Mehr is designed to run on hardware ranging from $30 solar-powered relays to GPU workstations. Every device participates at whatever level its hardware allows.

## Device Tiers Overview

> **Specification**
Mehr defines five hardware tiers spanning $30 solar relays to GPU workstations. Each tier maps to a protocol participation level: Minimal nodes relay only (L1), while Gateway and above run the full L2 stack with marketplace, storage, and compute.

| Tier | Hardware | Cost | Power | Primary Role |
|------|----------|------|-------|-------------|
| **Minimal** | ESP32 + LoRa SX1276 | ~$30 | 0.5W (solar) | Relay only |
| **Community** | Pi Zero 2 W + LoRa HAT + WiFi | ~$100 | 3W | LoRa/WiFi bridge, basic compute |
| **Gateway** | Pi 4/5 + LoRa + cellular modem + SSD | ~$300 | 10W | Internet uplink, storage, compute |
| **Backbone** | Mini PC + directional WiFi + fiber | ~$500+ | 25W+ | High-bandwidth backbone |
| **Inference** | x86 + GPU + Ethernet | ~$500+ | 100W+ | Heavy compute, ML inference |

## Minimal Relay Node

**Target**: Lowest-cost, always-on mesh relay

```
Components:
  - ESP32-S3 microcontroller
  - LoRa SX1276/SX1262 radio module
  - Small solar panel (2W) + LiPo battery
  - Weatherproof enclosure

Capabilities:
  - Packet relay only
  - MHR-Byte interpreter (~50 KB)
  - No storage beyond routing tables
  - 24/7 operation on solar power

Software:
  - Mehr firmware (Rust, no_std)
  - Transport: LoRa only
  - Runs: routing, payment channels, gossip
  - Cannot run: WASM, storage, heavy compute
```

**Earns from**: Routing fees (1-5 μMHR per packet relayed)

**Range**: 2-15 km line-of-sight with LoRa

> **Key Insight**
A $30 solar-powered ESP32 relay earns MHR from routing fees at zero operating cost. The economics work because the node provides irreplaceable value — extending mesh range — that no centralized alternative can replicate from a data center.

## Community Bridge Node

**Target**: Bridge between LoRa mesh and local WiFi network

```
Components:
  - Raspberry Pi Zero 2 W
  - LoRa HAT (SX1262)
  - Built-in WiFi
  - SD card (32 GB)
  - USB power supply (5V/2A)

Capabilities:
  - LoRa ↔ WiFi bridging
  - Basic MHR-Compute (MHR-Byte + light WASM)
  - Local storage (~16 GB usable)
  - MHR-DHT participation
  - Message caching for offline nodes

Software:
  - Mehr daemon (Rust, Linux)
  - Dual transport: LoRa + WiFi
  - Full protocol stack
```

**Earns from**: Bridging fees, compute delegation, storage

## Gateway Node

**Target**: Internet uplink for the mesh

```
Components:
  - Raspberry Pi 4/5 (4 GB+ RAM)
  - LoRa HAT
  - 4G/LTE cellular modem (or Ethernet)
  - SSD (256 GB+)
  - Powered supply (12V)

Capabilities:
  - Internet gateway (HTTP proxy, DNS relay)
  - Full MHR-Store storage node
  - MHR-DHT backbone participation
  - Full WASM compute
  - Epoch consensus participation

Software:
  - Full Mehr stack
  - Triple transport: LoRa + WiFi + Cellular/Ethernet
  - Gateway proxy services
```

**Earns from**: Internet gateway fees, storage fees, compute fees, routing

## Backbone Node

**Target**: High-bandwidth infrastructure linking mesh segments

```
Components:
  - Mini PC (Intel NUC or equivalent)
  - Directional WiFi antenna (point-to-point)
  - Fiber connection (where available)
  - SSD (1 TB+)
  - UPS/battery backup

Capabilities:
  - High-throughput routing (100+ Mbps)
  - Large-scale storage
  - Full compute services
  - Neighborhood discovery services
  - Epoch consensus coordination

Software:
  - Full Mehr stack, optimized for throughput
  - Transport: Directional WiFi + Fiber + Ethernet
```

**Earns from**: Bulk routing fees, backbone transit, storage

## Inference Node

**Target**: Heavy compute (ML inference, transcription, TTS)

```
Components:
  - x86 PC or server
  - GPU (NVIDIA RTX series or equivalent)
  - Ethernet connection
  - SSD (512 GB+)
  - Standard power supply

Capabilities:
  - ML model inference (Whisper, LLaMA, Stable Diffusion, etc.)
  - Speech-to-text, text-to-speech
  - Translation services
  - Any GPU-accelerated computation

Software:
  - Full Mehr stack
  - WASM runtime + native GPU compute
  - Model serving framework
  - Advertises offered_functions with pricing
```

**Earns from**: Compute fees for ML inference and heavy processing

---

### Device Capabilities by Tier
<!-- Source: docs/hardware/device-tiers.md -->

# Device Capabilities by Tier

Each hardware tier has different capabilities, which determine what the node can do on the network and how it earns MHR.

## Capability Matrix

> **Key Insight**
Every tier participates in gossip and payment channels — even the cheapest ESP32 relay. The protocol scales by restricting *what* each tier does (no WASM on Minimal, no ML on Gateway), not *whether* it participates in the economy.

| Capability | Minimal | Community | Gateway | Backbone | Inference |
|-----------|---------|-----------|---------|----------|-----------|
| Packet relay | Yes | Yes | Yes | Yes | Yes |
| LoRa transport | Yes | Yes | Yes | Optional | No |
| WiFi transport | No | Yes | Yes | Yes (directional) | Optional |
| Cellular transport | No | No | Yes | No | No |
| Ethernet/Fiber | No | No | Optional | Yes | Yes |
| MHR-Byte contracts | Yes | Yes | Yes | Yes | Yes |
| WASM contracts | No | Light | Full | Full | Full |
| MHR-Store storage | No | ~16 GB | ~256 GB | ~1 TB | ~512 GB |
| MHR-DHT participation | Minimal | Yes | Backbone | Backbone | Yes |
| Epoch consensus | No | No | Yes | Yes | Yes |
| Internet gateway | No | No | Yes | Optional | Optional |
| ML inference | No | No | No | No | Yes |
| Gossip participation | Full | Full | Full | Full | Full |
| Payment channels | Yes | Yes | Yes | Yes | Yes |

## Power and Deployment

| Tier | Power Draw | Power Source | Typical Deployment |
|------|-----------|-------------|-------------------|
| Minimal | 0.5W | Solar + LiPo | Outdoor, pole/tree-mounted |
| Community | 3W | USB wall adapter | Indoor, near window for LoRa |
| Gateway | 10W | 12V supply | Indoor, weatherproof enclosure |
| Backbone | 25W+ | Mains + UPS | Indoor, rack-mounted or desktop |
| Inference | 100W+ | Mains | Indoor, rack or desktop |

## Earning Potential

What each tier naturally earns from:

### Minimal (ESP32 + LoRa)
- Packet relay fees only
- Low per-packet revenue but high volume and zero operating cost (solar)
- Value: extends mesh range, maintains connectivity

### Community (Pi Zero + LoRa + WiFi)
- Bridging fees (LoRa ↔ WiFi translation)
- Basic compute delegation
- Small-scale storage
- Value: connects LoRa mesh to local WiFi network

### Gateway (Pi 4/5 + Cellular)
- Internet gateway fees (highest per-byte revenue)
- Storage fees
- Compute fees
- Epoch consensus participation
- Value: connects mesh to the wider internet

### Backbone (Mini PC + Directional WiFi)
- High-volume transit routing
- Large-scale storage
- Full compute services
- Value: high-bandwidth links between mesh segments

### Inference (GPU/NPU Workstation)
- ML inference fees (highest per-invocation revenue)
- Heavy compute services
- Includes: GPU workstations, servers with NPU/TPU, FPGA accelerators
- Value: provides advanced capabilities to the entire mesh

## Delegation Patterns

Since nodes delegate what they can't do locally, natural delegation chains form:

```
Minimal relay
  → delegates everything except routing to →
Community bridge
  → delegates bulk storage and internet to →
Gateway node
  → delegates heavy compute to →
Inference node (GPU/NPU)
```

Each delegation is a bilateral [capability agreement](../marketplace/agreements) with payment flowing through [channels](../economics/payment-channels).

> **Trade-off**
Delegation chains add latency and cost at each hop. A Minimal relay delegating compute to a Community node, which delegates storage to a Gateway, creates a 3-link economic dependency. If any link fails, the upstream node must re-discover and re-negotiate — trading simplicity for resilience.


---

## Interoperability

### Cross-Network Compatibility
<!-- Source: docs/interoperability/overview.md -->

# Cross-Network Compatibility

Mehr is not an island. A decentralized mesh protocol that can't talk to other networks is a walled garden with extra steps. This page explores how Mehr connects to existing protocols, which projects are worth bridging, and the architectural principles that make interoperability possible without compromising Mehr's core design.

## Design Principle: Bridges as Services, Not Primitives

Mehr deliberately avoids building interoperability into the protocol layer. Instead, bridges are **standalone gateway services** that advertise in the [capability marketplace](../marketplace/overview) like any other service — storage, compute, or relay.

This was an explicit [design decision](../development/design-decisions#protocol-bridges-standalone-gateway-services). The rationale:

> **Key Insight**
Bridges are marketplace services, not protocol primitives. Each bridge is discoverable, negotiable, verifiable, and payable through the same capability marketplace that handles storage and compute — no special protocol support required.

- **Protocol bridges need persistent connections** to external systems (Matrix federation, SSB replication, Nostr relays). MHR-Compute contracts are sandboxed with no network I/O — bridges don't fit the compute model.
- **Each external protocol evolves independently.** A protocol-level primitive would need to track Matrix spec changes, SSB protocol upgrades, and Nostr NIPs — an endless maintenance burden on the core spec.
- **The marketplace already solves service discovery.** A bridge is just another capability: discoverable, negotiable, verifiable, payable.
- **Bridge operators can specialize.** One operator runs a Matrix bridge. Another runs SSB. A third bridges both plus Nostr. They set their own pricing and compete on quality.

```mermaid
flowchart TB
    MEHR["Mehr Network"]
    MEHR --> MB["Matrix Bridge\n(L2)"]
    MEHR --> SB["SSB Bridge\n(L2)"]
    MEHR --> NB["Nostr Bridge\n(L2)"]
    MB --> MF["Matrix Federation"]
    SB --> SP["SSB Pubs"]
    NB --> NR["Nostr Relays"]
```

Each bridge is an L2 Mehr node that also speaks an external protocol. From the Mehr side, it's a service provider. From the external side, it's a participant in that protocol's network.

## Identity Attestation

The core interop mechanism builds on [MHR-ID's ExternalIdentity claims](../services/mhr-id/verification#identity-linking). An ExternalIdentity claim is a signed assertion linking a Mehr identity to an external platform account, verified via [crawler or OAuth challenges](../services/mhr-id/verification#crawler-challenge) — the same methods used by [FUTO ID](https://docs.polycentric.io/futo-id/).

For protocol bridges specifically, the bridge node acts as a **verification oracle** — it can verify the user's external identity via OAuth and publish a vouch, just like any [verification oracle](../services/mhr-id/verification#verification-oracles). The bridge also stores a protocol-specific attestation for message routing:

```
BridgeAttestation {
    mehr_pubkey: Ed25519PublicKey,       // Mehr identity
    external_protocol: enum {
        Matrix, SSB, Nostr, Briar, Meshtastic, LXMF
    },
    external_identity: Vec<u8>,          // protocol-specific ID
    bridge_node: NodeID,                 // which bridge created this
    timestamp: LamportTimestamp,
    signature: Ed25519Signature,         // signed by mehr_pubkey
}
```

**How it works**:

1. Alice has a Mehr identity (Ed25519 keypair) and a Matrix account (`@alice:example.org`)
2. She publishes an `ExternalIdentity` claim for platform `matrix`, handle `@alice:example.org`
3. She connects to a Matrix bridge service, which verifies her identity via OAuth and publishes a vouch
4. The bridge stores a `BridgeAttestation` for message routing
5. Other Mehr nodes can verify the claim via trust-weighted vouches — no need to trust the bridge blindly

This is stronger than a bridge-only attestation because the ExternalIdentity claim is part of Alice's [MHR-ID profile](../services/mhr-id#profile-assembly) — visible to anyone who views her profile, with verification status from multiple independent oracles.

**What the bridge doesn't know**: The content of E2E encrypted messages passing through it. The bridge translates metadata and routing, not plaintext.

**What the bridge does know**: Which Mehr identity maps to which external identity. This is inherent — you can't bridge without knowing both sides. Users choose which bridges to trust with this mapping.

## Payment Flows

Bridge economics use existing Mehr primitives — no new payment mechanisms required.

### Mehr-to-External

Alice (Mehr) sends a message to Bob (Matrix):

```
Alice → [pays relay MHR] → Bridge Node → [Matrix federation, bridge pays] → Bob
```

- Alice pays Mehr-side relay costs via normal [payment channels](../economics/payment-channels)
- The bridge operator pays Matrix-side costs (homeserver hosting, bandwidth)
- Bridge recoups costs through service fees (per-message, subscription, or ad-supported)

### External-to-Mehr

Carol (Matrix) sends a message to Dave (Mehr):

```
Carol → [Matrix federation, free] → Bridge Node → [pays relay MHR] → Dave
```

- Carol sends via Matrix (free from her perspective)
- The bridge node bears Mehr-side relay costs
- Bridge recoups via Matrix-side monetization, donations, or operates as a public good
- Alternatively, Dave pays the bridge for inbound message delivery (pull model)

### Cross-Bridge

Alice (SSB) sends to Bob (Matrix), both connected to Mehr:

```
Alice → SSB Bridge → [Mehr relay] → Matrix Bridge → Bob
```

Both bridges participate in the Mehr network. The message traverses Mehr's mesh as ordinary encrypted traffic. Each bridge handles its own external protocol costs.

## What Makes a Good Bridge Target

Not every protocol is worth bridging. The best candidates share these properties:

| Property | Why It Matters |
|----------|---------------|
| **Decentralized identity** | Attestation works without asking permission from a central authority |
| **Offline tolerance** | Mehr's store-and-forward model maps naturally to protocols that handle delays |
| **Gossip or relay-based** | Similar distribution model; easier to translate |
| **Ed25519 or compatible crypto** | Reduces key management complexity |
| **Active community** | Bridge is only useful if people use the other side |
| **Open protocol** | Can implement a bridge without licensing or API keys |

Properties that make bridging harder:

| Property | Challenge |
|----------|-----------|
| **Requires always-on internet** | Bridge node must maintain persistent connection; can't operate in mesh-only mode |
| **Global consensus required** | Blockchain-based protocols add settlement latency and complexity |
| **Proprietary or closed** | Can't implement without reverse engineering or vendor cooperation |
| **Different encryption model** | E2E translation requires re-encryption at the bridge (breaks zero-knowledge) |

## Bridge Categories

### Transport-Level Bridges

These operate at Layer 0 — translating between Mehr's transport and another mesh protocol's transport. Packets flow between networks at the radio/link level.

**Example**: [Meshtastic](meshtastic) — LoRa mesh nodes that can forward Mehr packets as opaque payloads.

**Advantage**: Deepest integration. External nodes contribute physical infrastructure (radio coverage, relay hops) to the Mehr mesh.

**Challenge**: Requires the external protocol to support opaque payload forwarding.

### Protocol-Level Bridges

These operate at the application layer — translating messages, posts, or data between Mehr's service primitives and an external protocol's data model.

**Examples**: [Matrix](matrix) (room ↔ topic), [Scuttlebutt](scuttlebutt) (feed ↔ DHT), Nostr (event ↔ pub).

**Advantage**: No changes needed to the external protocol. The bridge is just another client/server on both sides.

> **Trade-off**
Protocol-level bridges face semantic mismatch: Mehr’s immutable DataObjects don’t map 1:1 to Matrix’s mutable room state or SSB’s append-only feeds. Each bridge needs protocol-specific translation logic, and E2E encryption breaks at the bridge boundary.

**Challenge**: Semantic mismatch. Mehr's immutable DataObjects don't map 1:1 to Matrix's mutable room state or SSB's append-only feeds. Each bridge needs protocol-specific translation logic.

### Ecosystem Bridges

These connect Mehr to protocols it already shares infrastructure with — particularly the [Reticulum ecosystem](reticulum-ecosystem).

**Example**: LXMF messages carried natively on the same Reticulum transport Mehr uses.

**Advantage**: Near-zero translation overhead. Same wire format, same crypto, same transport.

**Challenge**: Coordinating upgrade paths as Mehr adds economic extensions that pure Reticulum nodes don't understand.

## Compatibility Landscape

### Tier 1 — High Alignment, Build First

| Project | Bridge Type | Key Alignment | Detailed Page |
|---------|------------|---------------|---------------|
| **Meshtastic** | Transport | Same LoRa hardware, massive deployed base | [Meshtastic Bridge](meshtastic) |
| **Reticulum / LXMF** | Ecosystem | Shared transport layer, native coexistence | [Reticulum Ecosystem](reticulum-ecosystem) |
| **Scuttlebutt (SSB)** | Protocol | Gossip-based, offline-first, Ed25519, aligned values | [Scuttlebutt Bridge](scuttlebutt) |

### Tier 2 — Good Fit, Build Second

| Project | Bridge Type | Key Alignment | Notes |
|---------|------------|---------------|-------|
| **BitTorrent** | Protocol | Content-addressed, massive DHT (10–25M nodes), BEP-44 uses Ed25519 | [BitTorrent Bridge](bittorrent) |
| **Matrix** | Protocol | Federated, well-specified, transitive bridge access to dozens of protocols | [Matrix Bridge](matrix) |
| **Nostr** | Protocol | Simple event model, sovereignty-focused, growing community | Relay ↔ MHR-Pub translation |
| **Yggdrasil** | Transport | Encrypted mesh overlay; alternative backbone for internet-connected nodes | Could supplement Reticulum for IP links |

### Tier 3 — Interesting, Community-Driven

| Project | Bridge Type | Key Alignment | Notes |
|---------|------------|---------------|-------|
| **IPFS / libp2p** | Protocol | Content-addressing, libp2p as alternative transport | Heavy bandwidth assumptions conflict with constrained links |
| **Briar** | Protocol | Tor-based, offline-capable, similar threat model | Tor dependency adds complexity |
| **Althea** | Protocol | Paid relay economics, shared incentive model | Ethereum dependency, different economic model |

### Tier 4 — Watch, Don't Build

| Project | Why Watch | Why Wait |
|---------|-----------|----------|
| **Holochain** | Agent-centric like Mehr, CRDT-compatible | Heavy runtime, small community |
| **GNUnet** | Privacy-focused mesh, strong academic foundation | Small community, complex protocol |
| **Dat / Hypercore** | Append-only logs, good P2P sync | Niche adoption, no incentive layer |

## What Mehr Does NOT Bridge

Some things are deliberately out of scope:

- **Cross-protocol atomic swaps.** Mehr's CRDT ledger has different finality guarantees than blockchains. Token exchange happens through gateway operators or bilateral agreement, not protocol-level swap primitives.
- **Universal identity federation.** Mehr doesn't maintain a global directory mapping all identities across all protocols. [ExternalIdentity claims](../services/mhr-id/verification#identity-linking) link your Mehr key to external platforms, but each bridge maintains its own routing attestations. Users choose which bridges they trust.
- **Protocol-level name resolution for external systems.** Mehr's [naming system](../services/mhr-name) resolves Mehr names via trust-weighted resolution. External names resolve through their own systems, with bridges translating at the boundary.
- **Backward compatibility shims.** Bridge operators handle version mismatches. The Mehr protocol doesn't adapt its wire format to accommodate external protocol changes.

## Building a Bridge

For developers wanting to create a bridge service:

1. **Run an L2 Mehr node** — full protocol stack, marketplace participation
2. **Advertise bridging as a capability** — `compute.offered_functions` includes bridge-specific function IDs
3. **Implement identity attestation** — store and serve `BridgeAttestation` records
4. **Handle message translation** — protocol-specific logic for each direction
5. **Manage payment** — collect fees via Mehr [service agreements](../marketplace/agreements), pay external protocol costs
6. **Gossip attestation availability** — so other Mehr nodes can discover which identities are reachable through your bridge

The bridge is a regular Mehr service. It earns MHR through service agreements, pays for relay through payment channels, and builds reputation through the [trust system](../economics/trust-neighborhoods). No special protocol support is needed — the marketplace handles everything.

## Roadmap Integration

Cross-network bridges are planned for [Phase 4](../development/roadmap#phase-4-full-ecosystem) (Milestone 4.3), after the core protocol, economics, mobile apps, and mesh radio are proven. This is deliberate — bridges depend on a stable, battle-tested protocol. Building bridges before the foundation is solid creates fragile integrations that break with every protocol change.

The phasing:

| Phase | Interop Activity |
|-------|-----------------|
| Phase 1 | TCP/IP transport only — servers talk to each other |
| Phase 2 | Economics validated — bridge payment model can be tested |
| Phase 3 | Multi-transport proven — bridge nodes can run on diverse hardware |
| Phase 4 | **Protocol bridges ship** — SSB, Matrix, Briar as standalone gateway services |

Early community experimentation with bridges during Phase 2-3 is encouraged — the marketplace is designed to support it. But official bridge implementations come after the protocol stabilizes.

---

### Meshtastic Bridge
<!-- Source: docs/interoperability/meshtastic.md -->

# Meshtastic Bridge

Meshtastic is the highest-priority bridge target for Mehr. Tens of thousands of LoRa nodes are already deployed worldwide — cheap, solar-powered, community-operated. A transport-level bridge between Mehr and Meshtastic gives Mehr instant access to physical radio infrastructure without waiting for dedicated Mehr hardware deployments.

## Why Meshtastic First

| Factor | Detail |
|--------|--------|
| **Same physical layer** | Both use LoRa on ISM bands (868/915 MHz). Same radios, same antennas, same propagation characteristics. |
| **Massive hardware base** | LILYGO T-Beam, Heltec WiFi LoRa 32, RAK WisBlock — the same boards listed in Mehr's [reference designs](../hardware/reference-designs). Many are already deployed and powered. |
| **Low barrier** | No firmware changes needed on existing Meshtastic nodes for basic L0 relay. Bridge nodes handle translation. |
| **Complementary features** | Meshtastic provides GPS tracking, channel-based messaging, range testing. Mehr adds economics, storage, compute, and E2E encryption. |
| **Community alignment** | Open-source, community-driven, off-grid focused. Same user base. |

## Integration Architecture

The bridge operates at the **transport level** — deeper than a typical protocol bridge. Rather than translating application-layer messages, it translates at the packet/frame level, allowing Meshtastic nodes to participate as physical infrastructure in the Mehr mesh.

### Three Integration Modes

#### Mode 1: Meshtastic as L0 Transport (Opaque Relay)

Meshtastic nodes forward Mehr packets as opaque payloads without understanding them.

```mermaid
flowchart LR
    L2A["Mehr L2 Node"] --> BA["Bridge Node\nMehr L1 + MT"]
    BA --> MT1["MT Node\n(L0)"]
    MT1 --> MT2["MT Node\n(L0)"]
    MT2 --> MT3["MT Node\n(L0)"]
    MT3 --> BB["Bridge Node\nMehr L1 + MT"]
    BB --> L2B["Mehr L2 Node"]
    style MT1 fill:#e8d5f5,stroke:#7b2d8e
    style MT2 fill:#e8d5f5,stroke:#7b2d8e
    style MT3 fill:#e8d5f5,stroke:#7b2d8e
```

**How it works**:

1. Bridge node runs dual firmware: Mehr L1 + Meshtastic
2. Mehr packet arrives at bridge node via Mehr routing
3. Bridge encapsulates it as a Meshtastic `TEXT_MESSAGE_APP` or custom `PRIVATE_APP` portnum payload
4. Meshtastic mesh forwards it using Meshtastic's own flood/routing
5. Destination bridge node extracts the Mehr packet and delivers it to the Mehr network

**Meshtastic payload format**:

```
MehrOverMeshtastic {
    magic: u16 = 0x4D48,             // "MH" — identifies Mehr payload
    version: u8,                      // encapsulation version
    flags: u8,                        // fragmentation, priority
    fragment_id: u16,                 // for payloads exceeding Meshtastic MTU
    fragment_offset: u8,              // fragment sequence
    fragment_total: u8,               // total fragments
    payload: [u8],                    // Mehr packet (encrypted, opaque to MT nodes)
}
```

**MTU handling**: Meshtastic's maximum payload is ~228 bytes (varies by region/settings). Mehr packets that exceed this are fragmented at the bridge and reassembled at the destination bridge. The `fragment_id` field allows interleaving fragments from different Mehr packets.

**Key property**: Existing Meshtastic nodes require **zero changes**. They see a Meshtastic packet and forward it like any other. The Mehr content is opaque — encrypted and meaningless to nodes that don't understand it.
> **Key Insight**
Mode 1 (opaque relay) gives Mehr instant access to the entire deployed Meshtastic network with zero firmware changes. Existing nodes forward Mehr packets as regular Meshtastic messages — they never need to understand the content.

#### Mode 2: Meshtastic Nodes as Mehr L0 (Firmware Extension)

A lightweight firmware module lets Meshtastic nodes understand Mehr's announce format and participate in Mehr routing as L0 transport nodes.

```mermaid
flowchart LR
    A["Mehr L2"] <--> B["Mehr L1"]
    B <--> C["MT + L0\nMehr L0 module"]
    C <--> D["MT + L0\nMehr L0 module"]
    D <--> E["Mehr L1"]
    E <--> F["Mehr L2"]
    style C fill:#e8d5f5,stroke:#7b2d8e
    style D fill:#e8d5f5,stroke:#7b2d8e
```

**What the L0 module does**:

- Recognizes Mehr announce packets (by magic bytes in the Meshtastic payload)
- Forwards them using Meshtastic's mesh routing (opaque byte forwarding)
- Reports link quality metrics (RSSI, SNR, hop count) that bridge nodes translate to [CompactPathCost](../protocol/network-protocol#mehr-extension-compact-path-cost)
- Does NOT parse economic extensions, run VRF lottery, or maintain payment channels

**Implementation**: ~2-5 KB of additional firmware on ESP32. The module hooks into Meshtastic's packet receive/forward pipeline and recognizes the `0x4D48` magic prefix.

**Benefit over Mode 1**: Better routing decisions. L0-aware Meshtastic nodes can prioritize Mehr traffic and report accurate link metrics, rather than treating Mehr packets as generic messages competing with Meshtastic traffic.

#### Mode 3: Dual-Protocol Node (Full Convergence)

A single device runs both Meshtastic and Mehr L1, sharing the same LoRa radio via time-division.

```mermaid
flowchart TB
    subgraph node["Dual-Protocol Node"]
        direction TB
        subgraph stacks[" "]
            direction LR
            MT["Meshtastic\nStack"]
            MEHR["Mehr L1\nStack"]
        end
        TDMA["Radio Time-Division\nManager (TDMA/slot)"]
        RADIO["LoRa Radio\nSX1262/76"]
    end
    MT --> TDMA
    MEHR --> TDMA
    TDMA --> RADIO
```

**Time-division approach**:

- Radio time is split between Meshtastic and Mehr traffic
- Default: 70% Meshtastic / 30% Mehr (configurable)
- Priority override: Mehr relay lottery wins get immediate transmission
- Listen periods are shared — both stacks receive all packets

**Why time-division, not frequency-division**: Most LoRa nodes have a single radio on a single frequency. Frequency splitting would halve bandwidth for both protocols. Time-division preserves full bandwidth for whichever protocol is transmitting.

**Target hardware**: ESP32-S3 with SX1262 (e.g., Heltec WiFi LoRa 32 V3, LILYGO T-Beam Supreme). These have enough flash (8 MB) and RAM (512 KB) for both stacks.

## Bridge Node Specification

A Meshtastic-Mehr bridge node is a physical device that participates in both networks. Minimum requirements:

| Component | Requirement |
|-----------|------------|
| **MCU** | ESP32-S3 (dual-core, 512 KB SRAM, 8 MB flash) |
| **Radio** | SX1262 LoRa transceiver |
| **Firmware** | Meshtastic + Mehr L1 (Mode 2 or 3) |
| **Power** | Solar viable (bridge adds ~15% power consumption over base Meshtastic) |
| **Cost** | $15-30 (same hardware as existing Meshtastic nodes) |

### Bridge Capabilities Advertised

The bridge node advertises itself in the Mehr [capability marketplace](../marketplace/overview):

```
NodeCapabilities {
    connectivity: {
        bandwidth_bps: 1200,          // LoRa link speed
        latency_ms: 2000,             // typical LoRa hop latency
        cost_per_byte: 10,            // μMHR per byte (higher than WiFi)
        internet_gateway: false,
    },
    bridge: {
        protocols: [Meshtastic],
        meshtastic_channels: 8,       // number of MT channels bridged
        meshtastic_region: "US",      // regulatory region
        mt_node_count: 23,            // known MT nodes reachable
    },
    availability: Solar,              // or AlwaysOn if grid-powered
}
```

### Routing Cost Translation

Meshtastic provides hop count and SNR. Mehr needs [CompactPathCost](../protocol/network-protocol#mehr-extension-compact-path-cost). The bridge translates:

```
CompactPathCost from Meshtastic metrics:
    cumulative_cost = base_lora_cost × mt_hop_count
    worst_latency_ms = mt_hop_count × avg_lora_hop_latency
    bottleneck_bps = lora_datarate (from Meshtastic region config)
    hop_count = mt_hop_count + mehr_hop_count
```

This lets Mehr's [cost-weighted routing](../protocol/network-protocol#routing) make informed decisions about paths that traverse Meshtastic segments. A Mehr node choosing between a 3-hop WiFi path and a 2-hop Meshtastic path can compare costs accurately.

## Message Translation

### Mehr-to-Meshtastic

For users who want to reach Meshtastic contacts from Mehr:

1. Mehr user sends message to a Meshtastic destination (identified by bridge attestation)
2. Bridge node receives the Mehr packet, decrypts the transport layer
3. Bridge re-encrypts for the Meshtastic channel (PSK-based, per Meshtastic's model)
4. Message delivered as a standard Meshtastic text message
5. Meshtastic recipient sees: `[MHR:alice] Hello from the other side`

**Security note**: E2E encryption breaks at the bridge. Mehr uses per-recipient Ed25519-based encryption; Meshtastic uses shared channel PSK. The bridge must decrypt and re-encrypt. Users are warned that messages crossing the bridge are readable by the bridge operator. For sensitive content, both parties should be on the same protocol.

> **Trade-off**
The Meshtastic bridge is trust-sensitive: E2E encryption terminates at the bridge node. The bridge operator can read plaintext of all messages crossing protocols. For sensitive conversations, both parties must be on the same protocol — the bridge is only suitable for non-confidential traffic.

### Meshtastic-to-Mehr

1. Meshtastic user sends a message on a bridged channel
2. Bridge node receives the Meshtastic packet
3. If the message targets a Mehr user (prefix `@mehr:` or configured mapping), bridge translates
4. Bridge encrypts E2E for the Mehr recipient and sends via Mehr routing
5. Bridge pays Mehr relay costs from its own balance

### Position and Telemetry

Meshtastic nodes broadcast GPS position and device telemetry. Bridges can optionally translate this:

- **Position** → Mehr `GeoPresence` claim (with consent — GPS data is sensitive)
- **Telemetry** → Mehr node health metrics (battery, temperature, signal quality)
- **Traceroute** → Contributes to Mehr's routing cost estimates for Meshtastic segments

This data flows one-way (Meshtastic → Mehr) unless the Meshtastic user has explicitly opted into Mehr identity attestation.

## Migration Path

For Meshtastic community members who want to adopt Mehr incrementally:

### Stage 1: Passive Bridge (Zero Changes)

- A community member deploys a bridge node alongside existing Meshtastic infrastructure
- Meshtastic nodes continue operating normally
- Bridge forwards Mehr traffic as opaque Meshtastic payloads (Mode 1)
- Meshtastic users notice nothing different
- Mehr users gain LoRa coverage through the Meshtastic mesh

### Stage 2: Awareness (Optional Firmware Update)

- Interested Meshtastic operators flash the L0-aware firmware extension
- Their nodes start reporting link quality to bridge nodes
- Mehr routing improves across Meshtastic segments
- No economic participation yet — pure transport contribution

### Stage 3: Economic Participation (L1 Upgrade)

- Operators who want to earn MHR upgrade to dual-protocol firmware (Mode 3)
- Their nodes participate in the VRF relay lottery
- Relay wins earn MHR through [payment channels](../economics/payment-channels)
- A $20 solar LoRa node becomes a revenue-generating relay

### Stage 4: Full Mehr (L2)

- Operators with Raspberry Pi or better hardware run full Mehr nodes
- Participate in storage, compute, and marketplace
- Meshtastic continues as one of their radio interfaces
- They are now bridge operators, earning from both relay and bridge services

Each stage is optional. A community can stay at Stage 1 indefinitely — Mehr gets radio coverage, Meshtastic users experience no disruption. The upgrade path exists for those who want economic participation.

## Meshtastic Protocol Considerations

### Channel Allocation

Meshtastic supports up to 8 channels. The bridge uses one channel for Mehr traffic:

| Channel | Use |
|---------|-----|
| 0 | Default Meshtastic (LongFast, community) |
| 1-6 | User-configured Meshtastic channels |
| 7 | Mehr bridge traffic (configurable) |

The Mehr bridge channel uses a well-known PSK derived from the bridge node's public key. Meshtastic nodes that don't understand Mehr simply ignore traffic on this channel (standard Meshtastic behavior — unknown channels are not displayed).

### Regional Compliance

Meshtastic enforces regional LoRa parameters (frequency, bandwidth, duty cycle) per its firmware configuration. The bridge inherits these constraints:

- **EU868**: 1% duty cycle limits → Mehr traffic budgeted within this limit
- **US915**: More permissive → higher Mehr throughput available
- **Duty cycle accounting**: Bridge tracks airtime for both Meshtastic and Mehr traffic combined, never exceeding regional limits

### Mesh Routing Interaction

Meshtastic uses a managed flood routing protocol. Mehr uses [Kleinberg small-world routing](../protocol/network-protocol#routing). These models differ fundamentally:

- **Meshtastic**: Broadcast-oriented, packets flood to all reachable nodes
- **Mehr**: Unicast-oriented, packets follow cost-optimal paths

The bridge resolves this mismatch:

- **Mehr → Meshtastic**: Bridge sends as Meshtastic direct message (unicast within Meshtastic's routing) when possible, or broadcast on the bridge channel for unknown destinations
- **Meshtastic → Mehr**: Bridge receives all Meshtastic traffic on the bridge channel, forwards only packets addressed to Mehr destinations

## Hardware Compatibility

All hardware listed in Meshtastic's [supported devices](https://meshtastic.org/docs/hardware/devices/) is compatible with Mode 1 (opaque relay — no firmware changes). Mode 2 and 3 require firmware space:

| Device | Mode 1 | Mode 2 (L0) | Mode 3 (Dual) | Notes |
|--------|--------|-------------|---------------|-------|
| Heltec V3 | Yes | Yes | Yes | 8 MB flash, good for dual stack |
| LILYGO T-Beam | Yes | Yes | Yes | GPS included, solar-ready |
| RAK WisBlock | Yes | Yes | Yes | Modular, industrial use |
| Heltec V2 | Yes | Yes | Limited | 4 MB flash, tight for dual |
| nRF52840 boards | Yes | Partial | No | Different MCU, limited flash |

**Recommended bridge hardware**: LILYGO T-Beam Supreme S3 — ESP32-S3, SX1262, GPS, solar charging circuit, 8 MB flash. ~$30, solar-capable, proven in Meshtastic deployments.

---

### Reticulum Ecosystem
<!-- Source: docs/interoperability/reticulum-ecosystem.md -->

# Reticulum Ecosystem

Mehr is built on [Reticulum](https://reticulum.network/). This isn't a bridge — it's coexistence. Mehr extends Reticulum with economic primitives, storage, compute, and a capability marketplace. Pure Reticulum applications (LXMF, Sideband, NomadNet) already share the same transport layer. The question isn't *how to bridge* but *how to coexist gracefully and offer an upgrade path*.

## The Relationship

```mermaid
flowchart TB
    subgraph transport["Shared Transport — Reticulum Wire Protocol"]
        direction LR
        RE["Pure Reticulum\n(LXMF, Sideband,\nNomadNet)\n**L0**"]
        L1["Mehr L1\n(Relay +\neconomy)\n**L1**"]
        L2["Mehr L2\n(Full\nstack)\n**L2**"]
    end
```

All three participation levels share the same Reticulum transport. An L0 node running Sideband and an L2 node running full Mehr relay packets for each other on the same mesh. They use the same encryption, the same announce mechanism, the same link establishment protocol.

Mehr's economic extensions ([CompactPathCost](../protocol/network-protocol#mehr-extension-compact-path-cost), VRF lottery proofs, settlement records) are carried as opaque payload in Reticulum's announce DATA field. Pure Reticulum nodes forward these announces without parsing the Mehr-specific bytes — they just see more data in the announce and relay it.

## LXMF Compatibility

[LXMF](https://github.com/markqvist/LXMF) (Lightweight Extensible Message Format) is Reticulum's message layer. Sideband and NomadNet use LXMF for messaging.

### How Messages Coexist

LXMF messages and Mehr messages both travel over Reticulum links. They are distinguished by their Reticulum **aspect** — the application identifier in the destination hash:

| Message Type | Reticulum Aspect | Handling |
|-------------|-----------------|----------|
| LXMF message | `lxmf.delivery` | Processed by LXMF-aware applications |
| Mehr DataObject | `mehr.store` | Processed by Mehr L2 nodes |
| Mehr Pub notification | `mehr.pub` | Processed by Mehr L2 nodes |

A node running both Sideband and Mehr L2 handles both aspects simultaneously. LXMF messages arrive through the LXMF stack. Mehr messages arrive through the Mehr stack. Same radio, same transport, different application handlers.

### LXMF-to-Mehr Message Translation

For users who want Sideband contacts to receive Mehr messages (or vice versa), a bridge service translates between the two:

```
LXMF Message                    Mehr Message
┌──────────────────┐            ┌──────────────────┐
│ source_hash      │   Bridge   │ sender: NodeID   │
│ destination_hash │  ────────→ │ recipient: NodeID│
│ content          │            │ DataObject       │
│ timestamp        │            │   (immutable)    │
│ title (optional) │            │ MHR-Pub notify   │
│ fields (dict)    │            │ payment: channel │
└──────────────────┘            └──────────────────┘
```

**Translation details**:

1. **Identity**: LXMF uses Reticulum destination hashes (derived from Ed25519 keys). Mehr uses the same Ed25519 keys and the same destination hash derivation. **Identity is natively shared** — no attestation needed if the user controls the same key on both sides.

2. **Content**: LXMF `content` field → Mehr immutable DataObject (stored in [MHR-Store](../services/mhr-store)). LXMF `fields` dict → Mehr DataObject metadata.

3. **Delivery**: LXMF delivery → Mehr [MHR-Pub](../services/mhr-pub) notification. The bridge publishes a Pub notification that triggers Mehr-side delivery.

4. **Encryption**: Both use the same underlying crypto (X25519 ECDH + symmetric cipher). LXMF link encryption maps directly to Reticulum link encryption, which is the same link encryption Mehr uses. No re-encryption needed at the bridge for transport-layer security. E2E encryption differs: LXMF uses Reticulum's built-in E2E; Mehr uses its own envelope format. The bridge re-wraps E2E content.

### Shared Identity (Zero-Cost Bridge)

Because LXMF and Mehr both derive identity from Ed25519 keypairs via Reticulum's destination hash:

- A user with the same keypair is **the same identity** on both LXMF and Mehr
- No attestation, no bridge registration, no identity mapping
- Sideband shows your Reticulum hash. Mehr shows the same hash.
- If Alice's Sideband identity is `a1b2c3d4...`, her Mehr identity is `a1b2c3d4...`

This means a "bridge" for existing Reticulum users is really just **running Mehr alongside their existing Reticulum applications**. Same key, same identity, additional capabilities.

> **Key Insight**
LXMF and Mehr share the same Ed25519 keypair and Reticulum destination hash derivation. A user’s Sideband identity and Mehr identity are cryptographically identical — no attestation, registration, or bridge configuration needed.

## Sideband Integration

[Sideband](https://github.com/markqvist/Sideband) is the most popular Reticulum messaging application — available on Android, Linux, and macOS.

### Coexistence Path

Sideband users adopt Mehr in stages:

**Stage 1: Shared Transport (Today)**

- Sideband and Mehr daemon run on the same device
- Both register with the local Reticulum instance
- Both use the same radio interfaces
- No interaction between the two — they share transport, not data

**Stage 2: Unified Contact List**

- Mehr reads Sideband's known destinations (same key format)
- Contacts appear in both applications
- Messages between two Mehr users go through Mehr (with economics, storage)
- Messages to Sideband-only users go through LXMF (free, no economics)

**Stage 3: Mehr-Enhanced Sideband**

- Sideband plugin or fork that optionally uses Mehr services:
  - **MHR-Store** for persistent message storage (Sideband messages are ephemeral)
  - **MHR-Pub** for group notifications (Sideband has limited group support)
  - **MHR-DHT** for contact discovery beyond direct announces
  - **Payment channels** for relay incentives (Sideband relays are volunteer-only)

**Stage 4: Sideband as Mehr Frontend**

- Sideband evolves into a Mehr client application
- All messaging uses Mehr primitives
- LXMF compatibility maintained as a legacy bridge
- The Sideband UI, which is already familiar to the Reticulum community, becomes a Mehr frontend

This path is **not a hostile takeover**. Each stage is optional. Many Sideband users may never go past Stage 1, and that's fine — they're already contributing to the transport layer by running Reticulum nodes.

## NomadNet Integration

[NomadNet](https://github.com/markqvist/NomadNet) is a terminal-based communication platform on Reticulum — messaging, file sharing, and microblogging via a text-based interface.

### Bridgeable Features

| NomadNet Feature | Mehr Equivalent | Bridge Approach |
|-----------------|----------------|-----------------|
| Pages (microblog) | [Social posts](../applications/social) | NomadNet page → Mehr `PostEnvelope` DataObject |
| File hosting | [MHR-Store](../services/mhr-store) | NomadNet hosted files → Mehr DataObjects with storage agreements |
| Messaging | [Messaging](../applications/messaging) | LXMF translation (same as above) |
| Node directory | [MHR-DHT](../services/mhr-dht) | NomadNet announces → Mehr DHT entries |

NomadNet's page system (Markdown-like `.mu` format) maps naturally to Mehr's social layer. A bridge can publish NomadNet pages as Mehr social posts and vice versa — expanding the audience for NomadNet content to Mehr users.

## RNode Hardware

[RNode](https://reticulum.network/manual/hardware.html) is the reference hardware for Reticulum — a LoRa transceiver firmware that runs on common ESP32+LoRa boards. RNodes are used by both Reticulum/Sideband and can serve Mehr.

### RNode as Mehr Transport

An RNode running its standard firmware serves as a radio modem for a host running Reticulum. If that host also runs Mehr, the RNode provides LoRa transport for Mehr traffic — no RNode firmware changes needed.

```
[Raspberry Pi]
  ├── Reticulum daemon
  ├── Mehr L2 daemon
  └── USB serial ──→ [RNode (ESP32 + LoRa)]
                          │
                     LoRa radio
                          │
                     [Other nodes]
```

### RNode with Mehr L1

For standalone operation (no host computer), an RNode can be extended with Mehr L1 firmware:

- Parse CompactPathCost from announces
- Run VRF relay lottery
- Maintain minimal payment channel state (200 bytes per channel)
- Report to a nearby L2 node for settlement

This turns a $15-25 RNode into a self-contained, revenue-generating Mehr relay. The same hardware that today serves as a volunteer Reticulum modem becomes an economically incentivized node.

## Protocol Compatibility Details

### Announce Format

Reticulum announces carry a DATA field that applications can populate. Mehr uses this field for CompactPathCost and capability advertisements:

```
Reticulum Announce:
  ├── destination_hash (16 bytes)
  ├── public_key (32 bytes)
  ├── app_data (variable)       ← Mehr extensions go here
  │     ├── CompactPathCost (6 bytes)
  │     ├── capability_bitfield (2 bytes)
  │     ├── cost_tier (1 byte)
  │     └── load (1 byte)
  └── signature (64 bytes)

Total Mehr overhead in announce: ~10 bytes
```

Pure Reticulum nodes see the `app_data` field but don't parse it — they forward the announce with the field intact. Mehr-aware nodes parse it and use it for routing and marketplace decisions.

**Backward compatibility**: If a pure Reticulum application sets its own `app_data`, Mehr nodes ignore unrecognized formats. The `0x4D48` ("MH") magic prefix identifies Mehr-specific app_data.

### Link Encryption

Reticulum provides link-layer encryption:
- X25519 ECDH key exchange
- AES-256-CBC (current) or ChaCha20-Poly1305 (future)
- Counter-based nonces
- Automatic key rotation

Mehr uses this encryption directly — no additional link-layer crypto. E2E encryption is an additional layer on top, but transport security is inherited from Reticulum.

### Gossip Bandwidth

Mehr's [gossip protocol](../protocol/network-protocol#gossip-protocol) operates within a strict bandwidth budget (10% of link capacity, tiered). On a shared Reticulum transport, this budget covers both Mehr gossip and Reticulum's native announce propagation:

| Traffic Type | Budget Allocation |
|-------------|-------------------|
| Reticulum announces (routing) | Part of Tier 1 (3%) |
| Mehr CompactPathCost gossip | Part of Tier 2 (3%) |
| Mehr service/DHT/pub gossip | Tier 3 (2%) |
| Mehr social/trust gossip | Tier 4 (2%) |
| LXMF message delivery | User traffic (90% budget) |
| Mehr data transfer | User traffic (90% budget) |

LXMF messages and Mehr data transfers share the 90% user traffic budget. On constrained LoRa links, Mehr's priority queuing ensures high-priority messages (both LXMF and Mehr) are delivered first.

## Community Considerations

The Reticulum community values simplicity, volunteer operation, and freedom from economic coercion. Mehr's economic layer could be seen as contradicting these values. The coexistence model addresses this:

1. **Opt-in economics**: No Reticulum user is forced to participate in Mehr economics. Running a pure Reticulum node alongside Mehr nodes works perfectly — L0 participation with zero economic overhead.

2. **Free between friends**: Mehr's [trust-based free tier](../economics/trust-neighborhoods) means a local Reticulum community can adopt Mehr and still relay for free among trusted peers. Economics only activate when traffic crosses trust boundaries.

3. **No protocol tax**: Mehr doesn't charge existing Reticulum traffic. LXMF messages between two Sideband users continue to flow for free even on a link shared with Mehr traffic.

4. **Volunteer operation remains viable**: Operators who want to relay without earning MHR simply run as L0 (or L1 with lottery earnings they ignore). The protocol doesn't require economic participation.

5. **Upstream contribution**: Mehr development benefits Reticulum — performance improvements, hardware support, and testing capacity flow back to the transport layer.

The goal is to be a **superset**, not a replacement. Reticulum is excellent at what it does. Mehr adds what it doesn't do (economics, storage, compute) without diminishing what it already provides.
> **Trade-off**
Mehr’s economic layer risks alienating the Reticulum community, which values volunteer operation and simplicity. The coexistence model mitigates this: economics are strictly opt-in, LXMF traffic remains free, and L0 participation requires zero economic overhead.


---

### Scuttlebutt Bridge
<!-- Source: docs/interoperability/scuttlebutt.md -->

# Scuttlebutt Bridge

[Scuttlebutt](https://scuttlebutt.nz/) (SSB) is the protocol most philosophically aligned with Mehr. Both are gossip-based, offline-first, community-centric, and built on Ed25519 cryptographic identity. Both reject global consensus in favor of local-first operation. Both believe social networks should emerge from relationships, not platforms.

The differences are complementary: SSB has a mature social ecosystem with thousands of users. Mehr has economic incentives, radio mesh transport, and a capability marketplace. A bridge between them lets each side benefit from what the other provides.

> **Key Insight**
SSB and Mehr are the two most aligned protocols in the decentralized ecosystem: both are gossip-based, offline-first, Ed25519-identity, and reject global consensus. Their differences — SSB has the social network, Mehr has the economic layer — are complementary rather than competing.

## Protocol Alignment

| Property | SSB | Mehr | Compatibility |
|----------|-----|------|---------------|
| **Identity** | Ed25519 keypair → feed ID (`@...=.ed25519`) | Ed25519 keypair → destination hash | High — same key type, different derivation |
| **Data model** | Append-only signed log per identity | Immutable/mutable DataObjects in DHT | Moderate — log entries map to DataObjects |
| **Replication** | Social graph-based gossip (friends + friends-of-friends) | DHT + Pub/Sub + trust neighborhoods | Moderate — different scoping models |
| **Discovery** | LAN broadcast, pub servers, room servers | [Concentric ring discovery](../marketplace/discovery), DHT lookup | Low friction — bridge advertises in marketplace |
| **Encryption** | Secret Handshake (SHS) for connections, private-box for DMs | X25519 ECDH link encryption, E2E per-message | Compatible — both use Curve25519-derived keys |
| **Offline tolerance** | Excellent — feeds are self-contained | Excellent — store-and-forward, partition-tolerant | Native alignment |
| **Economics** | None — volunteer pubs | MHR token, VRF relay lottery, CRDT ledger | Bridge handles economic boundary |

## Identity Bridge

SSB and Mehr both use Ed25519 — but derive identities differently.

**SSB**: Feed ID = base64-encoded Ed25519 public key, prefixed with `@` and suffixed with `.ed25519`
```
@pAhDFHPLFCKPAlGrOAO9Pn5GBlsVsCj6EZLbT8FMCVU=.ed25519
```

**Mehr**: Destination hash = truncated Blake3 hash of the Ed25519 public key (16 bytes)
```
a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6
```

### Key Reuse vs. Key Attestation

Two approaches:

**Option A: Same Keypair (Recommended for new users)**

A user generates one Ed25519 keypair and uses it for both SSB and Mehr. Their SSB feed ID and Mehr destination hash both derive from the same public key. The bridge can verify this cryptographically:

```
Verify same key:
    ssb_pubkey = base64_decode(ssb_feed_id)
    mehr_destination = Blake3(mehr_pubkey)[0:16]
    assert ssb_pubkey == mehr_pubkey  // same key → same person
```

No attestation needed. The bridge just verifies the math.

**Option B: Separate Keypairs (Existing SSB users)**

Users who already have an SSB identity create a separate Mehr keypair and link them via attestation:

```
SSBBridgeAttestation {
    mehr_pubkey: Ed25519PublicKey,
    ssb_feed_id: String,              // "@...=.ed25519"
    proof_ssb: SSBMessage,            // signed SSB message containing mehr_pubkey
    proof_mehr: Ed25519Signature,     // Mehr key signs ssb_feed_id
    bridge_node: NodeID,
    timestamp: LamportTimestamp,
}
```

Both keys sign the other's identifier — bidirectional proof of ownership. The bridge stores these attestations and serves them to either network on request.

## Data Model Translation

### SSB Feeds → Mehr DataObjects

SSB organizes data as append-only logs. Each user has one feed — a sequence of signed JSON messages. Mehr organizes data as content-addressed DataObjects stored in a DHT.

```
SSB Feed Message                    Mehr DataObject
┌──────────────────────┐            ┌──────────────────────┐
│ previous: %hash      │            │ hash: Blake3(content) │
│ author: @feedID      │            │ owner: NodeID         │
│ sequence: 42         │            │ type: Immutable       │
│ timestamp: 1706...   │            │ created: Lamport(ts)  │
│ content: {           │   Bridge   │ data: {               │
│   type: "post",      │  ────────→ │   ssb_type: "post",   │
│   text: "Hello SSB", │            │   text: "Hello SSB",  │
│   channel: "#mehr"   │            │   scope: "topic:mehr", │
│ }                    │            │   ssb_sequence: 42,   │
│ signature: ...       │            │   ssb_previous: %hash │
│                      │            │ }                     │
└──────────────────────┘            └──────────────────────┘
```

**Translation rules**:

| SSB Field | Mehr Field | Notes |
|-----------|-----------|-------|
| `author` | `owner` | Mapped via identity bridge |
| `sequence` | Stored in metadata | Preserves SSB ordering |
| `previous` | Stored in metadata | Preserves SSB feed chain |
| `content.type` | DataObject metadata | `post`, `contact`, `vote`, `about` |
| `content.text` | DataObject data | Unmodified content |
| `content.channel` | [Scope](../economics/trust-neighborhoods) | `#channel` → `topic:channel` |
| `content.mentions` | DataObject references | Hash references to other objects |
| `signature` | Stored alongside | SSB signature preserved for verification |

**Key invariant**: The bridge preserves SSB's append-only property. Each SSB message becomes an immutable Mehr DataObject. The SSB `previous` hash chain is stored in metadata so the full feed can be reconstructed from Mehr's DHT.

### Mehr DataObjects → SSB Messages

Going the other direction, Mehr social posts become SSB feed messages:

1. Bridge receives a Mehr [social post](../applications/social) (`PostEnvelope` DataObject)
2. Bridge translates it into an SSB message on its own feed (bridge identity)
3. SSB message `content.text` includes the post text
4. SSB message `content.mentions` includes a reference to the Mehr author
5. SSB users see: `[mehr:a1b2c3d4] posted: "Hello from Mehr mesh"`

**Attribution**: The bridge's SSB feed clearly attributes content to Mehr authors. SSB users can follow the bridge feed to see all Mehr-bridged content, or use clients that display bridged content inline.

### Private Messages

SSB uses `private-box` (asymmetric encryption for up to 7 recipients). Mehr uses per-recipient E2E encryption.

**Bridge handling**: Private messages require re-encryption at the bridge. The bridge decrypts from one format and re-encrypts in the other. This means:

> **Threat**
Private messages crossing the SSB–Mehr bridge require re-encryption. The bridge operator can read plaintext during translation. For sensitive conversations, both parties should use the same protocol or establish a sealed E2E channel through the bridge.

- The bridge operator can read private messages that cross the bridge
- Users are warned about this trust requirement
- For truly sensitive conversations, both parties should use the same protocol
- Bridge can optionally support a **sealed mode** where both parties establish an E2E channel through the bridge using Mehr's native E2E, with the bridge only translating the routing

## Gossip Model Translation

SSB and Mehr both use gossip — but scope it differently.

### SSB Gossip

SSB replicates feeds based on social graph:
- **Hops = 1**: Replicate feeds of accounts you follow
- **Hops = 2**: Replicate feeds of accounts your follows follow
- **Hops = 3**: Broader network awareness (optional, bandwidth-heavy)

Each SSB node has a different view of the network, determined by who it follows.

### Mehr Gossip

Mehr gossips based on [concentric rings](../marketplace/discovery):
- **Ring 0**: Direct neighbors (full detail)
- **Ring 1**: 2-3 hops (summarized capabilities)
- **Ring 2**: Trust neighborhood (periodic)
- **Ring 3**: Beyond neighborhood (on-demand)

### Bridge Translation

The bridge reconciles these models:

```
SSB social graph    →  Mehr trust graph
─────────────────     ─────────────────
follow(@alice)     →  trust_peer(alice_mehr_id)
block(@bob)        →  (no Mehr equivalent — bridge filters)
hops=2 replication →  Ring 1 gossip scope
pub server         →  Bridge node (L2 service)
```

**SSB follows → Mehr trust**: When an SSB user follows an account that has a Mehr bridge attestation, the bridge can optionally add them to the Mehr trust graph. This is **not automatic** — it requires user consent, since Mehr trust relationships have economic implications (free relay).

**SSB channels → Mehr scopes**: SSB channels (`#topic`) map to Mehr topic scopes (`topic:topic`). The bridge subscribes to relevant SSB channels and publishes/relays content to the corresponding Mehr scopes.

## Bridge Architecture

```
┌─────────────────────────────────────────┐
│            SSB-Mehr Bridge Node          │
│                                          │
│  ┌──────────────┐  ┌──────────────────┐ │
│  │  SSB Stack    │  │   Mehr L2 Stack  │ │
│  │              │  │                  │ │
│  │ • ssb-db2    │  │ • MHR-Store     │ │
│  │ • ssb-conn   │  │ • MHR-DHT      │ │
│  │ • ssb-friends│  │ • MHR-Pub      │ │
│  │ • ssb-blobs  │  │ • Marketplace   │ │
│  └──────┬───────┘  └────────┬─────────┘ │
│         │                    │           │
│  ┌──────┴────────────────────┴─────────┐ │
│  │        Translation Layer             │ │
│  │                                      │ │
│  │  • Identity attestation registry    │ │
│  │  • Feed ↔ DataObject translator     │ │
│  │  • Channel ↔ Scope mapper           │ │
│  │  • Blob ↔ DataObject mapper         │ │
│  │  • Private message re-encryptor     │ │
│  └──────────────────────────────────────┘ │
│                                          │
│  Advertised capability:                  │
│    bridge.protocols: [SSB]               │
│    bridge.ssb_feeds_indexed: 1,234       │
│    bridge.ssb_channels: [mehr, ...]      │
└─────────────────────────────────────────┘
```

**Hardware requirements**: Raspberry Pi 4 or equivalent. SSB's database (ssb-db2) and Mehr's full L2 stack both need storage and memory. Minimum 2 GB RAM, 32 GB storage.

**Not suitable for ESP32**: SSB requires JavaScript (ssb-db2 is Node.js) or Go (go-ssb). Neither runs on microcontrollers. The bridge is an L2 service, not a constrained-device operation.

## SSB Blob ↔ Mehr DataObject

SSB stores binary content (images, files) as blobs — content-addressed by SHA-256 hash. Mehr stores content as DataObjects — content-addressed by Blake3 hash.

```
SSB Blob                          Mehr DataObject
────────                          ───────────────
&hash.sha256  ←── rehash ──→  Blake3(content)
```

**Bridge approach**:

1. SSB blob arrives at bridge
2. Bridge stores it as a Mehr immutable DataObject (Blake3-addressed)
3. Bridge maintains a mapping: `sha256_hash → blake3_hash`
4. Mehr users request by Blake3 hash; SSB users request by SHA-256 hash
5. Bridge serves both, translating the hash reference

**Bandwidth consideration**: Blobs (images, files) are large. On constrained Mehr links (LoRa), the bridge respects Mehr's `min_bandwidth` field — large blobs are only replicated to nodes with sufficient bandwidth. Metadata (post text, references) flows everywhere; blobs flow only where bandwidth permits.

## Practical Scenarios

### Alice (SSB) posts to #mesh channel

1. Alice publishes on SSB: `{ type: "post", text: "Great LoRa range today", channel: "#mesh" }`
2. Bridge replicates Alice's feed (follows Alice on SSB)
3. Bridge creates Mehr DataObject with scope `topic:mesh`
4. Mehr users subscribed to `topic:mesh` via [MHR-Pub](../services/mhr-pub) receive the post
5. Mehr users see Alice's post attributed to her SSB identity (via bridge attestation)

### Bob (Mehr) sends a DM to Alice (SSB)

1. Bob looks up Alice's Mehr identity via bridge attestation in [MHR-DHT](../services/mhr-dht)
2. Bob sends E2E encrypted message to bridge, addressed to Alice's Mehr-side identity
3. Bridge decrypts, re-encrypts with Alice's SSB public key (private-box)
4. Bridge publishes as SSB private message on its own feed, encrypted for Alice
5. Alice's SSB client decrypts and displays the message

### Community mesh (Mehr) shares content with SSB Scuttleverse

1. Community operates a local Mehr mesh with LoRa relays
2. Bridge node has both LoRa (Mehr mesh) and internet (SSB pubs) connectivity
3. Local posts tagged `topic:community-name` are translated to SSB channel `#community-name`
4. Global SSB users discover the community's content through the bridge's SSB feed
5. SSB replies flow back through the bridge to the local Mehr mesh

## Why SSB Before Other Social Protocols

| Factor | SSB | Nostr | Mastodon/ActivityPub |
|--------|-----|-------|---------------------|
| **Offline-first** | Native — designed for it | No — requires relay connectivity | No — requires server |
| **Gossip-based** | Yes — social graph replication | No — relay-based | No — HTTP federation |
| **Ed25519 identity** | Yes | No (secp256k1) | No (server-issued) |
| **No server dependency** | Yes (pubs optional) | Relays required | Server required |
| **Community ethos** | Aligned — off-grid, community, privacy | Partially aligned | Different philosophy |
| **Partition tolerant** | Excellent | Moderate | Poor |

SSB is the closest match to Mehr's design philosophy. Bridging SSB first creates the strongest ideological and technical alignment, and the SSB community is most likely to appreciate what Mehr adds (economic incentives for the infrastructure they currently run as volunteers).

---

### Matrix Bridge
<!-- Source: docs/interoperability/matrix.md -->

# Matrix Bridge

[Matrix](https://matrix.org/) is the strategic bridge target for Mehr. Not because Matrix is the most philosophically aligned protocol — [Scuttlebutt](scuttlebutt) holds that distinction — but because Matrix is the **most connected**. Matrix's federation protocol already bridges to IRC, Slack, Discord, Signal, Telegram, WhatsApp, and dozens of other platforms. A single Mehr↔Matrix bridge gives Mehr transitive access to all of them.

```mermaid
flowchart LR
    MM["Mehr Mesh"] --> MMB["Mehr↔Matrix\nBridge"]
    MMB --> MN["Matrix\nNetwork"]
    MN --> MPB["Matrix↔Platform\nBridges"]
    MPB --> Slack
    MPB --> Discord
    MPB --> IRC
    MPB --> Signal
    MPB --> Telegram
```

One bridge. Dozens of reachable networks. This is the multiplier effect.

> **Key Insight**
Matrix is the strategic bridge target because of its transitive connectivity. A single Mehr↔Matrix bridge gives Mehr access to IRC, Slack, Discord, Signal, Telegram, and every other platform Matrix already bridges to — one integration, dozens of networks.

## Why Matrix

| Property | Value for Mehr |
|----------|---------------|
| **Open standard** | Fully specified ([spec.matrix.org](https://spec.matrix.org/)), no vendor lock-in |
| **Federation** | No central server — homeservers federate like email. Bridge runs its own homeserver. |
| **E2E encryption** | Olm/Megolm (Double Ratchet-based). Widely deployed. |
| **Rich ecosystem** | Element, FluffyChat, Nheko, Cinny — dozens of clients across all platforms |
| **Bridge infrastructure** | Mature bridge ecosystem via [matrix-appservice-bridge](https://github.com/matrix-org/matrix-appservice-bridge) |
| **Room model** | Rooms are the unit of communication — maps to Mehr's pub/sub topics |
| **DAG-based state** | Room state is a directed acyclic graph (not a linear chain). Handles concurrent events. |
| **Active development** | Matrix 2.0 (sliding sync, OIDC, VoIP) actively evolving |

**What Matrix lacks that Mehr provides**: Offline operation, mesh radio transport, constrained-device support, economic incentives, partition tolerance. Matrix assumes always-on internet — a Mehr bridge extends Matrix's reach to off-grid communities.

## Architecture

The Mehr↔Matrix bridge is an L2 Mehr node that also runs a Matrix homeserver (or connects to one as an application service).

### Deployment Options

**Option A: Bridge with Embedded Homeserver (Recommended)**

The bridge runs its own lightweight Matrix homeserver (e.g., [Conduit](https://conduit.rs/) — written in Rust, low resource usage):

```
┌──────────────────────────────────────────┐
│          Mehr↔Matrix Bridge Node          │
│                                           │
│  ┌──────────────┐  ┌───────────────────┐ │
│  │  Mehr L2      │  │  Conduit          │ │
│  │  Stack        │  │  (Matrix HS)      │ │
│  │               │  │                   │ │
│  │  • MHR-Store  │  │  • Federation     │ │
│  │  • MHR-Pub    │  │  • Room state     │ │
│  │  • MHR-DHT    │  │  • E2E (Megolm)   │ │
│  │  • Marketplace│  │  • Media store    │ │
│  └──────┬────────┘  └────────┬──────────┘ │
│         │                     │            │
│  ┌──────┴─────────────────────┴──────────┐ │
│  │         Translation Layer              │ │
│  │                                        │ │
│  │  • Room ↔ Topic mapper                │ │
│  │  • User ↔ Identity attestation        │ │
│  │  • Message format translator          │ │
│  │  • Media ↔ DataObject mapper          │ │
│  │  • E2E re-encryption handler          │ │
│  └────────────────────────────────────────┘ │
│                                           │
│  Internet ←→ Matrix federation            │
│  Mesh     ←→ Mehr network                 │
└──────────────────────────────────────────┘
```

**Advantage**: Self-contained. The bridge controls its own homeserver, can operate in restricted federation mode, and doesn't depend on external homeserver operators.

**Option B: Application Service on Existing Homeserver**

The bridge registers as a [Matrix application service](https://spec.matrix.org/latest/application-service-api/) on an existing homeserver (e.g., a community Synapse/Dendrite instance):

```
[Existing Homeserver] ←─ appservice ─→ [Mehr Bridge] ←─ mesh ─→ [Mehr Network]
```

**Advantage**: Uses existing homeserver infrastructure. Community that already runs a Matrix server adds Mehr connectivity with just the bridge component.

**Disadvantage**: Depends on external homeserver operator. Bridge availability tied to homeserver uptime.

## Room ↔ Topic Mapping

Matrix rooms map to Mehr [MHR-Pub](../services/mhr-pub) topics and [scope subscriptions](../economics/trust-neighborhoods).

### Mapping Rules

```
Matrix Room                        Mehr Equivalent
───────────                        ───────────────
#general:example.org          →    topic:general
#portland-mesh:example.org    →    geo:us/oregon/portland
#dev:mehr.network             →    topic:mehr/dev
!roomid:server (unnamed)      →    topic:bridge/<room_id_hash>
```

| Matrix Concept | Mehr Concept | Translation |
|---------------|-------------|-------------|
| Room | MHR-Pub topic + scope | Bridge creates/joins corresponding topic |
| Room membership | Scope subscription | Join room → subscribe to topic |
| Message (m.room.message) | DataObject + Pub notification | Content stored in MHR-Store, notification via MHR-Pub |
| Room state events | Mutable DataObjects | Room name, topic, avatar → mutable DataObjects in DHT |
| Reactions | DataObject references | Reaction → small DataObject referencing original |
| Threads | DataObject reply chain | Thread root → DataObject; replies reference parent |
| Media (images, files) | DataObjects with MIME metadata | Stored in MHR-Store; served to Matrix via homeserver media API |

### Bidirectional Sync

**Matrix → Mehr**:

1. Bridge's homeserver receives federated events for bridged rooms
2. Translation layer converts each event to Mehr DataObjects
3. DataObjects stored in [MHR-Store](../services/mhr-store)
4. [MHR-Pub](../services/mhr-pub) notifies Mehr subscribers of the new content
5. Mehr users see Matrix messages attributed to Matrix identities (via bridge attestation)

**Mehr → Matrix**:

1. Bridge subscribes to relevant Mehr topics via MHR-Pub
2. New Mehr posts trigger translation to Matrix events
3. Bridge's homeserver sends events to bridged rooms via federation
4. Matrix users see Mehr messages attributed to virtual Matrix users (e.g., `@mehr_a1b2c3d4:bridge.mehr.network`)

### Virtual Users

The bridge creates Matrix "ghost" users for each Mehr identity that sends messages through the bridge:

```
Mehr identity: a1b2c3d4e5f6a7b8
Matrix ghost:  @mehr_a1b2c3d4:bridge.mehr.network
Display name:  "alice [Mehr]" (if Mehr profile has display name)
Avatar:        Bridged from Mehr profile (if available)
```

Matrix users can message these ghost users directly (DMs) or interact with them in rooms. The bridge translates all interactions back to Mehr.

## Identity

### Matrix → Mehr Attestation

A Matrix user wanting a Mehr identity:

1. Creates a Mehr Ed25519 keypair (or uses existing one)
2. Sends a verification message to the bridge bot in Matrix: `!mehr verify a1b2c3d4...`
3. Bridge sends a challenge to the Mehr identity
4. User signs the challenge with their Mehr key and responds
5. Bridge creates and stores the attestation:

```
MatrixBridgeAttestation {
    mehr_pubkey: Ed25519PublicKey,
    matrix_user_id: "@alice:example.org",
    bridge_node: NodeID,
    challenge: [u8; 32],
    mehr_signature: Ed25519Signature,    // Mehr key signs challenge
    matrix_proof: String,                // Matrix message ID containing Mehr pubkey
    timestamp: LamportTimestamp,
}
```

### Mehr → Matrix Attestation

A Mehr user wanting to be reachable from Matrix:

1. User connects to the bridge service on the Mehr network
2. Bridge provides a Matrix ghost user ID for their Mehr identity
3. User can optionally claim an existing Matrix account (via the challenge flow above)
4. Without claiming: messages from Mehr appear from the ghost user
5. With claiming: messages appear from their real Matrix account (bridge posts on their behalf via application service)

## Message Format Translation

### Text Messages

```
Matrix event (m.room.message):          Mehr DataObject:
{                                       {
  "type": "m.room.message",              hash: Blake3(content),
  "content": {                           owner: bridge_node_id,
    "msgtype": "m.text",                 type: Immutable,
    "body": "Hello from Matrix!",        data: {
    "format": "org.matrix.custom.html",    text: "Hello from Matrix!",
    "formatted_body": "<b>Hello</b>..."    format: "html",
  },                                       source: "matrix",
  "sender": "@alice:example.org",          sender_attestation: <hash>,
  "origin_server_ts": 1706000000000      },
}                                       }
```

**Format preservation**: Matrix supports HTML formatting. The bridge preserves this in Mehr DataObject metadata. Mehr clients that understand HTML render it; others display the plaintext `body` fallback.

### Media Messages

Matrix media (images, video, files) are uploaded to the homeserver's media repository. The bridge:

1. Downloads media from Matrix homeserver
2. Stores as Mehr immutable DataObject in [MHR-Store](../services/mhr-store)
3. Sets `min_bandwidth` based on file size:
   - Images below 50 KB: `min_bandwidth: 1200` (OK for LoRa)
   - Images 50 KB-1 MB: `min_bandwidth: 10000` (WiFi only)
   - Video/large files: `min_bandwidth: 100000` (broadband only)
4. Mehr clients receive metadata (filename, size, type) over any link; content downloads only when bandwidth permits

Going the other direction:

1. Mehr DataObject with media content arrives at bridge
2. Bridge uploads to its Matrix homeserver media repository
3. Bridge posts Matrix message with `mxc://` media URL
4. Matrix clients download media from the homeserver as usual

### Encrypted Messages (Megolm ↔ Mehr E2E)

Matrix E2E uses Megolm (a group ratchet). Mehr uses per-recipient E2E encryption. These are fundamentally different:

- **Megolm**: One outbound session key per sender per room. All room members decrypt with the same session key. Efficient for groups.
- **Mehr E2E**: Each message encrypted individually for each recipient's Ed25519 public key. More expensive for large groups but simpler.

**Bridge handling**:

For **E2E rooms**, the bridge must be a member of the room's Megolm session to decrypt messages. This means:

- Bridge is trusted with plaintext (same as any room member)
- Bridge decrypts from Megolm, re-encrypts with Mehr E2E for destination
- Users are warned: E2E guarantee is bridge-to-endpoint, not true end-to-end across the bridge

For **unencrypted rooms** (common for public channels), no re-encryption is needed — the bridge simply translates the message format.

> **Trade-off**
Matrix E2E (Megolm group ratchet) and Mehr E2E (per-recipient encryption) are fundamentally different. The bridge must be a Megolm session member to decrypt, making it a trust boundary. True end-to-end encryption across the bridge is not possible — only bridge-to-endpoint.

**Recommendation**: Use unencrypted Matrix rooms for public/community bridging. For private conversations that require E2E, both parties should be on the same protocol.

## Payment Flows

### Public Channel Bridging

Community rooms (public, unencrypted) bridged at the bridge operator's discretion:

```
Revenue model for bridge operator:
  • Mehr users pay per-message via service agreement (e.g., 10 μMHR/message)
  • Matrix users use the room for free (standard Matrix)
  • Bridge operator pays:
    - Homeserver hosting costs (fiat)
    - Mehr relay costs for outbound (MHR)
    - MHR-Store costs for caching Matrix content (MHR)
  • Bridge operator earns:
    - Service fees from Mehr users
    - Optionally: MHR relay rewards for hosting a well-connected node
```

### Direct Message Bridging

DMs between a Mehr user and a Matrix user:

```
Mehr user → Bridge:
  Pays per-message service fee (marketplace agreement)
  Bridge handles Matrix delivery

Matrix user → Bridge:
  Free (standard Matrix)
  Bridge pays Mehr relay costs for delivery to mesh
  Bridge recoups via:
    - Inbound delivery fee charged to Mehr recipient
    - Or subsidized as part of public bridge operation
    - Or donation-supported
```

### Self-Hosting Economics

A community running its own bridge:

- Operate a Raspberry Pi as both Matrix homeserver (Conduit) and Mehr L2 node
- Bridge their community Matrix room to their local Mehr mesh
- **Zero external cost** if all parties are in the community's [trust neighborhood](../economics/trust-neighborhoods) (free relay)
- Internet connectivity only needed for Matrix federation to the wider Matrix network
- If internet goes down, local Mehr mesh continues; Matrix bridge resumes when connectivity returns

## Offline and Partition Behavior

Matrix assumes persistent internet. Mehr doesn't. The bridge handles the mismatch:

### Bridge Online (Normal Operation)

- Messages flow bidirectionally in near real-time
- Matrix federation delivers to/from the bridge homeserver
- Mehr pub/sub delivers to/from mesh subscribers
- Latency: Matrix-side ~1s (internet), Mehr-side 2-30s (depending on transport)

### Bridge Offline (Internet Down, Mesh Up)

- Matrix federation stops — no new messages from Matrix network
- Mehr mesh continues operating independently
- Local Mehr users still communicate with each other
- Bridge queues outbound Matrix messages for delivery when internet returns
- On reconnection: queued messages sent, missed Matrix messages synced via federation backfill

### Bridge Partitioned (Mesh Splits)

- Mehr mesh partition: each partition's bridge node continues operating independently
- If a room is bridged by multiple bridge nodes in different partitions, each operates independently
- On partition heal: Mehr's CRDT-based state converges; duplicate messages are deduplicated by content hash

### Extended Offline

If the bridge is offline for an extended period:

- Matrix: Federation handles backfill. Bridge catches up on all missed events when it reconnects (Matrix homeservers cache events for offline servers).
- Mehr: [MHR-Store](../services/mhr-store) preserves messages. Bridge retrieves missed Mehr posts from the DHT on reconnection.

No messages are lost in either direction, assuming storage nodes on both sides remain operational during the outage.

## Deployment Guide

### Minimum Hardware

| Component | Specification |
|-----------|--------------|
| **Device** | Raspberry Pi 4 (2 GB RAM minimum, 4 GB recommended) |
| **Storage** | 32 GB SD card (64 GB for active communities) |
| **Network** | Internet connectivity (WiFi or Ethernet) + Mehr mesh interface |
| **Mehr interface** | USB-connected RNode (LoRa) or WiFi (for internet-only Mehr) |

### Software Stack

```
┌─────────────────────────────┐
│  Conduit (Matrix homeserver) │  Port 8448 (federation)
│  Rust, ~50 MB RAM           │  Port 443 (client API)
├─────────────────────────────┤
│  Mehr L2 daemon             │  Reticulum transport
│  Rust, ~30 MB RAM           │
├─────────────────────────────┤
│  Bridge service              │  Connects both stacks
│  Rust, ~20 MB RAM           │
└─────────────────────────────┘
  Total: ~100 MB RAM + OS
```

### Configuration

```
# bridge.toml

[matrix]
homeserver = "bridge.mehr.network"    # federation domain
port = 8448
registration_enabled = false          # ghost users only, no public registration

[mehr]
node_identity = "path/to/keyfile"
trust_config = "path/to/trust.toml"
marketplace_fee = 10                  # μMHR per bridged message

[bridge]
rooms = [
    { matrix = "#community:example.org", mehr_scope = "topic:community" },
    { matrix = "#portland:example.org", mehr_scope = "geo:us/oregon/portland" },
]
media_max_size = 5_000_000            # 5 MB max media bridge
media_min_bandwidth = 10000           # only bridge media to WiFi+ links
```

## Comparison with Existing Matrix Bridges

Matrix already has bridges to many platforms. How does a Mehr bridge compare?

| Aspect | Typical Matrix Bridge | Mehr↔Matrix Bridge |
|--------|----------------------|-------------------|
| **Transport** | Internet-only (HTTP API) | Mesh + internet (LoRa, WiFi, cellular) |
| **Availability** | Requires both sides online | Mehr side tolerates offline, store-and-forward |
| **Economics** | Free (bridge operator absorbs costs) | Service marketplace — bridge earns MHR |
| **Identity** | Platform-specific (Slack ID, Discord tag) | Cryptographic (Ed25519 attestation) |
| **E2E** | Varies (some bridges break E2E) | Explicitly handled — users warned about trust boundary |
| **Decentralized** | Bridge is a single point of failure | Multiple bridge operators possible per room |
| **Partition tolerant** | No — internet outage breaks bridge | Yes — Mehr side continues, reconciles on reconnect |

The Mehr bridge is unique in that the bridged network (Mehr) can operate independently during internet outages. This makes it particularly valuable for communities in areas with unreliable connectivity — they get Matrix-world access when internet is available and local mesh communication when it's not.

---

### BitTorrent Bridge
<!-- Source: docs/interoperability/bittorrent.md -->

# BitTorrent Bridge

[BitTorrent](https://www.bittorrent.org/) is the world's most widely deployed content distribution protocol, with 10–25 million active DHT nodes at any given time. It shares a key property with Mehr: **content addressing** — a torrent's info hash uniquely identifies its content, just as Blake3 hashes identify DataObjects in [MHR-Store](../services/mhr-store).

A BitTorrent bridge brings the entire BitTorrent content library into the Mehr mesh. Mesh-only users — even those on LoRa with no internet — can request torrent content through the bridge. The bridge fetches it from the BitTorrent network, verifies piece hashes, stores it as DataObjects in MHR-Store, and serves it to mesh users through normal Mehr protocols.

## Protocol Alignment

| Property | BitTorrent | Mehr | Compatibility |
|----------|-----------|------|---------------|
| **Content addressing** | SHA-1 info hash (v1) or SHA-256 Merkle root (v2) | Blake3 DataObject hash | High — both content-addressed, different hash functions |
| **Identity** | Random 20-byte peer ID (no cryptographic binding) | Ed25519 keypair → destination hash | None — BitTorrent has no identity system |
| **DHT** | Mainline DHT: Kademlia over UDP, 160-bit XOR, k=8 | [MHR-DHT](../services/mhr-dht): Kademlia-style, k=3, XOR + cost weighting | Moderate — same algorithmic family, different parameters |
| **Transport** | uTP (UDP) or TCP, requires IP addresses | Transport-agnostic (LoRa, WiFi, etc.), no IP required | Low — bridge must proxy between IP and mesh addressing |
| **Mutable data (BEP-44)** | Ed25519-signed DHT entries, 1 KB max, monotonic sequence | Ed25519-signed IdentityClaims, NameBindings | High — same curve, same signing pattern |
| **Encryption** | MSE/PE: obfuscation only (RC4), not security | ChaCha20-Poly1305 E2E encryption | Bridge re-encrypts at boundary |
| **Piece verification** | SHA-1 per piece (v1), SHA-256 Merkle tree (v2) | Blake3 per DataObject | Bridge verifies BT hashes, then re-hashes as Blake3 |
| **Economics** | None — volunteer seeders | MHR token, payment channels | Bridge charges for bandwidth + storage |

## Bridge Architecture

The BitTorrent bridge is a **gateway node** (internet-connected) that participates in both networks simultaneously:

```mermaid
flowchart LR
    subgraph mesh["Mehr Mesh"]
        A["Mesh Node A"]
        B["Mesh Node B"]
        C["Mesh Node C"]
    end
    A --- BR["BT Bridge\n(L2)"]
    B --- BR
    C --- BR
    subgraph internet["Internet"]
        DHT["Mainline DHT\n(UDP)"]
        BTP["BitTorrent Peers\n(uTP)"]
        TR["Trackers\n(HTTP/UDP)"]
    end
    BR --- DHT
    BR --- BTP
    BR --- TR
```

The bridge runs a full BitTorrent client (DHT, peer wire protocol, tracker announcements) and a full Mehr L2 node (MHR-Store, MHR-DHT, marketplace). It translates between the two worlds.

## Content Flow: BitTorrent → Mehr

When a mesh user wants torrent content:

```
Request flow:

  1. User provides info hash (magnet link or torrent file)
     e.g., magnet:?xt=urn:btih:abc123...

  2. Bridge resolves the info hash:
     - Queries Mainline DHT for peers
     - Contacts trackers if specified
     - Downloads metadata via BEP-9 (ut_metadata extension)

  3. Bridge downloads the content:
     - Connects to BitTorrent peers via uTP or TCP
     - Downloads pieces, verifies SHA-1/SHA-256 piece hashes
     - Assembles complete files

  4. Bridge stores content in MHR-Store:
     - Chunks files into 4 KB DataObjects (Mehr's chunk size)
     - Hashes each chunk with Blake3
     - Creates a manifest DataObject linking chunks
     - Stores with configurable replication

  5. Bridge returns Blake3 root hash to the requesting user
     User fetches DataObjects from MHR-Store via normal Mehr protocols

  6. Bridge caches the content — subsequent mesh requests
     are served directly from MHR-Store, no re-download
```

### Content Verification

The bridge performs a **hash translation**: it verifies BitTorrent integrity (SHA-1/SHA-256 piece hashes) during download, then produces Blake3 hashes for Mehr storage. The user trusts the bridge to have performed this translation honestly.

To reduce trust requirements:

- **Multiple bridges**: If two independent bridges produce the same Blake3 root hash for the same info hash, the content is almost certainly correct. Clients can query multiple bridges and compare.
- **Bridge reputation**: Bridges build [reputation](../protocol/security#reputation) through the standard trust system. A bridge that serves corrupted content loses reputation and trust.
- **Torrent file forwarding**: The bridge can forward the original torrent metadata (piece hashes) to the user. The user can verify individual pieces against the original SHA-1/SHA-256 hashes if they want to cross-check the bridge's Blake3 hashing.

## Content Flow: Mehr → BitTorrent

The bridge can also seed Mehr content into the BitTorrent network:

```
Seeding flow:

  1. Mehr user publishes content to MHR-Store (Blake3-addressed DataObjects)
  2. User requests the bridge to seed the content on BitTorrent
  3. Bridge fetches DataObjects from MHR-Store, reassembles files
  4. Bridge creates a torrent file (computes SHA-1/SHA-256 piece hashes)
  5. Bridge announces to the Mainline DHT and begins seeding
  6. BitTorrent users can now download via magnet link or torrent file
  7. Bridge is paid by the Mehr user for outbound bandwidth via payment channels
```

This makes Mehr content available to the billions of devices running BitTorrent clients, without those clients needing to know anything about Mehr.

## Naming Integration

BitTorrent content can be given human-readable names via [MHR-Name](../services/mhr-name):

```
Name binding:
  my-distro@topic:linux → ContentHash(Blake3 root of Ubuntu ISO fetched via bridge)
  paper-collection@topic:science → ContentHash(Blake3 root of archive fetched via bridge)
```

A user can register a name pointing to the Blake3 hash of content originally sourced from BitTorrent. Other mesh users look up the name and fetch the content from MHR-Store — they never need to know it came from BitTorrent.

For content that updates (e.g., a regularly updated dataset), the bridge can use the torrent's BEP-46 mutable torrent feature: a mutable DHT entry (Ed25519-signed, monotonic sequence number) points to the latest info hash. The bridge maps this to an MHR-Name binding that updates when the mutable torrent updates.

## BEP-44 Alignment

BitTorrent's BEP-44 (arbitrary DHT data storage) uses the same cryptographic primitives as Mehr:

| BEP-44 | Mehr Equivalent |
|--------|----------------|
| Ed25519 public key (32 bytes) | Ed25519 public key (32 bytes) |
| Ed25519 signature (64 bytes) | Ed25519 signature (64 bytes) |
| Monotonic sequence number | Monotonic sequence number (NameBinding, Vouch, etc.) |
| Mutable DHT entry (1 KB max) | IdentityClaim, NameBinding (~122–465 bytes) |
| Immutable DHT entry (keyed by SHA-1 of value) | Immutable DataObject (keyed by Blake3 of content) |

A bridge can translate between BEP-44 mutable entries and Mehr NameBindings or IdentityClaims with minimal impedance. Both systems support the pattern: "signed record, monotonically increasing version, stored in a DHT."

## Economic Model

The bridge is a [capability marketplace](../marketplace/overview) service, discoverable and payable like any other:

| Cost | Who Pays | Mechanism |
|------|---------|-----------|
| BitTorrent download bandwidth | Bridge operator (internet costs) | Recouped from requester's service fee |
| Mesh-side storage (MHR-Store) | Requester | Standard [storage pricing](../services/mhr-store) |
| Mesh-side relay to requester | Requester | Standard [relay payment channels](../economics/payment-channels) |
| Bridge seeding (Mehr → BT) | Content owner | Pays bridge for outbound internet bandwidth |

The bridge advertises `Capability(bittorrent_bridge, ...)` and competes with other bridges on price, speed, and reliability. Popular torrents get cached in MHR-Store and served from the mesh — subsequent requests don't go through the bridge at all.

## Constraints and Limitations

**Internet required**: The bridge must have internet access to reach the BitTorrent network. Mesh-only nodes access BitTorrent content indirectly through the bridge.

**Latency**: Downloading a torrent takes time (seconds to minutes depending on swarm size and content). The bridge uses store-and-forward — the user's request is asynchronous. For popular content already cached in MHR-Store, latency is just mesh relay time.

**Bandwidth mismatch**: BitTorrent assumes broadband; Mehr supports LoRa at 500 bps. Large files (multi-GB) are practical only for mesh nodes on WiFi or better. The bridge can serve partial content (specific files from a multi-file torrent) to reduce bandwidth.

**No E2E encryption with BitTorrent peers**: BitTorrent's MSE/PE is obfuscation, not encryption. The bridge sees the cleartext content. Mesh-side delivery is encrypted end-to-end via Mehr's standard E2E layer — but the bridge itself is a trust boundary for content integrity.

**Legal**: BitTorrent is a neutral protocol used for both legitimate and infringing content. Bridge operators are responsible for compliance with applicable law, just as BitTorrent tracker operators are today. The bridge is a service, not infrastructure — operators choose what to cache and serve.

## Comparison

| | Direct BitTorrent | Via Mehr Bridge |
|---|---|---|
| **Requires internet** | Yes | No (bridge proxies) |
| **Requires IP address** | Yes (for peering) | No (mesh addressing) |
| **Content cached locally** | Only while seeding | Stored in MHR-Store, served from mesh |
| **Payment** | None (volunteer seeding) | MHR micropayments for bridge + relay + storage |
| **Content discovery** | Magnet links, trackers, DHT | MHR-Name + magnet links via bridge |
| **Works on LoRa** | No | Yes (for small files; bridge handles BT-side) |
| **Content integrity** | SHA-1/SHA-256 piece hashes | Bridge verifies BT hashes + Blake3 for Mehr storage |
| **Identity** | None (anonymous peer IDs) | Mehr Ed25519 identity for requesting user |

---

## Marketplace

### Marketplace Overview
<!-- Source: docs/marketplace/overview.md -->

# Layer 4: Capability Marketplace

The capability marketplace is the unifying abstraction of Mehr. Every node advertises what it can do. Every node can request capabilities it lacks. The marketplace matches supply and demand through local, bilateral negotiation — no central coordinator.

This is the layer that makes Mehr a **distributed computer** rather than just a network.

> **Key Insight**
There are no fixed node roles in Mehr. Every capability — relay, storage, compute, connectivity — is discoverable, negotiable, verifiable, and payable through the same marketplace abstraction. Specialization is emergent, not assigned.

## The Unifying Abstraction

In Mehr, there are no fixed node roles. Instead:

- A node with a LoRa radio and solar panel advertises: *"I can relay packets 24/7"*
- A node with a GPU advertises: *"I can run Whisper speech-to-text"*
- A node with an SSD advertises: *"I can store 100 GB of data"*
- A node with a cellular modem advertises: *"I can route to the internet"*

Each of these is a **capability** — discoverable, negotiable, verifiable, and payable.

## Capability Advertisement

Every node broadcasts its capabilities to the network:

```
NodeCapabilities {
    node_id: NodeID,
    timestamp: Timestamp,
    signature: Ed25519Signature,

    // ── CONNECTIVITY ──
    interfaces: [{
        medium: TransportType,
        bandwidth_bps: u64,
        latency_ms: u32,
        reliability: u8,            // 0-255 (avoids FP on constrained devices)
        cost_per_byte: u64,
        internet_gateway: bool,
    }],

    // ── COMPUTE ──
    compute: {
        cpu_class: enum { Micro, Low, Medium, High },
        available_memory_mb: u32,
        mhr_byte: bool,            // can run basic contracts
        wasm: bool,                // can run full WASM
        cost_per_cycle: u64,
        offered_functions: [{
            function_id: Hash,
            description: String,
            cost_structure: CostStructure,
            max_concurrent: u32,
        }],
    },

    // ── STORAGE ──
    storage: {
        available_bytes: u64,
        storage_class: enum { Volatile, Flash, SSD, HDD },
        cost_per_byte_day: u64,
        max_object_size: u32,
        serves_content: bool,
    },

    // ── AVAILABILITY ──
    uptime_pattern: enum {
        AlwaysOn, Solar, Intermittent, Scheduled(schedule),
    },
}
```

### No Special Protocol Primitives

Heavy compute like ML inference, transcription, translation, and text-to-speech are **not protocol primitives**. They are compute capabilities offered by nodes that have the hardware to run them.

A node with a GPU advertises `offered_functions: [whisper-small, piper-tts]`. A consumer requests execution through the standard compute delegation path. The protocol is agnostic to what the function does — it only cares about discovery, negotiation, verification, and payment.

## Emergent Specialization

Nodes naturally specialize based on hardware and market dynamics:

| Hardware | Natural Specialization | Earns From | Delegates |
|---|---|---|---|
| ESP32 + LoRa + solar | Packet relay, availability | Routing fees | Everything else |
| Raspberry Pi + LoRa + WiFi | Compute, LoRa/WiFi bridge | Compute delegation, bridging | Bulk storage |
| Mini PC + SSD + Ethernet | Storage, DHT, HTTP proxy | Storage fees, proxy fees | Nothing |
| Phone (intermittent) | Consumer, occasional relay | Relaying while moving | Almost everything |
| GPU workstation | Heavy compute (inference, etc.) | Compute fees | Nothing |

## Capability Chains

Delegation cascades naturally:

```mermaid
flowchart TB
    A["Node A\n(LoRa relay)"] -->|delegates compute| B["Node B\n(Raspberry Pi)"]
    B -->|delegates storage| C["Node C\n(Mini PC + SSD)"]
    C -->|delegates connectivity| D["Node D\n(Gateway)"]
```

Each link is a bilateral agreement with its own [payment channel](../economics/payment-channels). No central coordination required.

> **Trade-off**
Capability chains create multi-hop economic dependencies — if any intermediate node goes offline, the entire delegation chain must re-negotiate. The protocol mitigates this with `valid_until` expiration and automatic re-discovery, but latency spikes during re-negotiation are unavoidable.

<!-- faq-start -->

## Frequently Asked Questions

<details className="faq-item">
<summary>What is the marketplace actually for?</summary>

The capability marketplace is how nodes buy and sell services from each other — relay bandwidth, storage space, compute power, and internet connectivity. If your node lacks something (e.g., storage), it can find a nearby node that offers it, negotiate a price, and pay automatically via payment channels. Think of it as a local services market that runs in the background.

</details>

<details className="faq-item">
<summary>How are prices set? Is there a fixed rate?</summary>

Prices are set by individual node operators in their capability advertisements. There is no fixed rate — it’s a free market. If many nodes offer storage in your area, competition drives prices down. If relay bandwidth is scarce, prices go up. Your node selects providers based on cost, latency, and reliability automatically.

</details>

<details className="faq-item">
<summary>Can I browse available services without buying anything?</summary>

Yes. Capability advertisements propagate freely through the mesh. Your node maintains a local view of nearby capabilities — who offers storage, compute, relay, and at what prices — without forming any agreements or spending MHR. You only pay when you actually consume a service.

</details>

<details className="faq-item">
<summary>What happens if a provider doesn’t deliver the service I paid for?</summary>

For verifiable services (storage, deterministic compute), the protocol includes challenge-response proofs — you can check that a storage node actually holds your data. For opaque services, repeated failure damages the provider’s reputation, causing other nodes to avoid them. Payment channels settle incrementally, so you’re never out more than one settlement period’s worth of fees.

</details>

<!-- faq-end -->

---

### Capability Discovery
<!-- Source: docs/marketplace/discovery.md -->

# Capability Discovery

Discovery uses concentric rings to minimize bandwidth while ensuring nodes can find the capabilities they need. Most needs are satisfied locally — physically close nodes are cheapest and fastest.

## Discovery Rings

### Ring 0 — Direct Neighbors

```
Scope: Nodes directly connected via any transport
Update frequency: Every gossip round (60 seconds)
Detail level: Full capability exchange
Cost: Free (direct neighbor communication)
```

This is the most detailed and most frequently updated view. A node knows exactly what its immediate neighbors can offer.

### Ring 1 — 2-3 Hops

```
Scope: Nodes reachable in 2-3 hops
Update frequency: Every few minutes
Detail level: Summarized capabilities, aggregated by type
Example: "There's a WASM node 2 hops away, cost ~X"
```

Capabilities are summarized to reduce gossip bandwidth. Instead of full advertisements, nodes share aggregated summaries using `CapabilitySummary` records:

```
CapabilitySummary {
    type: u8,           // matches beacon bitfield (0=relay, 1=gateway, 2=storage, etc.)
    count: u8,          // number of providers of this type (capped at 255)
    min_cost: u16,      // cheapest provider (log₂-encoded μMHR/byte)
    avg_cost: u16,      // average cost across providers (log₂-encoded)
    min_hops: u8,       // nearest provider (hop count)
    max_hops: u8,       // farthest provider (hop count)
}
// 8 bytes per capability type; typical Ring 1 summary: 5-6 types × 8 = 40-48 bytes

Cost encoding: identical to CompactPathCost log₂ formula:
  encoded = round(16 × log₂(value + 1))
  decoded = (2 ^ (encoded / 16.0)) - 1

Special values:
  0x0000 = free (0 μMHR) — valid for trusted-peer services
  0xFFFF = cost unknown or unavailable
```

A Ring 1 gossip message contains one `CapabilitySummary` per capability type present in the 2-3 hop neighborhood. Nodes that appear in multiple capability types are counted in each.

### Ring 2 — Trust Neighborhood

```
Scope: Nodes reachable through the trust graph (friends of friends)
Update frequency: Periodic, via trust-weighted gossip
Detail level: Neighborhood capability summary
Example: "Your neighborhood has 5 gateways, 20 storage nodes"
```

The trust graph provides a natural scope for aggregated capability information. Trusted peers share more detailed information than strangers — this is both efficient (trust = proximity in most cases) and privacy-preserving.

### Ring 3 — Beyond Neighborhood

```
Scope: Nodes beyond the trust graph
Update frequency: On demand (query-based)
Detail level: Coarse hints via Reticulum announces
Example: "A node with GPU compute exists at cost ~X, 8 hops away"
```

Beyond-neighborhood discovery is intentionally coarse and query-driven. The details are resolved when a node actually needs to use a remote capability.

## Bandwidth Efficiency

The ring structure ensures that the most detailed (and most bandwidth-expensive) capability information is only exchanged between direct neighbors, where communication is free. As discovery scope increases, detail decreases proportionally:

```
Ring 0: ~200 bytes per neighbor per round (full capabilities)
Ring 1: ~50 bytes per summary per round (aggregated)
Ring 2: proportional to trust neighborhood size (periodic)
Ring 3: 0 bytes proactive (query-only)
```

On constrained links (< 10 kbps), Rings 2-3 are pull-only — no proactive gossip, only responses to explicit requests. This fits within [Tier 3 of the bandwidth budget](../protocol/network-protocol#bandwidth-budget).

## Discovery Process

When a node needs a capability it doesn't have locally:

1. **Check Ring 0**: Can any direct neighbor provide this?
2. **Check Ring 1**: Are there known providers 2-3 hops away?
3. **Check Ring 2**: Does the trust neighborhood have this capability?
4. **Query Ring 3**: Send a capability query beyond the neighborhood

Most requests resolve at Ring 0 or Ring 1. The further out a query goes, the higher the latency and cost — which naturally incentivizes local provision of common capabilities.

> **Key Insight**
The concentric ring design means discovery bandwidth scales with detail — Ring 0 exchanges full capabilities for free between neighbors, while Ring 3 transmits zero proactive bytes. On constrained LoRa links, this keeps discovery overhead under 50 bytes per gossip round.

## Interest-Based Greedy Routing

Capability discovery (Rings 0–3) answers "who can store my data?" or "who has compute?" — it finds **service providers**. Interest-based routing answers a different question: "who shares my interests?" or "where is the community discussing X?" — it finds **content and communities**.

### Interests as Coordinates

Each node maintains a lightweight **interest vector** derived from the topics it subscribes to (via [MHR-Pub](../services/mhr-pub)), the feeds it follows (via [social](../applications/social)), and the names it resolves frequently (via [MHR-Name](../services/mhr-name)). The interest vector is not shared directly — instead, nodes exchange **interest summaries**: a compact Bloom filter (64–256 bytes) encoding the topic hashes they care about.

```
InterestSummary {
    node_id: [u8; 16],
    bloom: [u8; 64],       // Bloom filter of subscribed topic hashes
    depth: u8,             // 0 = own interests, 1 = neighbors' aggregate, etc.
    cardinality_hint: u8,  // approximate number of distinct interests (log₂-encoded)
}
// 82 bytes — exchanged during Ring 1 gossip
```

The Bloom filter uses 3 hash functions over Blake3-hashed topic strings, giving a false-positive rate under 5% for up to 50 interests in a 512-bit filter.

### Greedy Forwarding

When a node wants to discover a community or topic it doesn't know about, it uses greedy routing — forwarding the query to the neighbor whose interest summary has the highest overlap with the target:

1. **Encode the target** as a topic hash (e.g., `Blake3("community:urban-farming")`)
2. **Check local subscriptions** — if you already follow this topic, done
3. **Check Ring 0 neighbors** — does any neighbor's Bloom filter match the target hash?
4. **Forward to best match** — pick the neighbor whose interest summary has the most bits in common with the query target
5. **Repeat** — the next node checks its neighbors and forwards again

This converges because trust graphs exhibit **small-world structure**: people cluster around shared interests, and a few "bridge" nodes connect disparate clusters. The interest overlap at each hop increases monotonically (on average), just like geographic distance decreases in Kleinberg's navigable small-world model.

```mermaid
flowchart LR
    A["You<br/>(gardening, cooking)"] -->|"best overlap"| B["Neighbor<br/>(gardening, farming)"]
    B -->|"best overlap"| C["Bridge Node<br/>(farming, urban ag)"]
    C -->|"match!"| D["Community Hub<br/>(urban-farming)"]

    style A fill:#4a90d9,color:#fff
    style D fill:#27ae60,color:#fff
    style C fill:#f39c12,color:#fff
```

### Convergence Properties

Greedy routing on small-world graphs reaches the target in $O(\log^2 n)$ hops on average (Kleinberg, 2000). In Mehr's trust graph:

- **Average path length**: For a network of 10,000 nodes, expected ~8–12 hops to reach any interest cluster
- **Failure mode**: If no neighbor is closer to the target than the current node ("local minimum"), the query falls back to Ring 3 DHT lookup — broadcast to the wider network
- **Loop prevention**: Queries carry a visited-set Bloom filter (128 bytes); nodes that detect themselves in the filter drop the query

### Depth Aggregation

Nodes don't just know their own interests — they aggregate neighbor interests at increasing depth:

| Depth | Scope | Filter Size | Update Frequency |
|-------|-------|-------------|------------------|
| 0 | Own subscriptions | 64 bytes | On subscription change |
| 1 | Direct neighbors' aggregated interests | 128 bytes | Every gossip round |
| 2 | 2-hop neighborhood interests | 256 bytes | Every few minutes |

Deeper aggregation uses larger Bloom filters to maintain acceptable false-positive rates as cardinality grows. A depth-2 summary lets a node answer "is anyone within 2 hops interested in X?" without forwarding the query — useful for deciding whether to subscribe to a new topic based on local demand.

### Privacy Considerations

Interest summaries reveal what topics a node cares about. To limit exposure:

- **Noise injection**: Nodes add random bits to their Bloom filter (increasing false positives from 5% to ~15%), obscuring exact interests
- **Depth-only sharing**: Nodes can share only depth ≥ 1 summaries (aggregate neighbor interests) without revealing their own subscriptions
- **Opt-out**: Interest routing is optional. Nodes that don't publish an `InterestSummary` are simply skipped during greedy forwarding — they still participate in Ring-based capability discovery normally

> **Key Insight**
Interest-based routing turns the trust graph into a navigable map of communities. Instead of searching "what services exist nearby?", you search "what communities exist that match my goals?" — and the small-world structure of human social connections ensures you reach them in logarithmically few hops.

## Mobile Handoff

When a mobile node (phone, laptop, vehicle) moves between areas, its Ring 0 neighbors change. Old relay agreements and payment channels become unreachable. The handoff protocol re-establishes connectivity in the new location.

### Presence Beacons

Mehr nodes periodically broadcast a lightweight presence beacon on all their interfaces:

> **Specification**
PresenceBeacons are only 20 bytes and broadcast every 10 seconds. They are **never relayed** — only originating nodes transmit them, keeping bandwidth overhead minimal even in dense deployments.

```
PresenceBeacon {
    node_id: [u8; 16],       // destination hash
    capabilities: u16,        // bitfield (see below)
    cost_tier: u8,            // 0=free/trusted, 1=cheap, 2=moderate, 3=expensive
    load: u8,                 // current utilization (0-255)
}
// 20 bytes — broadcast every 10 seconds

Capability bitfield assignments:
  Bit 0:  relay (L1+ — will forward packets)
  Bit 1:  gateway (internet uplink available)
  Bit 2:  storage (MHR-Store provider)
  Bit 3:  compute_byte (MHR-Byte interpreter)
  Bit 4:  compute_wasm (WASM runtime — Light or Full)
  Bit 5:  pubsub (MHR-Pub hub)
  Bit 6:  dht (MHR-DHT participant)
  Bit 7:  naming (MHR-Name resolver)
  Bits 8-15: reserved (must be 0; future: inference, bridge, etc.)
```

Beacons are transport-agnostic — they go out over whatever interfaces the node has (LoRa, WiFi, BLE, etc.). A mobile node passively receives beacons to discover local Mehr nodes before initiating any connection. This is the decentralized equivalent of a cellular tower scan.

**Beacon propagation rules**:
- Beacons are broadcast by the originating node only — **not relayed** by others
- Scope: local interface (each transport broadcasts independently)
- Collision handling: CSMA/CA at the transport layer (listen-before-talk on LoRa)
- Missed beacons: a node that misses one beacon catches the next in 10 seconds
- **Density adaptation**: if local channel utilization exceeds 50% (measured via CSMA/CA back-off frequency), beacon interval doubles to 20 seconds. Above 75%, interval increases to 30 seconds. This prevents beacons from consuming excessive bandwidth in dense deployments
- Redundancy: Ring 1 gossip (CapabilitySummary) provides backup discovery for nodes that miss beacons — discovery is not solely beacon-dependent

### Handoff Sequence

When a mobile node detects new beacons (new area) or loses contact with its current relay:

```
Handoff:
  1. Select best relay from received beacons (lowest cost × load)
  2. Connect and establish link-layer encryption
  3. Open payment channel (both sign initial state)
  4. Resume communication through new relay
```

On high-bandwidth links (WiFi, BLE), this completes in under 500ms. On LoRa, a few seconds.

### Credit-Based Fast Start

For latency-sensitive handoffs (e.g., active voice call), a credit mechanism allows immediate relay before the payment channel is fully established:

```
CreditGrant {
    grantor: NodeID,              // relay
    grantee: NodeID,              // mobile node
    credit_limit_bytes: u32,      // relay allowance before channel required
    valid_for_ms: u16,            // credit window (default: 30 seconds)
    condition: enum {
        VisibleBalance(min_mhr),  // grantee has balance on CRDT ledger
        TrustGraph,               // grantee is in grantor's trust graph
        Unconditional,            // free initial credit (attract users)
    },
}
```

The relay checks the mobile node's balance on the CRDT ledger (already available via gossip) and extends temporary credit. Packets flow immediately while the channel opens in the background.

**Staleness tolerance**: The relay's CRDT view may be stale (especially after a partition). The credit grant is bounded by `credit_limit_bytes` and `valid_for_ms`, limiting risk to at most one credit window of unpaid traffic. Relays rate-limit fast start grants to **one active grant per unknown node** — a node that exhausts its credit without opening a channel cannot receive another grant for 10 minutes. For `VisibleBalance` grants, the relay requires a balance of at least `2 × credit_limit_bytes × cost_per_byte` to absorb staleness.

If the mobile node has no visible balance and no trust relationship, it must complete the channel open first.

### Roaming Cache

Mobile nodes cache relay information for areas they've visited:

```
RoamingEntry {
    area_fingerprint: [u8; 16],  // Blake3 hash of sorted beacon node_ids
    relays: [{
        node_id: NodeID,
        capabilities: u16,
        last_cost_tier: u8,
        last_seen: Timestamp,
        channel: Option<ChannelState>,  // preserved from last visit
    }],
    ttl: Duration,                // expire after 30 days of non-visit
}
```

When a mobile node enters a previously visited area, it recognizes the beacon fingerprint and reconnects to a cached relay. If a preserved `ChannelState` exists and the relay is still alive, the old channel resumes with zero handoff latency — no new negotiation needed.

**Fingerprint tolerance**: The area fingerprint is approximate — node churn between visits is expected. The mobile node matches if at least 60% of current beacon node_ids appear in the cached fingerprint's sorted set. This is computed as `|intersection| / |cached_set| >= 0.6`. If below threshold, the area is treated as new (full discovery). Beacon node_ids are sorted by numeric value of the destination hash.

### Graceful Departure

No explicit teardown:
- Old agreements expire naturally via `valid_until`
- Old payment channels remain valid and settle lazily (next contact, or via gossip)
- The mobile node's trust graph travels with it — if trusted peers exist in the new area, relay is free immediately

---

### Capability Agreements
<!-- Source: docs/marketplace/agreements.md -->

# Capability Agreements

When a requester finds a suitable provider through [discovery](discovery), they form a bilateral agreement. Agreements are between two parties only — no network-wide registration required.

## Cost Structure

Agreements use a discriminated cost model that adapts to different capability types:

```
CostStructure: enum {
    PerByte {
        cost_per_byte: u64,         // μMHR per byte transferred
    },
    PerInvocation {
        cost_per_call: u64,         // μMHR per function invocation
        max_input_bytes: u32,       // cost covers up to this input size
    },
    PerDuration {
        cost_per_epoch: u64,        // μMHR per epoch of service
    },
    PerCycle {
        cost_per_million_cycles: u64, // μMHR per million compute cycles
        max_cycles: u64,             // hard limit
    },
}
```

| Capability | Typical CostStructure |
|-----------|----------------------|
| Relay / Bandwidth | `PerByte` |
| Storage | `PerDuration` |
| Compute (contract) | `PerCycle` |
| Compute (function) | `PerInvocation` |
| Internet gateway | `PerByte` or `PerDuration` |

## Agreement Structure

> **Specification**
A `CapabilityAgreement` is a bilateral, time-bounded contract signed by both parties. It references an existing payment channel and specifies a cryptographic proof method for verification — no third party or network-wide registration is involved.

```
CapabilityAgreement {
    provider: NodeID,
    consumer: NodeID,
    capability: CapabilityType,     // compute, storage, relay, proxy
    payment_channel: ChannelID,     // existing bilateral channel
    cost: CostStructure,
    valid_until: Timestamp,

    proof_method: enum {
        DeliveryReceipt,            // for relay
        ChallengeResponse,          // for storage (random read challenges)
        ResultHash,                 // for compute (hash of output)
        Heartbeat,                  // for ongoing services
    },

    signatures: (Sig_Provider, Sig_Consumer),
}
```

## Key Properties

### Bilateral

Agreements are strictly between two parties. This means:

- No central registry of agreements
- No third party needs to be involved or informed
- Agreements can be formed and dissolved without network-wide coordination
- Privacy is preserved — only the two parties know the terms

### Payment-Linked

Every agreement references an existing [payment channel](../economics/payment-channels) between the two parties. Payment flows automatically as the service is delivered.

### Time-Bounded

Agreements have an expiration (`valid_until`). This prevents stale agreements from persisting when nodes move or go offline. Parties can renew by forming a new agreement.

### Proof-Verified

Each agreement specifies how the consumer verifies that the provider is actually delivering. See [Verification](verification) for details on each proof method.

## Agreement Lifecycle

```
Agreement states:
  Active:   now < valid_until                        — service is being delivered
  Expired:  now >= valid_until                       — no new service; grace period begins
  Grace:    expired + up to 1 gossip round (60s)     — allows in-flight operations to complete
  Closed:   after grace period                       — agreement is fully terminated
```

### Expiry Behavior

| Capability | On Expiry | Grace Period |
|-----------|-----------|-------------|
| **Relay/Bandwidth** | No new packets routed after `valid_until` | In-flight packets in queue are delivered (up to 60s drain) |
| **Storage** | No new writes accepted | Data remains stored for 1 additional epoch; then subject to [garbage collection](../services/mhr-store#garbage-collection) |
| **Compute** | No new invocations accepted | Running invocations complete; results remain retrievable for 60s |
| **Internet Gateway** | Connection torn down at `valid_until` | In-flight TCP streams drained for up to 60s |

### Renewal

To renew, the consumer sends a new `CapabilityRequest` before the current agreement expires. If terms are unchanged, the provider can respond with a `CapabilityOffer` that extends `valid_until` — no full re-negotiation needed. If terms change, both parties sign a new `CapabilityAgreement`.

### Billing Boundaries

- **PerByte / PerInvocation / PerCycle**: Charged for actual usage up to `valid_until`. No charge after expiry.
- **PerDuration**: Charged for completed epochs only. Partial epochs at agreement end are not billed.

## Agreement Types

| Capability | Typical Duration | Proof Method | Example |
|-----------|-----------------|--------------|---------|
| **Relay/Bandwidth** | Per-packet or ongoing | Delivery Receipt | "Route my packets for the next hour" |
| **Storage** | Hours to months | Challenge-Response | "Store this 10 MB file for 30 days" |
| **Compute** | Per-invocation | Result Hash | "Run Whisper on this audio file" |
| **Internet Gateway** | Ongoing | Heartbeat | "Proxy my traffic to the internet" |

## Negotiation

Negotiation is **single-round** (take-it-or-leave-it) and strictly local — no auction, no bidding, no global price discovery:

```
Negotiation protocol:
  1. Consumer sends CapabilityRequest to provider
  2. Provider responds with CapabilityOffer (or Reject)
  3. Consumer accepts (signs) or walks away
  4. If accepted: both signatures form the CapabilityAgreement
  5. Service begins; payment flows through the channel

CapabilityRequest {
    consumer: NodeID,
    capability: CapabilityType,       // compute, storage, relay, proxy
    desired_cost: CostStructure,      // max cost consumer will accept
    desired_duration: u32,            // seconds
    payment_channel: ChannelID,       // existing channel with this provider
    proof_preference: ProofMethod,    // DeliveryReceipt, ChallengeResponse, etc.
    nonce: u64,                       // replay prevention
}
// Signed by consumer

CapabilityOffer {
    request_nonce: u64,               // matches the request
    provider: NodeID,
    actual_cost: CostStructure,       // provider's terms (≤ desired_cost, or reject)
    valid_until: Timestamp,           // agreement expiration
    proof_method: ProofMethod,        // may differ from preference
    constraints: Option<Vec<u8>>,     // provider-specific (e.g., max object size)
}
// Signed by provider
```

**Timeout**: If the provider doesn't respond within 30 seconds (or 3 gossip rounds on constrained links), the request is considered rejected. The consumer may retry with a different provider.
> **Trade-off**
Single-round negotiation (take-it-or-leave-it) sacrifices price optimality for latency. On LoRa links where each message takes seconds, a multi-round auction would be impractical. Consumers who want better terms must retry with different providers or adjusted parameters.

**No counter-offers**: The provider either meets or undercuts the consumer's desired cost, or rejects. This keeps negotiation to a single round-trip — critical for LoRa where each message takes seconds. If the consumer wants to negotiate, they send a new request with adjusted terms.

Prices are set by providers based on their own cost structure. Within [trust neighborhoods](../economics/trust-neighborhoods), trusted peers often offer discounted or free services.

---

### Verification
<!-- Source: docs/marketplace/verification.md -->

# Verification

The capability marketplace requires that consumers can verify providers are actually delivering the agreed service. Mehr uses different verification methods depending on the type of capability.

## Relay / Bandwidth Verification

**Method**: Cryptographic delivery receipts

The destination node signs a receipt proving the packet arrived. The relay chain can prove it delivered. This creates an unforgeable chain of evidence:

```
Packet sent by Alice → relayed by Bob → relayed by Carol → received by Dave

Dave signs: Receipt(packet_hash, timestamp)
Carol proves: "I forwarded to Dave, here's Dave's receipt"
Bob proves: "I forwarded to Carol, here's the chain"
```

A relay node can only earn routing fees by actually delivering packets to their destination.

> **Threat**
Delivery receipts prove delivery, not legitimacy. A Sybil attacker can fabricate traffic between colluding nodes and produce valid receipts. Economic defenses (demand-backed minting, revenue-capped minting) make this self-dealing structurally unprofitable.

**Note**: Delivery receipts prove that packets were delivered, not that the traffic represents legitimate demand. A Sybil attacker can fabricate traffic between colluding nodes and produce valid delivery receipts. The economic defense against this is [demand-backed minting](../economics/payment-channels#demand-backed-minting-eligibility) — VRF wins only count for minting if the packet traversed a funded payment channel, and [revenue-capped minting](../economics/payment-channels#revenue-capped-minting) ensures self-dealing is always unprofitable.

## Storage Verification

**Method**: Merkle-proof challenge-response (see [MHR-Store](../services/mhr-store#proof-of-storage) for full details)

The consumer challenges a random chunk and the provider returns a Blake3 hash plus a Merkle proof:

```
Challenge-Response Protocol:
1. At storage time, consumer builds a Merkle tree over 4 KB chunks
   and stores only the merkle_root locally
2. Periodically, consumer sends:
   Challenge(data_hash, random_chunk_index, nonce)
3. Provider responds:
   Proof(Blake3(chunk_data || nonce), merkle_siblings)
4. Consumer recomputes merkle root from proof — if it matches, data is verified
```

This is:
- **Lightweight**: Runs on ESP32 in under 10ms — no GPU, no heavy crypto
- **Nonce-protected**: The random nonce prevents pre-computation of responses
- **Merkle-verified**: Consumer only stores the root hash, not the full data
- **Bandwidth-efficient**: ~320 bytes per proof (for a 1 MB file)
- **Partition-safe**: Works between any two directly connected nodes, no chain needed

Three consecutive failed challenges trigger [repair](../services/mhr-store#repair) — the consumer reconstructs the lost shard from erasure-coded replicas and stores it on a replacement node.

## Compute Verification

Compute verification uses three tiers, scaled to the stakes involved:

### Tier 1: Reputation Trust (Cheapest)

Accept the result. The provider has no incentive to lie — getting caught destroys their reputation and all future income.

**Use for**: Low-stakes operations where the cost of a wrong answer is low.

### Tier 2: Optimistic Verification (Moderate)

Accept the result but randomly re-execute 1-in-N requests on a different node. Divergent results flag the provider for investigation.

```
Optimistic Verification:
1. Send compute request to Provider A
2. Accept result immediately
3. With probability 1/N, also send same request to Provider B
4. Compare results
5. If divergent: flag Provider A, reduce reputation
```

**Use for**: Medium-stakes operations. The random audit probability can be tuned — higher for newer/less-trusted providers, lower for established ones.

### Tier 3: Redundant Execution (Expensive)

Send the same request to K independent nodes. The majority result wins.

```
Redundant Execution:
1. Send compute request to K nodes (e.g., K=3)
2. Collect results
3. Majority wins (2 of 3 agree)
4. Dissenting node is flagged
```

**Use for**: High-stakes operations where the result affects payments or irreversible state changes.

> **Key Insight**
Compute verification is tiered by stakes, not by capability. Most operations use optimistic verification (accept then spot-check), keeping costs near 1x while maintaining deterrence through random audits. Only payment-affecting computation requires expensive redundant execution.

## Verification Cost Tradeoffs

| Tier | Cost | Latency | Trust Required | Use Case |
|------|------|---------|---------------|----------|
| Reputation | 1x | 1x | High | Cheap, frequent ops |
| Optimistic | ~1.1x | 1x | Moderate | Default for most compute |
| Redundant | Kx | ~1x (parallel) | Minimal | Payment-affecting compute |

## Heartbeat Verification

For ongoing services (internet gateway, persistent connections), a simple heartbeat mechanism verifies continued availability:

```
Heartbeat Protocol:
1. Consumer sends periodic ping (every N seconds)
2. Provider responds with signed pong
3. If M consecutive heartbeats are missed: agreement terminated
4. Payment stops when heartbeats stop
```

This is suitable for services where the consumer can directly observe whether the service is working (e.g., "I can reach the internet through this gateway").

---

## Applications

### Messaging
<!-- Source: docs/applications/messaging.md -->

# Messaging

End-to-end encrypted, store-and-forward messaging built on the Mehr service primitives.

> **App Manifest**
Messaging is packaged as a **Full** (UI + compute) [AppManifest](../services/mhr-app). It composes MHR-Store for persistent message storage, MHR-Pub for delivery notifications and presence, and MHR-Compute contracts for group key management and co-admin delegation. The UI bundle handles compose/read views while pub/sub topic templates map to per-conversation and per-group notification channels.

## Architecture

Messaging composes multiple service layers:

| Component | Built On |
|-----------|----------|
| Message storage & persistence | [MHR-Store](../services/mhr-store) |
| Delivery notifications | [MHR-Pub](../services/mhr-pub) |
| Transport encryption | Link-layer encryption (Reticulum-derived) |
| End-to-end encryption | [E2E encryption](../protocol/security#end-to-end-encryption-data-payloads) |

## How It Works

1. **Compose**: Alice writes a message to Bob
2. **Encrypt**: Message encrypted end-to-end for Bob's public key
3. **Store**: Encrypted message stored as an immutable DataObject in MHR-Store
4. **Notify**: MHR-Pub sends a notification to Bob (or his relay nodes)
5. **Deliver**: If Bob is online, he retrieves immediately. If offline, relay nodes cache the message for later delivery.
6. **Pay**: Relay and storage fees paid automatically via [payment channels](../economics/payment-channels)

## Offline Delivery

Relay nodes cache messages for offline recipients. When Bob comes back online:

1. His nearest relay nodes inform him of pending messages
2. He retrieves and decrypts them
3. The relay nodes are paid for the storage duration

This is store-and-forward messaging — similar to email, but encrypted and decentralized.

## Group Messaging

Group messages use shared symmetric keys managed by an MHR-Compute contract:

```
GroupState {
    group_id: Blake3Hash,
    members: Set<NodeID>,
    current_key: ChaCha20Key,        // current group symmetric key
    key_epoch: u64,                   // increments on every rotation
    admin: NodeID,                    // creator; can add/remove members
    co_admins: Vec<CoAdminCertificate>,  // up to 3 delegated co-admins
    admin_sequence: u64,             // monotonic counter for admin operations
}

CoAdminCertificate {
    co_admin: NodeID,
    permissions: enum { Full, MembersOnly, RotationOnly },
    granted_by: NodeID,              // must be the group creator
    signature: Ed25519Signature,     // creator's signature over (group_id, co_admin, permissions)
}
```

### Key Management

- **Creation**: The group creator generates the first symmetric key and encrypts it individually for each member's public key (standard E2E envelope per member)
- **Rotation**: When a member joins or leaves, the admin (or any authorized co-admin) generates a new key and distributes it to all current members. The key epoch increments. Old keys are retained locally so members can decrypt historical messages
- **No forward secrecy for groups**: A new member receives only the current key — they cannot decrypt messages sent before they joined. A removed member retains old keys for messages they already received but cannot decrypt new messages (new key was never sent to them)
- **Maximum group size**: Practical limit of ~100 members, constrained by key distribution bandwidth (each rotation sends one E2E-encrypted key envelope per member, ~100 bytes each)

### Co-Admin Delegation

The group creator can delegate admin authority to up to 3 co-admins via signed `CoAdminCertificate` records. This solves the single-admin availability problem without requiring threshold cryptography.

- **Any co-admin can independently**: add/remove members, rotate the group key, and (if granted `Full` permission) promote/demote other co-admins
- **Conflict resolution**: All admin operations carry a monotonically increasing `admin_sequence` number. If two co-admins issue conflicting operations (e.g., simultaneous key rotations), members accept the operation with the highest sequence number. Ties are broken by lowest admin public key hash
- **No threshold crypto**: Co-admin delegation uses only Ed25519 signatures — no multi-round key generation protocols, no new cryptographic primitives. Each delegation certificate is ~128 bytes
- **Graceful degradation**: If all admins go offline, the group continues functioning with its current key. No key rotation or membership changes occur until at least one admin returns

## Bandwidth on LoRa

A 1 KB text message over LoRa takes approximately 10 seconds to transmit — comparable to SMS delivery times. This is viable for text-based communication in constrained environments.

Attachments are DataObjects with `min_bandwidth` set appropriately. A photo attachment might declare `min_bandwidth: 10000` (10 kbps), meaning it will transfer when the recipient has a WiFi link available but won't be attempted over LoRa.

## Security Considerations

<details className="security-item">
<summary>Relay Metadata Leakage</summary>

**Vulnerability:** Even though message content is end-to-end encrypted, relay nodes can observe traffic patterns — who is communicating with whom, when, and how often. Timing correlation attacks could de-anonymize users.

**Mitigation:** Mehr's multi-hop relay architecture provides plausible deniability — a relay node cannot distinguish whether a neighbor originated a packet or is relaying it for someone else. Messages are encrypted blobs with no plaintext metadata. For high-sensitivity scenarios, users can pad messages to uniform sizes and introduce random delays.

</details>

<details className="security-item">
<summary>Group Admin Key Compromise</summary>

**Vulnerability:** If the group creator's private key is stolen, the attacker can add themselves to the group, rotate the symmetric key, and read all future messages. No threshold cryptography is used.

**Mitigation:** The group creator can perform [key rotation](../services/mhr-id) via MHR-ID, which invalidates the compromised key. Co-admin delegation allows trusted members to manage the group if the creator is unavailable. Group members who notice unauthorized changes can leave and form a new group. The design trades multi-party key management complexity for simplicity — appropriate for a mesh network where constrained devices cannot run heavy MPC protocols.

</details>

<details className="security-item">
<summary>No Forward Secrecy for Groups</summary>

**Vulnerability:** Group messaging uses a shared symmetric key. Compromising this key exposes all past messages encrypted with it — there is no per-message forward secrecy as in Signal's Double Ratchet protocol.

**Mitigation:** Admin-initiated key rotation periodically refreshes the group key, limiting the window of exposure. The key is distributed via individual X25519 key exchanges with each member, so compromising one member doesn't reveal the distribution channel to others. For conversations requiring forward secrecy, use direct (1:1) messaging, which supports per-session key exchange.

</details>

---

### Social
<!-- Source: docs/applications/social.md -->

# Social

A decentralized content distribution network built on Mehr primitives. No central servers, no algorithmic recommendations, no ads. Authors pay to publish — skin in the game. Readers pay bandwidth to access — infrastructure sustains itself. Popular content self-funds and propagates wider. Unpopular content expires. Economics replaces algorithms.

While "social" implies short text posts, the same architecture handles **any content type**: music albums, scientific papers, video courses, games, software, journalism, podcasts — anything that can be stored as a DataObject. The envelope/post split, kickback economics, and propagation rules are content-agnostic.

| Content Type | Envelope Shows | Paid Content | Typical Kickback |
|-------------|---------------|-------------|-----------------|
| Text post | Full text (under 280 chars) | Full text + links | Low (cheap to read) |
| Photo essay | Summary + blurhash thumbnails | Full-resolution images | Moderate |
| Music album | Track listing + artist + duration | Audio files | High (large files) |
| Video course | Lesson titles + descriptions | Video files | High |
| Scientific paper | Title + abstract + authors | Full PDF | Moderate |
| Game / software | Name + description + screenshots | Binary + assets | High |
| Podcast episode | Title + show notes | Audio file | Moderate |
| Curated collection | Curator notes per item | References to originals | Curator earns on collection; authors earn on items |

> **App Manifest**
Social is packaged as a **Full** (UI + compute) [AppManifest](../services/mhr-app). It composes MHR-Store for envelopes, posts, profiles, and media DataObjects; MHR-Pub for feed subscriptions across geographic and interest scopes; MHR-Compute for moderation contracts and kickback accounting; MHR-DHT for content discovery; and MHR-Name for human-readable profile addresses. The state schema defines CRDT types for follow lists, feed indices, and profile fields.

## Architecture

Every publication on Mehr has two layers: a **free envelope** that propagates everywhere (browsable at zero cost), and **paid content** that requires retrieval fees. Users browse envelopes to decide what's worth accessing, then pay only for content they actually want.

```mermaid
graph TD
    subgraph FREE["FREE LAYER (PostEnvelope)<br/>Propagates via MHR-Pub to all scope subscribers<br/>~300–500 bytes — fits in a single LoRa frame"]
        Headline
        Summary
        Blurhash
        Scopes["Scopes, metadata"]
    end

    subgraph PAID["PAID LAYER (SocialPost)<br/>Fetched on demand — reader pays relay fees<br/>Size proportional to content"]
        FullText["Full text"]
        Media["Media (images, video, audio)"]
        Links
    end

    Headline -->|post_id| FullText
    Summary -->|post_id| FullText
    Blurhash -->|post_id| FullText
    Scopes -->|post_id| FullText

    FullText -->|Kickback| Author
    Media -->|Kickback| Author
    Links -->|Kickback| Author
```

### PostEnvelope (Free Layer)

The envelope is a lightweight, separate [DataObject](../services/mhr-store) that propagates freely across the mesh. It contains everything a reader needs to decide whether to fetch the full post:

```
PostEnvelope {
    post_id: Option<Blake3Hash>,            // stable ID of the SocialPost (None for boost-only envelopes)
    author: NodeID,
    headline: Option<String>,               // title (~100 chars, author-set)
    summary: Option<String>,                // author-written preview (None for boosts — use the original's)
    media_hints: Vec<MediaHint>,            // lightweight descriptions of attachments
    scopes: Vec<HierarchicalScope>,         // max 1 Geo + up to 3 Topic; total ≤ 1 KB
    reply_to: Option<Blake3Hash>,           // post_id of parent (threading)
    boost_of: Option<Blake3Hash>,           // post_id of boosted post
    references: Vec<Blake3Hash>,            // post_ids of related posts (bidirectional content graph)
    content_size: u32,                      // full post size in bytes (0 for boost-only)
    created: Timestamp,
    sequence: u64,                          // monotonic version counter (incremented on edit)
    kickback_rate: u8,                      // author's desired share of retrieval fees (0-255)
    signature: Ed25519Sig,                  // signed by author (proves authenticity)
}

MediaHint {
    content_type: String,                   // "image/jpeg", "video/mp4", etc.
    size: u32,                              // bytes
    blurhash: Option<String>,               // visual placeholder (~30 bytes)
    alt_text: Option<String>,               // accessibility description
}
```

Envelope size: ~300–500 bytes. Fits in a single LoRa frame with `min_bandwidth: 0`. A reader on a LoRa-only node can browse headlines, summaries, and blurhash thumbnails without paying anything.

### SocialPost (Paid Layer)

The full post is a mutable [DataObject](../services/mhr-store) containing the actual content. Fetching it costs retrieval fees:

```
SocialPost {
    post_id: Blake3Hash,                    // stable ID: Blake3(author ‖ created ‖ nonce)
    author: NodeID,
    content: PostContent {
        text: Option<String>,               // full post body (UTF-8)
        media: Vec<Blake3Hash>,             // references to media DataObjects
        links: Vec<String>,                 // external URLs (for internet-connected nodes)
    },
    sequence: u64,                          // monotonic version counter (0 on first publish)
    edited: Option<Timestamp>,              // None on first publish, Some on edits
    signature: Ed25519Sig,
}
```

The SocialPost is lean — scopes, timestamps, and metadata live on the envelope. The post contains only the content that costs money to retrieve. Both the envelope and the post are mutable DataObjects addressed by `(author, post_id)`. The `post_id` is a stable identifier generated at creation time (`Blake3(author ‖ created ‖ nonce)`) that never changes, even when the content is edited.

### Profile

A mutable DataObject containing identity information. The `claims` list references [IdentityClaims](../services/mhr-id#claims) — including [ProfileField](../services/mhr-id#profile-fields) claims for display name, bio, avatar, and any other key-value data. Each claim has its own [visibility](../services/mhr-id#visibility-controls), so different viewers see different fields depending on their trust relationship with the profile owner.

```
UserProfile {
    node_id: NodeID,
    display_name: String,                   // quick-access copy (also in ProfileField claims)
    bio: Option<String>,                    // quick-access copy
    avatar: Option<Blake3Hash>,             // quick-access copy
    scopes: Vec<HierarchicalScope>,         // from TrustConfig
    claims: Vec<Blake3Hash>,                // references to IdentityClaims (profile fields, geo, links, etc.)
    sequence: u64,                          // monotonic version counter
    signature: Ed25519Sig,
}
```

The `display_name`, `bio`, and `avatar` fields are **quick-access copies** of the corresponding [ProfileField](../services/mhr-id#profile-fields) claims — included directly so clients can render a basic profile without fetching individual claims. The full profile (including visibility-controlled fields, linked accounts, achievements, and verification status) is assembled from the `claims` list. See [Profile Assembly](../services/mhr-id#profile-assembly) for the full resolution flow.

## Feed Types

Mehr social supports five feed types. All feeds are assembled **locally** — no server decides what you see.

```mermaid
graph TD
    Follow["1. FOLLOW<br/>Specific users you choose<br/>Node(alice)"]
    Geographic["2. GEOGRAPHIC<br/>Content from a place:<br/>neighborhood → city → region"]
    Interest["3. INTEREST<br/>Content by topic:<br/>pokemon, physics, jazz"]
    Intersection["4. INTERSECTION<br/>Client-side filter on BOTH:<br/>'Portland Pokemon' = geo ∩ topic"]
    Curated["5. CURATED<br/>Human editor selects best content<br/>Readers subscribe to curator's feed<br/>Two kickback flows: curator + original author"]

    Geographic --> Intersection
    Interest --> Intersection
```

### 1. Direct Follow Feed

Follow specific users. Unchanged from the basic MHR-Pub model.

```
// Follow Alice — see everything she posts
subscribe(Node(alice_node_id), Push);
```

This is the foundation. You follow people you trust, and you see their posts in reverse-chronological order.

### 2. Geographic Feed

Subscribe to content from a geographic area at any level of the [scope hierarchy](../economics/trust-neighborhoods#hierarchical-scopes):

```
// Everything from my neighborhood
subscribe(Scope(Geo("north-america", "us", "oregon", "portland", "hawthorne"), Exact), Push);

// Everything from Portland (all neighborhoods)
subscribe(Scope(Geo("north-america", "us", "oregon", "portland"), Prefix), Digest);

// Everything from Oregon
subscribe(Scope(Geo("north-america", "us", "oregon"), Prefix), PullHint);
```

Geographic feeds are the **local newspaper** — events, news, discussions relevant to where you physically are. Content is cheapest and fastest at the neighborhood level, progressively more expensive at higher scopes.

### 3. Interest Feed

Subscribe to content by topic, independent of geography:

```
// All Pokemon content globally
subscribe(Scope(Topic("gaming", "pokemon"), Prefix), Digest);

// Only competitive Pokemon
subscribe(Scope(Topic("gaming", "pokemon", "competitive"), Exact), Push);

// All science content
subscribe(Scope(Topic("science"), Prefix), PullHint);
```

Interest feeds are **sparse** — they connect people across geography. A Pokemon feed connects Portland, Tokyo, and Berlin. Content propagates through [interest relay nodes](../economics/propagation#interest-relay-nodes) that bridge geographic clusters, but only after [local validation](../economics/propagation#local-first-interest-propagation) — the author's local community must engage with the content (boost, retrieve, or curate) before interest relays forward it globally.

### 4. Intersection Feed

Combine geographic and interest scopes client-side:

```
// Subscribe to Portland AND Pokemon
subscribe(Scope(Geo("...", "portland"), Exact), Push);
subscribe(Scope(Topic("gaming", "pokemon"), Prefix), Push);

// Client-side: show only posts that appear in BOTH feeds
// Result: Portland Pokemon community
```

The protocol delivers by individual scope. The application composes intersection feeds locally by filtering posts that match multiple subscriptions. This keeps the protocol simple while enabling powerful queries.

### 5. Curated Feed

Follow a **curator** — a human who selects the best content from a broader scope:

```
CuratedFeed {
    curator: NodeID,
    name: String,                           // "Portland's Best", "Quantum Physics Weekly"
    description: String,
    entries: Vec<CuratedEntry>,             // max 256 entries per feed page
    scope: HierarchicalScope,               // what this feed covers
    updated: Timestamp,
    sequence: u64,                          // monotonic version counter
    kickback_rate: u8,                      // curator's share of retrieval fees
    signature: Ed25519Sig,
}

CuratedEntry {
    post_id: Blake3Hash,                    // stable ID of original post
    added: Timestamp,
    note: Option<String>,                   // curator's commentary
}
```

A single CuratedFeed holds at most **256 entries**. For larger archives, the curator publishes multiple feed pages as separate DataObjects, each covering a time period or sub-topic. This keeps individual feed objects small enough for constrained devices to fetch and parse.

**How curation works:**

1. Alice follows 200 people and reads the Portland geographic feed daily
2. She publishes a `CuratedFeed` selecting the best posts — "Portland Daily Digest"
3. Bob subscribes to Alice's curated feed instead of following 200 people
4. Bob fetches the CuratedFeed DataObject — Alice earns kickback on it (the curation list has its own `kickback_rate`)
5. Bob's client fetches the **PostEnvelopes** for each curated entry — free, showing headlines + summaries + Alice's curator notes
6. Bob taps posts that interest him — fetches the full SocialPost, and the **original author** earns kickback (the post has its own `kickback_rate`)

These are **two independent kickback flows** on two different DataObjects — the curator's rate and the author's rate don't interact. Bob pays once per DataObject he retrieves, and each DataObject's kickback goes to its respective creator.

**The browsing experience**: Bob sees Alice's curator notes ("Must-read thread on the new bike lanes") alongside each post's envelope (headline, summary, blurhash). He can scroll the entire curated feed for nearly free — only paying when he opens a full post. This makes curated feeds the most bandwidth-efficient way to discover content.

**The curation hierarchy:**

```mermaid
graph TD
    Producers["Producers (authors)<br/>Create content, earn kickback from readers"]
    Curators["Curators (human editors)<br/>Select best content, earn from subscribers"]
    MetaCurators["Meta-curators (curators of curators)<br/>Select best curators, earn from subscribers"]
    Readers["Readers<br/>Pay bandwidth, choose what to subscribe to"]

    Producers --> Curators
    Curators --> MetaCurators
    MetaCurators --> Readers
```

Every level has skin in the game:
- Producers pay to post (anti-spam)
- Curators pay to store their curated feed (anti-spam for curation)
- Readers pay bandwidth (infrastructure sustains itself)
- Kickback flows backward at every level

### Publishing Flow

When an author creates a post, two mutable DataObjects are published:

```
1. Author writes post content + sets headline/summary
2. Client generates a stable post_id: Blake3(author ‖ created ‖ random_nonce)
3. Client creates SocialPost (paid layer):
     → stored as mutable DataObject keyed by (author, post_id)
     → sequence: 0 (first version)
     → only fetched when a reader requests the full content
4. Client creates PostEnvelope (free layer) with same post_id:
     → stored as mutable DataObject with min_bandwidth: 0
     → sequence: 0 (first version)
     → propagates via MHR-Pub to scope subscribers
     → no storage agreement needed within trust network
```

The `post_id` is a stable identifier that never changes — it's the address of both the envelope and the post across all edits. The envelope costs almost nothing to store and propagate (under 500 bytes). The full post costs proportional to its size. Authors pay for content storage, not for letting people know the content exists.

### Editing Posts

Authors can edit their posts by publishing new versions of both the SocialPost and PostEnvelope:

```
Editing flow:
  1. Author modifies content (and optionally headline/summary)
  2. Client publishes updated SocialPost:
       → same post_id, same (author, post_id) key
       → sequence: previous + 1
       → edited: current timestamp
  3. Client publishes updated PostEnvelope (if headline/summary changed):
       → same post_id, same key
       → sequence: previous + 1
  4. MHR-Store propagates the update (highest sequence wins)
  5. MHR-Pub notifies scope subscribers of the updated envelope
```

**Edit properties:**

- **Version history is not preserved** by default. Mutable DataObject semantics: the highest sequence number replaces the previous version. Storage nodes only keep the latest version.
- **Replies, boosts, and references are stable.** They reference the `post_id`, not the content. An edited post doesn't break its reply chains or reference graph.
- **Clients can show edit status.** The `edited` timestamp on SocialPost tells readers the post was modified. Clients may display "edited" alongside the post.
- **No edit limit.** Authors can edit as many times as they want. Each edit increments `sequence` and costs a storage update (negligible).
- **Boosts of edited posts** reflect the latest version. A reader fetching a boosted post always gets the current content.

## Content Economics

```mermaid
graph LR
    Author
    Storage["Storage Node"]
    Reader

    Author -->|"1. Pay storage"| Storage
    Storage -->|"2. Envelope propagates (free)"| Reader
    Reader -->|"3. Browse headlines,<br/>summaries (free)"| Reader
    Reader -->|"4. Fetch full post (paid)<br/>relay fees"| Storage
    Storage -->|"5. Kickback"| Author

    style Author fill:#f9f,stroke:#333
    style Reader fill:#bbf,stroke:#333
    style Storage fill:#bfb,stroke:#333
```

### Browse Before You Pay

Envelopes propagate freely through [MHR-Pub](../services/mhr-pub) notifications. Readers browse without spending:

```
Reader's feed experience:
  1. Receive PostEnvelopes via MHR-Pub subscription (free)
  2. Browse headlines, summaries, blurhash thumbnails (free)
  3. See content_size and estimated retrieval cost (free)
  4. Tap to fetch full SocialPost + media (paid)

LoRa-only node:
  → Sees all envelopes (text headlines + blurhash)
  → Cannot fetch large media (min_bandwidth too low)
  → Can still fetch text-only posts (small enough for LoRa)

WiFi node:
  → Sees all envelopes
  → Fetches full posts + images on demand
  → Fetches video only on high-bandwidth links
```

This solves the "pay before you know what you're getting" problem. The envelope is the shopfront window; the full post is what you pay for inside.

### Author Pays to Post

Every post is a [DataObject](../services/mhr-store) that requires a storage agreement. Posting costs money:

```
Cost to post (neighborhood scope):
    Text-only (~200 bytes):     ~1-5 μMHR per epoch
    With image (~50 KB):        ~50-100 μMHR per epoch
    With video (~5 MB):         ~500-1000 μMHR per epoch

Within trust network: free (no agreement, no payment)
```

This is the **anti-spam mechanism**. Every post costs tokens. Spam is economically irrational because you're paying for content nobody will read. No ML moderation needed — posting costs money, and money is limited.

### Readers Pay for Bandwidth

When someone outside your trust network retrieves your post, they pay relay fees:

```
Reader cost:
    Within trust network:       free
    Cross-trust retrieval:      ~5 μMHR per packet per hop
    Typical 5-hop path:         ~25 μMHR per retrieval

Total reader cost for a text post: ~25-75 μMHR
Total reader cost for an image post: ~250-750 μMHR
```

### Kickback to Author

When readers pay to retrieve content, a portion flows back to the author via the [kickback mechanism](../services/mhr-store#revenue-sharing-kickback):

```
Reader pays 100 μMHR to storage node
    → Storage node keeps (255 - kickback_rate) / 255 of retrieval fee
    → Storage node forwards kickback_rate / 255 to author

At default kickback_rate of 128 (~50%):
    → Storage node keeps ~50 μMHR
    → Author receives ~50 μMHR
```

### Self-Funding Content

Popular content earns more kickback than it costs to store. It becomes **self-sustaining** and [propagates upward](../economics/propagation) through the scope hierarchy automatically:

| Popularity | Scope | Funding Model |
|-----------|-------|--------------|
| Unread | Neighborhood | Author pays entirely |
| Local interest | City | Kickback offsets some storage cost |
| Regional hit | Region | Kickback exceeds storage cost (self-funding) |
| Viral | Country/Global | Self-funding at all levels |

Content that nobody reads expires when the author stops paying. Content that everyone reads lives forever, funded by its own readership. No platform decides — economics decides.

### Why No Likes or Upvotes

Mehr has no reactions, likes, upvotes, or downvotes. This is deliberate.

**The problem with reactions**: Any free reaction can be Sybil-farmed (create 50 accounts, like your own post 50 times). Any paid reaction where the money returns to the sender can be self-tipped in a loop (post → tip own post → kickback returns the tip). Even burn-to-signal (destroy tokens to upvote) creates a vanity metric that people optimize for — the same engagement treadmill that centralized platforms run on.

**Mehr's quality signals are economic, not social:**

| Signal | What It Means | Sybil-Resistant? |
|--------|--------------|-----------------|
| Retrieval count | Real people paid real tokens to read this | Yes — each retrieval costs the reader relay fees that don't return to the author |
| Self-funding threshold | Kickback exceeds storage cost — readership sustains the content | Yes — requires many distinct paying readers |
| Scope promotion | Content bubbled up from neighborhood to city/region | Yes — driven by retrieval demand from geographically distributed nodes |
| Curator inclusion | A human with skin in the game selected this as worth reading | Yes — curator's reputation is on the line |
| Boost count | Multiple people amplified this to their followers | Partially — boosts are free envelopes, but boosting Sybil content wastes your followers' attention |

An author self-retrieving their own post pays relay fees that don't come back via kickback (the storage node keeps its share, and the author is both payer and kickback recipient — net loss). A Sybil cluster retrieving each other's posts bleeds tokens on relay fees with no external demand to sustain it. The economics make fake popularity expensive.

**What readers see instead of like counts**: retrieval-driven propagation scope (neighborhood → city → region tells you how widely read something is), curator endorsements, and boost count from people in their trust graph.

### Clickbait Resistance

The envelope's `headline` and `summary` are author-written and free-form. An author can write a misleading preview. The protocol does not attempt to enforce summary accuracy — there is no way to mechanically verify that a summary "fairly represents" an image, video, or audio post. Instead, clickbait is handled by the same economic and social systems that handle everything else:

**Why clickbait is less profitable on Mehr than on ad-supported platforms:**

| | Ad-Supported Platform | Mehr |
|---|---|---|
| Cost to post | Free | Author pays storage |
| Revenue model | Infinite recurring ad impressions | One-time retrieval fee per reader |
| Amplification | Algorithm promotes high-engagement content | No algorithm — only boosts and curators |
| Second wave | Algorithm keeps serving it to new victims | Zero boosts + zero curation = no second wave |
| Reader cost | Free (attention only) | μMHR per retrieval — small but nonzero |

**Defense layers:**

1. **Local-first propagation** — Content doesn't spread globally on publication. Geographic content starts at neighborhood scope and promotes upward only when retrieval demand justifies it. Interest content starts in the author's local cluster and only propagates to distant clusters when [locally validated](../economics/propagation#local-first-interest-propagation) — at least one boost, multiple distinct retrievals, or curator inclusion. A garbage post tagged with a popular topic never leaves the author's neighborhood because nobody locally endorses it.

2. **Curators are the quality filter** — Raw geographic and interest feeds are unfiltered. Curated feeds are human-verified. For expensive content (video, large media), readers naturally prefer curator-endorsed content over unknown authors. A curator who includes clickbait loses subscribers and kickback revenue.

3. **Client-side author reputation** — After fetching from an author and finding garbage, the client locally downgrades that author. Future envelopes from flagged authors can be marked with a warning or hidden entirely. This is purely local — no protocol change, no centralized blocklist.

4. **Economic self-correction** — Clickbait earns from the first wave of disappointed readers. But with zero boosts, zero curator inclusion, and zero re-reads, the content never self-funds, never promotes to wider scope, and expires when the author stops paying storage. The clickbait author's profit is capped at one wave of retrieval fees minus ongoing storage costs.

5. **Low stakes per read** — A text post retrieval costs ~25–75 μMHR. Being clickbaited once is cheap. Expensive content (video at ~1000+ μMHR) is where the risk is higher — but that's exactly where readers naturally rely on curators rather than browsing raw feeds.

Clickbait on Mehr is a **diminishing-returns attack**: it works once per reader, generates no organic amplification, and the author pays ongoing storage for content that nobody will read a second time.

## Threading and Interaction

### Replies

Replies reference the parent post via `reply_to` on the envelope:

```
Reply to a post:
    PostEnvelope {
        reply_to: Some(parent_post_id),
        summary: "Great point!",
        ...
    }
    SocialPost {
        content: PostContent { text: "Great point! Here's why..." },
        ...
    }
```

Thread assembly is local — each client collects reply envelopes by querying the DHT for envelopes referencing a given hash. Threads are assembled client-side in chronological order. The envelope's summary is enough to display the thread tree — full posts are fetched only when a reader opens a specific reply.

Clients should limit thread traversal depth (recommended: 64 levels). Deeply nested reply chains beyond the limit are still accessible — the client just stops auto-fetching and shows a "load more" prompt. At mesh scale, threads rarely go deep; the economics of reply storage naturally limits chain length.

### Boosts

A boost (repost) references the original via `boost_of` on the envelope:

```
Boost a post:
    PostEnvelope {
        post_id: None,                        // no SocialPost — boost is envelope-only
        boost_of: Some(original_post_id),
        summary: None,                        // original's envelope has the summary
        content_size: 0,
        ...
    }
```

Boosts are envelope-only — `post_id` is None and no SocialPost is created. When a reader fetches the boosted content, the original author receives kickback — not the booster. Boosts are pure amplification without capturing revenue.

### References

References declare that a post is **related to** other posts — without threading (reply) or amplification (boost). They create a queryable content graph: the DHT can answer "what other posts reference this one?", enabling discovery of related discussions, counterarguments, and follow-ups across communities.

```
Reference other posts:
    PostEnvelope {
        references: [post_id_a, post_id_b, post_id_c],
        headline: "Why the bike lane debate misses the point",
        summary: "Alice, Bob, and Carol each analyzed the new bike lanes...",
        ...
    }
```

**What references enable:**

- **Related discussions**: Query the DHT for all envelopes where `references` contains a given hash. A reader viewing a popular post can discover every post that references it — counterarguments, analyses, translations, remixes.
- **Cross-community linking**: A Portland post referenced by a Tokyo post creates a connection between geographic communities. Interest relay nodes can surface cross-references.
- **Knowledge webs**: Scientific papers referencing other papers, course lessons linking to prerequisites, music remixes pointing to originals — any content type benefits from declared relationships.

**How clients render references** is application-dependent. A client might:

1. Show referenced envelopes as linked cards below the post (free — envelope fetch)
2. Show a "referenced by N posts" count with expandable list
3. Build a graph visualization of connected posts
4. Ignore references entirely (minimal client)

When a reader fetches a referenced post's full content, the **referenced author** gets kickback — same as any retrieval.

| | Reply | Boost | Reference |
|---|---|---|---|
| Creates new content | Yes | No (envelope-only) | Yes |
| Relationship | Vertical (parent → child) | Amplification (repost) | Horizontal (related posts) |
| Protocol-level field | `reply_to` on envelope | `boost_of` on envelope | `references` on envelope |
| Queryable via DHT | "What replied to X?" | "Who boosted X?" | "What references X?" |
| Who earns kickback on original | Original author | Original author | Original author |
| Multiple targets | No (one parent) | No (one target) | Yes (any number) |

## Scoped Content

Every post declares its scopes — where it's relevant and how it propagates:

### Local Post (Geographic Only)

```
PostEnvelope {
    scopes: [Geo("north-america", "us", "oregon", "portland")],
    headline: "Hawthorne Farmers Market",
    summary: "Farmers market on Hawthorne this Saturday! Fresh produce, live music...",
    ...
}
```

Envelope appears in Portland geographic feeds. Propagates cheaply within Portland. May bubble up to Oregon if popular.

### Interest Post (Topic Only)

```
PostEnvelope {
    scopes: [Topic("gaming", "pokemon", "competitive")],
    headline: "VGC Meta Analysis",
    summary: "New VGC meta analysis after the latest patch...",
    media_hints: [MediaHint { content_type: "image/png", size: 85000, ... }],
    ...
}
```

Envelope appears in Pokemon interest feeds globally. Readers see the headline and summary for free. The 85KB image is only fetched (and paid for) when a reader opens the full post. Propagates through interest relay nodes. No geographic bias.

### Cross-Scoped Post (Geographic + Interest)

```
PostEnvelope {
    scopes: [
        Geo("north-america", "us", "oregon", "portland"),
        Topic("gaming", "pokemon"),
    ],
    headline: "Portland Pokemon League Meetup",
    summary: "Meetup next Friday at Pioneer Square — all skill levels welcome",
    ...
}
```

Envelope appears in both Portland geographic feeds and Pokemon interest feeds. Portland Pokemon intersection subscribers see it automatically. Note: up to 3 Topic scopes are allowed (e.g., `Topic("gaming", "pokemon") + Topic("events", "meetups")`), but only 1 Geo scope per post, with total scope data capped at 1 KB — see [scope constraints](../economics/trust-neighborhoods#hierarchical-scopes).

### Neighborhood-Only Post (Private)

```
PostEnvelope {
    scopes: [],     // no declared scopes
    summary: "Block party at our place this weekend — neighbors only!",
    ...
}
```

Visible only within trust neighborhood. No propagation beyond trusted peers. The most private form of social posting. Since the audience is trusted peers only, the envelope and full post are both free to access.

Privacy depends on [MHR-Pub](../services/mhr-pub) scope routing: an envelope with no declared scopes matches no `Scope(...)` subscriptions, so it is never forwarded beyond direct MHR-Pub gossip between trusted peers. Nodes only gossip unscoped envelopes to their trusted peer set.

## Media Tiering

The envelope/post split naturally creates a tiered browsing experience. Envelopes carry blurhash thumbnails via `MediaHint`; full media lives in the paid SocialPost as separate DataObjects with `min_bandwidth` constraints:

| Layer | Content | Size | Cost | Available On |
|-------|---------|------|------|-------------|
| Envelope | Headline + summary | ~300-500 bytes | Free | Everywhere, including LoRa |
| Envelope | Blurhash thumbnails (in MediaHint) | ~30-64 bytes each | Free | Everywhere, including LoRa |
| Post | Full text body | ~200 bytes - 10 KB | Retrieval fee | Everywhere |
| Post | Compressed image | ~50 KB | Retrieval fee | WiFi and above |
| Post | Full resolution image | ~500 KB | Retrieval fee | WiFi and above |
| Post | Video | 1+ MB | Retrieval fee | High-bandwidth links only |

The application decides what to fetch based on current link quality:

```
// Envelopes always arrive via MHR-Pub (free)
display(envelope.headline, envelope.summary, envelope.media_hints);

// On user tap — fetch full content
let link = query_link_quality(storage_node);

if link.bandwidth_bps < 1000 {
    // LoRa: text content only
    fetch(post.content.text);
} else if link.bandwidth_bps < 100_000 {
    // Moderate: text + compressed images
    fetch(post.content.text);
    fetch(post.content.media, max_size: 50_000);
} else {
    // High bandwidth: everything
    fetch(post.content.text);
    fetch(post.content.media);
}
```

## Privacy

- **Public posts** (scoped): Propagate per declared scopes and [propagation economics](../economics/propagation). Anyone can pay to read them.
- **Neighborhood-only posts** (unscoped): Visible only within trust neighborhood. Gossip only between trusted peers. The most private social posting.
- **Interest-only posts**: No geographic scope. Propagate through interest graph only — no geographic trail.
- **No social graph leakage**: Following is a local [MHR-Pub](../services/mhr-pub) subscription. No central server has a copy of the social graph. Only the subscriber and the publisher know about the subscription.
- **Unfollowing is silent**: Stop subscribing. No notification, no record.

## Comparison: Economics vs. Algorithms

| | Centralized Social | Mehr Social |
|---|---|---|
| **Spam prevention** | AI moderation (arms race) | Posting costs tokens (economically irrational) |
| **Content ranking** | Opaque algorithm optimizing engagement | Economic signal (what people pay to read) |
| **Content lifespan** | Platform decides | Funded by readership |
| **Monetization** | Ads; platform takes 100% | Direct author-reader kickback; no middleman |
| **Censorship** | Platform discretion | No central point of control |
| **Feed curation** | Algorithmic, engagement-optimized | Human curators with skin in the game |
| **Cost to read** | "Free" (you're the product) | μMHR for strangers; free for friends |
| **Infrastructure** | Company-owned data centers | Self-sustaining through service fees |

---

### Voice
<!-- Source: docs/applications/voice.md -->

# Voice

Voice communication in Mehr ranges from push-to-talk over LoRa to full-duplex calls over WiFi, adapting to available bandwidth.

> **App Manifest**
Voice is packaged as a **Full** (UI + compute) [AppManifest](../services/mhr-app). It composes MHR-Pub for call signaling and presence, MHR-Compute for codec bridging contracts (Codec2 ↔ Opus transcoding and speech-to-text delegation), and MHR-Store for optional voicemail storage. Pub/sub topic templates handle per-call session channels, and the manifest declares `min_tier` based on codec requirements.

## Codec Selection by Link Quality

| Link Type | Codec | Bitrate | Mode |
|-----------|-------|---------|------|
| LoRa (10+ kbps) | Codec2 | 700-3,200 bps | Push-to-talk |
| WiFi / Cellular | Opus | 6-510 kbps | Full-duplex |

### Codec2 on LoRa

Codec2 is an open-source voice codec designed for very low bitrates. At 700 bps, it produces intelligible speech — not high-fidelity, but functional for communication. At 3,200 bps, quality is similar to AM radio.

A 10 kbps LoRa link has enough bandwidth for Codec2 push-to-talk with room for protocol overhead.

### Opus on WiFi

On higher-bandwidth links, Opus provides near-CD-quality voice with full-duplex operation (both parties can talk simultaneously).

## Encryption

Voice streams are **end-to-end encrypted** using the standard [E2E encryption](../protocol/security#end-to-end-encryption-data-payloads) mechanism — each session generates an ephemeral X25519 keypair, and the symmetric session key is derived from a Diffie-Hellman exchange with the remote party's public key. Relay nodes carry encrypted voice packets they cannot decrypt.

## Bandwidth Bridging

When participants are on different link types, the application can use compute delegation to bridge:

```mermaid
flowchart LR
    subgraph opt1["Option 1: Codec Conversion"]
        direction LR
        A1["Alice\\n(LoRa, Codec2)"] -->|LoRa| BR1["Bridge Node\\n(transcode)"]
        BR1 -->|WiFi, Opus| B1["Bob\\n(WiFi)"]
    end
    subgraph opt2["Option 2: Speech-to-Text Bridging"]
        direction LR
        A2["Alice\\n(LoRa, Codec2)"] -->|LoRa| STT["Compute Node\\n(Whisper STT)"]
        STT -->|text| B2["Bob\\n(WiFi, optional TTS)"]
    end
```

This is an **application-level decision** using standard [compute delegation](../marketplace/agreements). The protocol has no concept of "voice" — it routes bytes. The application decides how to adapt between bandwidth tiers.

## Push-to-Talk Protocol

On half-duplex links (LoRa), push-to-talk works as:

1. Sender presses talk button
2. Audio captured, encoded with Codec2
3. Encoded frames sent as a stream of small packets
4. Receiver buffers and plays back
5. Sender releases button, receiver can now respond

The protocol handles this as ordinary data packets — there is no special voice channel.

---

### Forums, Marketplace & Wiki
<!-- Source: docs/applications/community-apps.md -->

# Community Applications

Community applications — forums, marketplaces, and wikis — are built on [MHR-Compute](../services/mhr-compute) contracts managing CRDT state. All degrade gracefully to text-only on constrained links.

> **App Manifest**
Community apps are packaged as **Full** (UI + compute) [AppManifests](../services/mhr-app). Each variant (forum, marketplace, wiki) bundles MHR-Compute moderation and escrow contracts, MHR-Store for posts, listings, or wiki pages as CRDT DataObjects, MHR-Pub for neighborhood-scoped notifications, and MHR-DHT for local search indexing. The state schema defines CRDT merge rules per content type — append-only logs for forums, mutable registers for listings, and operational-transform text for wikis.

## Forums

Forums are append-only logs managed by moderation contracts:

- **Posts** are immutable DataObjects, appended to a topic log
- **Moderation** is handled by an MHR-Compute contract that enforces community rules
- **Threading** is local — each client assembles thread views from the flat log
- **Propagation** uses neighborhood-scoped gossip for local forums, or wider replication for public forums

### Moderation Model

The moderation contract defines:
- Who can post (trusted peers, vouched users, anyone)
- Content rules (enforced at the contract level)
- Moderator keys (who can remove content)
- Appeal mechanisms

Since moderation is a contract, different forums can have different moderation policies. There is no platform-wide content policy.

## Marketplace

Marketplace listings are DataObjects with neighborhood-scoped propagation:

- **Listings** are mutable DataObjects (sellers can update price, availability)
- **Search** is local — each node indexes listings it has received
- **Transactions** happen off-protocol (physical exchange, external payment) or through MHR escrow contracts
- **Reputation** feeds back into the node's general [reputation score](../protocol/security#sybil-resistance)

### Escrow

For MHR-denominated transactions, an escrow contract can hold payment:

1. Buyer deposits MHR into escrow contract
2. Seller delivers goods/services
3. Buyer confirms delivery
4. Contract releases payment to seller
5. Disputes resolved by community moderators (trusted peers with moderator keys)

## Wiki

Wikis are CRDT-merged collaborative documents:

- **Pages** are mutable DataObjects using CRDT merge rules
- **Concurrent edits** merge automatically without conflicts (using operational transforms or CRDT text types)
- **History** is preserved as a chain of immutable snapshots
- **Permissions** managed by an MHR-Compute contract (who can edit, who can view)

## Bandwidth Degradation

All community applications degrade to text-only on constrained links:

| Application | Full Experience | LoRa Experience |
|------------|----------------|-----------------|
| Forums | Rich text, images, threads | Text posts, flat view |
| Marketplace | Photos, maps, categories | Text listings |
| Wiki | Formatted text, images, tables | Plain text |

The application layer handles this adaptation using `query_link_quality()` — the protocol doesn't need to know about the application's content format.

---

### Hosting & Websites
<!-- Source: docs/applications/hosting.md -->

# Hosting on Mehr

Traditional web hosting requires a server, a domain name, a certificate, and ongoing payment to a hosting provider. On Mehr, hosting is just storing DataObjects and letting the network serve them. No server. No DNS. No certificate authority.

> **App Manifest**
Hosting is packaged as a **Static** (content only) [AppManifest](../services/mhr-app) for simple sites, or **Full** (UI + compute) for dynamic sites with server-side logic. It composes MHR-Store for HTML, CSS, JavaScript, and media DataObjects; MHR-Name for human-readable site addresses (e.g., `mysite@geo:portland`); and optionally MHR-Pub for feed updates and MHR-Compute for dynamic content generation or forum moderation contracts.

## Hosting a Website

A "website" on Mehr is a collection of [DataObjects](../services/mhr-store) — HTML, CSS, JavaScript, images — stored in MHR-Store and addressed by content hash or human-readable [MHR-Name](../services/mhr-name).

### Static Site

```
Site structure:
  root = DataObject {
      hash: Blake3("index.html contents"),
      content_type: Immutable,
      payload: Inline("<html>...links to sub-objects...</html>"),
      replication: Network(5),    // 5 copies across the network
  }

  Sub-objects:
    style.css  → DataObject { hash: ..., replication: Network(5) }
    logo.png   → DataObject { hash: ..., replication: Network(3) }
    about.html → DataObject { hash: ..., replication: Network(5) }
```

A visitor retrieves the root DataObject by name (`mysite@portland-mesh`) or by hash. The root object links to sub-objects by their content hashes. The visitor's node fetches each sub-object from the nearest replica in the mesh.

### How Visitors Access Your Site

```
Access flow:
  1. Visitor queries MHR-Name: "mysite@portland-mesh"
  2. Name resolves to root DataObject hash
  3. Visitor's node fetches root from nearest MHR-Store replica
  4. Root contains hashes of sub-objects (CSS, images, etc.)
  5. Visitor's node fetches sub-objects (in parallel, from different replicas)
  6. Local browser or app renders the content
```

Content-addressed storage means:
- **No single server** — content is served from whichever node has a copy
- **No downtime** — as long as any replica is reachable, the site is available
- **No tampering** — content hash guarantees integrity
- **Global CDN by default** — popular content gets cached everywhere automatically

### Updating Your Site

For static content, publish new DataObjects and update the MHR-Name binding to point to the new root hash. Old content remains available at its hash (immutable), but the name now resolves to the new version.

For dynamic content (blog, profile, etc.), use **mutable DataObjects**:

```
Blog post feed:
  feed = DataObject {
      content_type: Mutable,
      owner: your_node_id,
      payload: Inline([post_id_1, post_id_2, ...]),
      // Owner can update the post list by publishing a new version
      // Each individual post is immutable — only the feed index changes
  }
```

## Hosting a Social Feed

A social feed is an append-only log of posts, served via [MHR-Pub](../services/mhr-pub):

```
Your feed:
  1. Profile: Mutable DataObject (name, bio, avatar hash)
  2. Posts: Mutable DataObjects (editable, versioned via sequence number)
  3. Feed index: Mutable DataObject listing post IDs in order
  4. Subscriber notifications via MHR-Pub
```

### Publishing a Post

```
Publishing flow:
  1. Create a mutable DataObject for the post content (keyed by post_id)
  2. Store it in MHR-Store with replication
  3. Update your feed index (mutable) to include the new post_id
  4. MHR-Pub notifies all subscribers of the update
```

### Following Someone

```
Following flow:
  1. Subscribe to their feed via MHR-Pub (topic: Node(their_node_id))
  2. Choose delivery mode based on your link quality:
     - WiFi: Push (full content immediately)
     - Moderate: Digest (batched summaries)
     - LoRa: PullHint (hash-only, pull content when convenient)
  3. Your device assembles your timeline locally from all followed feeds
```

No server assembles your feed. No algorithm decides what you see. Your device pulls from the people you follow, in chronological order.

## Cost of Hosting

| What | Cost | Notes |
|------|------|-------|
| Store a 10 KB page | ~50 μMHR/month | With 5 replicas |
| Store a 1 MB image | ~5,000 μMHR/month | With 3 replicas |
| MHR-Name registration | Free | First-seen-wins within your community label |
| Bandwidth when someone views your page | Paid by the viewer | You don't pay for serving — viewers pay relay costs |

The key economic difference from traditional hosting: **you pay for storage, not for traffic.** The viewer pays the relay cost to reach your content. Popular content is cheaper to host because it gets widely cached.

## Comparison with Traditional Hosting

| Aspect | Traditional Web | Mehr Hosting |
|--------|----------------|---------------|
| **Server** | Required (or hosting provider) | None — content lives in the mesh |
| **Domain name** | Rent from registrar ($10-50/year) | MHR-Name (free, self-registered) |
| **SSL certificate** | Required (free via Let's Encrypt, or paid) | Not needed — all links encrypted, content verified by hash |
| **Uptime** | Depends on your server/provider | Depends on replica count — more replicas = higher availability |
| **Bandwidth costs** | You pay for traffic spikes | Viewers pay their own relay costs |
| **Censorship resistance** | Server can be seized, domain can be seized | No single point to seize — content is replicated across the mesh |
| **Offline access** | Not possible | Cached content available even when original author is offline |

## Hosting a Community Forum

A forum is a more complex application, but the primitives are the same:

```
Forum structure:
  Forum config: Mutable DataObject (rules, moderator keys)
  Topic list: Mutable DataObject (list of topic hashes)
  Each topic: Mutable DataObject (list of post hashes)
  Each post: Immutable DataObject (content + author signature)
  Moderation: MHR-Compute contract enforcing community rules
```

New posts are published as DataObjects, appended to the topic log, and propagated via neighborhood-scoped MHR-Pub to all subscribers. Moderation rules are enforced by an MHR-Compute contract — see [Community Apps](community-apps) for details.

## Running a Service

Beyond static hosting, you can run persistent services on the network:

- **API endpoint**: An MHR-Compute contract that responds to requests
- **Bot/automation**: A node running custom logic, discoverable via the capability marketplace
- **Proxy service**: Bridge Mehr to the traditional web (serve Mehr content via HTTP, or fetch web content for mesh users)

Services are advertised as capabilities and discovered through the [marketplace](../marketplace/overview). Consumers find your service, form agreements, and pay via payment channels — all handled by the protocol.

---

### Voting
<!-- Source: docs/applications/voting.md -->

# Voting

Voting on a decentralized mesh has a fundamental problem: **how do you tell 1 person with 50 keys apart from 50 people with 1 key each?** You can't. So instead of trying to count people, Mehr counts **trust flow** — how much trust the honest network places in you. Creating 50 fake accounts doesn't increase the trust flowing into you. It just divides it thinner.

> **App Manifest**
Voting is packaged as a **Headless** (compute only) [AppManifest](../services/mhr-app). It composes MHR-Compute contracts for trust-flow analysis, vote tallying, and secret ballot commit/reveal logic; MHR-Store for signed ballot storage and election configuration DataObjects; and MHR-DHT for voter eligibility lookups. No UI bundle is included — front-end rendering is handled by the host application that triggers the voting contract.

## The Attack

Before designing defenses, understand the attack:

```
The Sybil Cluster:
  1. Attacker creates 50 NodeIDs
  2. All 50 claim GeoPresence for "portland"
  3. All 50 vouch for each other's geographic claims
  4. Attacker establishes 2-3 trust links from fake nodes to real Portland nodes
  5. Under naive "one verified node, one vote": attacker gets 50 votes

  The 50 fake nodes look internally healthy:
    - They all have vouches ✓
    - They all have RadioRangeProof (attacker has radios) ✓
    - They all trust each other ✓
    - They have outbound trust links to real nodes ✓
```

Every defense below targets a different property of this attack.

## Defense Layer 1: Trust Flow Voting

The core anti-Sybil mechanism. Instead of counting votes, Mehr computes **how much trust flows from the broader honest network into each voter**.

### How It Works

Think of trust like water flowing through pipes. Each honest node is a faucet that produces 1.0 units of trust. Trust flows along trust edges, splitting equally among outbound connections. After many iterations, each node's accumulated trust is its **vote weight**.

```
TrustFlow algorithm (per voting scope):

  1. Initialize: every node in scope gets trust_balance = 1.0
  2. For each iteration (converge after ~10 rounds):
     For each node N:
       outflow = trust_balance(N) / count(N.trusted_peers_in_scope)
       For each trusted peer P of N:
         trust_balance(P) += outflow × damping_factor
     Normalize so total trust in system = number of nodes
  3. Node's vote weight = final trust_balance

  damping_factor = 0.85 (same as PageRank — prevents trust cycling)
```

### Why This Kills Sybil Clusters

```
Honest network (48 real Portland nodes):
  Each real node receives trust from multiple independent real nodes
  Total trust flowing into honest network ≈ 48.0

Sybil cluster (50 fake nodes, 2 outbound trust links):
  Trust enters the cluster ONLY through those 2 edges
  Total trust flowing into entire cluster ≈ 2.0 × damping_factor ≈ 1.7
  Per-Sybil vote weight ≈ 1.7 / 50 = 0.034

  Real node vote weight ≈ 1.0
  50 Sybils × 0.034 = 1.7 total Sybil voting power
  vs. 48 real nodes × ~1.0 = ~48.0 honest voting power
```

**Creating more Sybil nodes makes each one weaker.** The attacker's total voting power is bounded by their inbound trust edges, not their node count. To get 10 votes worth of power, you need 10 real trust relationships — which means convincing 10 real people to trust you, which is expensive and slow.

### Properties

- **Bottleneck principle**: A cluster's total vote weight is bounded by its inbound trust from outside the cluster
- **Dilution**: Adding more Sybil nodes divides the limited inbound trust thinner
- **Locally computable**: Each node runs TrustFlow from its own perspective on its known trust graph. No global consensus needed.
- **Converges quickly**: ~10 iterations on a neighborhood-sized graph (~100 nodes). Feasible on Raspberry Pi class hardware.

## Defense Layer 2: Personhood Vouching

Trust flow handles the math, but there's a social layer too. A **PersonhoodVouch** is a stronger assertion than a regular vouch:

```
PersonhoodVouch {
    voucher: NodeID,
    subject: NodeID,               // "I attest this is a unique human being"
    scope: HierarchicalScope,      // "...in Portland"
    epoch: u64,                    // when issued
    signature: Ed25519Sig,
}
```

### Scarcity Rules

| Rule | Value | Rationale |
|------|-------|-----------|
| Max personhood vouches per node per scope | 5 per epoch | You can't personally know 500 people well enough to vouch for them |
| Vouch expiry | 10 epochs | Forces periodic re-confirmation (people move, leave) |
| Revocable | Immediately | If you discover a Sybil, revoke instantly |

### Economic Liability

If a node you personhood-vouched for is later identified as a Sybil (via trust flow analysis or liveness challenge failure):

```
Sybil detection penalty:
  1. Your PersonhoodVouches for the flagged node are invalidated
  2. Your own vote weight multiplier decreases by 10% per bad vouch
  3. If you vouched for 3+ identified Sybils: you lose voting eligibility
     for the current vote (suspicious — either complicit or negligent)
  4. Penalty decays over 5 epochs (recoverable mistake, not permanent ban)
```

This makes personhood vouching **costly to get wrong** — you're putting your own voting power on the line.

### Personhood vs. Geographic Vouches

| | Geographic Vouch | Personhood Vouch |
|---|---|---|
| Asserts | "This node is in Portland" | "This node is a unique human in Portland" |
| Limit | Unlimited per epoch | 5 per scope per epoch |
| Economic penalty | None (information only) | Vote weight reduction if subject is Sybil |
| Required for voting | Yes | Yes (minimum 2 from distinct vouchers) |
| Can be automated | Yes (RadioRangeProof bots) | No (must be a conscious human decision) |

## Defense Layer 3: Hardware Liveness Challenge

For high-stakes votes, the protocol can require voters to prove they control **distinct physical hardware** simultaneously.

### Challenge Protocol

```
Liveness challenge (triggered by vote initiator for high-stakes votes):

  1. ANNOUNCE: Vote coordinator broadcasts challenge_nonce on LoRa
     Challenge {
         vote_id: Blake3Hash,
         nonce: [u8; 16],
         response_window: Duration,     // e.g., 30 seconds
     }

  2. RESPOND: Each voter must broadcast a signed response via LoRa
     ChallengeResponse {
         voter: NodeID,
         vote_id: Blake3Hash,
         nonce: [u8; 16],
         response_nonce: [u8; 16],      // voter's own random nonce
         signature: Ed25519Sig,
     }

  3. VERIFY: Witnesses observe LoRa responses
     - Each response must arrive as a distinct radio transmission
     - Responses from the same radio (same signal fingerprint, same antenna)
       within the response window indicate a single physical device
     - Witnesses sign attestations of which NodeIDs they observed
       as distinct transmissions

  4. RESULT: Voters who failed the liveness challenge have their
     vote weight zeroed for this vote
```

### Why This Works

An attacker with 50 NodeIDs but 3 LoRa radios:
- Can only transmit 3 distinct responses within the window
- The other 47 NodeIDs either don't respond (weight = 0) or respond from the same radio (detected by witnesses, weight = 0)
- Net result: 3 votes, not 50

### Radio Fingerprinting Algorithm

Witnesses distinguish distinct radios using a **multi-feature classifier** based on physical-layer characteristics:

```
Radio fingerprinting features:

  1. RSSI Pattern (primary):
     Multiple witnesses at known positions measure RSSI of each response.
     The RSSI vector (e.g., [-80, -65, -92] across 3 witnesses) forms
     a spatial signature. Two responses from the same antenna produce
     similar RSSI vectors; different antennas at different locations
     produce different vectors.

     Similarity metric:
       rssi_distance(a, b) = euclidean_distance(a.rssi_vector, b.rssi_vector)
       same_radio_threshold = 6 dB (total across all witnesses)
       Responses with rssi_distance < threshold → flagged as same radio

  2. Clock Drift (secondary):
     LoRa transmitters have slightly different crystal oscillator frequencies.
     Carrier frequency offset (CFO) measured by witnesses provides a
     per-radio fingerprint. CFO is stable over short time windows (minutes)
     but varies between different hardware units.

     Precision: ±50 Hz resolution on typical LoRa receivers
     Discrimination: most LoRa modules differ by >200 Hz CFO

  3. Timing Offset (tertiary):
     The exact arrival time of each response (measured in LoRa symbol
     periods) varies by radio — crystal accuracy affects transmission
     start time. With 30-second response windows, timing alone is weak
     but provides corroborating evidence.

  Combined classifier:
    fingerprint_score(a, b) = w1 × rssi_similarity(a, b)
                            + w2 × cfo_similarity(a, b)
                            + w3 × timing_similarity(a, b)
    Weights: w1=0.5, w2=0.35, w3=0.15

    same_radio if fingerprint_score > 0.75
```

### Error Rates

```
Estimated error rates (based on LoRa fingerprinting literature):

  False positive (two DIFFERENT radios classified as same):
    Urban (high interference):   ~3-5%
    Suburban (moderate):          ~1-2%
    Rural (low interference):    <1%

  False negative (SAME radio classified as different):
    With antenna/location change: ~5-10%
    Static position:              <1%

  Discrimination capacity:
    Typical LoRa deployment: 15-30 distinct radios reliably distinguished
    in a ~1 km² area with 3+ witnesses.
    Above ~50 radios: RSSI vectors become crowded; CFO becomes primary
    discriminant. Practical limit: ~100 radios per geographic cluster
    (bounded by witness processing, not physics).

  Known limitations:
    - Identical hardware from the same batch may have similar CFO
      (mitigated by RSSI spatial diversity)
    - An attacker can use directional antennas to manipulate RSSI
      (mitigated by requiring 3+ geographically distributed witnesses)
    - Environmental changes (temperature, humidity) shift CFO over hours
      (mitigated by the 30-second challenge window being much shorter)
```

### Remote Voter Sybil Resistance

Hardware liveness only works for LoRa-reachable voters. For internet-connected remote voters, Sybil resistance relies on the other defense layers:

```
Remote voter anti-Sybil (no hardware liveness):

  Layer 1: Trust Flow — still the primary defense. Remote Sybils need
           real inbound trust edges from the honest network.

  Layer 2: Personhood Vouching — remote voters still need 2 personhood
           vouches from distinct vouchers who know them personally.

  Layer 3: Temporal Requirements — 10+ epoch account age, service history.

  Layer 4 (remote-specific): IP/Transport Diversity Check
    Remote voters submit votes via internet gateways. Gateways record
    the transport metadata (not stored, just checked in real-time):
      - Multiple votes from the same gateway within the response window
        are flagged for additional scrutiny
      - Witnesses at gateways sign attestations of transport-layer diversity
      - NOT a hard block — just a weight reduction (0.8x multiplier)
        for same-gateway submissions

  Net effect:
    Remote voters have LOWER maximum vote weight than locally-present voters
    (they cannot earn the StronglyVerified geo multiplier of 1.2x).
    Their vote weight is capped at:
      trust_flow × 1.0 (Verified geo, not StronglyVerified) × age × service
    This naturally preferences local, physically-present voters for
    geo-scoped decisions — which is the correct behavior.
```

### Limitations

- Only works for LoRa-reachable voters (not internet-connected nodes voting remotely)
- Radio fingerprinting is imperfect — different radios may have similar characteristics
- Adds latency (30-second challenge window)
- Requires honest witnesses (but witnesses are selected from the trust graph, so Sybil witnesses are low-weight)

Liveness challenges are **optional** — the vote initiator decides whether the stakes warrant them. For a neighborhood potluck vote: probably not. For a community resource allocation: yes.

## Defense Layer 4: Temporal Requirements

You can't create accounts and vote immediately. Voting eligibility requires sustained presence:

```
VotingEligibility {
    voter: NodeID,
    scope: HierarchicalScope,

    // Identity requirements
    geo_verification: GeoVerificationLevel,  // from IdentityClaim
    personhood_vouches: u8,                  // minimum 2, from distinct vouchers
    account_age_epochs: u64,                 // minimum 10 epochs in scope

    // Trust flow (fixed-point: raw_value / 65536 = actual weight)
    trust_flow_weight: u32,                  // from TrustFlow algorithm, fixed-point Q16.16

    // Service history (optional, increases weight)
    service_reputation: u16,                 // from PeerReputation
    epochs_of_service: u64,                  // epochs providing relay/storage/compute in scope
}
```

### Minimum Thresholds

| Requirement | Minimum | Rationale |
|-------------|---------|-----------|
| Account age in scope | 10 epochs | Can't create accounts the day of a vote |
| Personhood vouches | 2 from distinct vouchers | At least 2 real people know you |
| Geographic verification | WeaklyVerified or above | At least some evidence of presence |
| Trust flow weight | greater than 0 | Must have at least one inbound trust edge from outside your immediate cluster |

### GeoVerificationLevel

```
GeoVerificationLevel: enum {
    Unverified,         // self-declared only, no vouches (cannot vote)
    WeaklyVerified,     // 1-2 peer vouches (limited voting weight)
    Verified,           // 3+ peer vouches OR RadioRangeProof (full voting weight)
    StronglyVerified,   // RadioRangeProof + 3+ peer vouches (maximum weight)
}
```

### Time Cost of Sybil Attack

To create a voting-eligible Sybil identity, an attacker must:

1. Create a NodeID (instant, free)
2. Wait 10 epochs (days to weeks — can't rush)
3. Get 2 personhood vouches from real people (social engineering required)
4. Establish trust links to honest nodes (requires providing real service)
5. Maintain presence throughout (continuous hardware operation)

**Per-identity cost**: significant time + hardware + social engineering. And after all that, the trust flow algorithm still limits the cluster's total voting power to its inbound trust edges.

## Vote Weight Formula

A voter's weight combines trust flow with eligibility multipliers:

```
vote_weight(node) = trust_flow_weight
                    × geo_multiplier
                    × age_multiplier
                    × service_multiplier

Where:
  trust_flow_weight:  from TrustFlow algorithm (the primary anti-Sybil signal)

  geo_multiplier:
    Unverified:          0.0  (cannot vote)
    WeaklyVerified:      0.5
    Verified:            1.0
    StronglyVerified:    1.2

  age_multiplier:
    min(account_age_epochs / 100, 1.5)
    (linear growth, capped at 1.5x after 100 epochs)

  service_multiplier:
    1.0 + min(epochs_of_service / 200, 0.5)
    (bonus for service providers, capped at 1.5x)
```

### Attack Scenarios

| Scenario | Attacker's Total Voting Power | Why |
|----------|-------------------------------|-----|
| 50 Sybils, 2 outbound trust edges | ~1.7 votes | Trust flow bottleneck |
| 50 Sybils, 10 outbound trust edges | ~8.5 votes | More edges help, but 10 real trust relationships is expensive |
| 50 Sybils, 0 outbound trust edges | 0 votes | No trust flows into the cluster at all |
| 1 real node, bribes 5 nodes to delegate | ~6 votes | Possible, but the bribed nodes risk their own weight if caught |
| 50 Sybils, all with service history | ~1.7 votes | Service multiplier helps, but trust flow is the dominant factor |

## Voting Mechanisms

### Simple Majority (Trust-Weighted)

```
Vote {
    voter: NodeID,
    vote_id: Blake3Hash,           // identifies the proposal
    choice: enum { Yes, No, Abstain },
    signature: Ed25519Sig,
}

Tally:
    yes_weight  = sum(vote_weight(v) for v where v.choice == Yes)
    no_weight   = sum(vote_weight(v) for v where v.choice == No)
    total_weight = yes_weight + no_weight

    Result: Yes wins if yes_weight / total_weight > 0.5
```

Suitable for binary decisions. Each voter's influence is proportional to their trust flow weight.

### Quadratic Voting

Voters spend MHR tokens to express preference intensity:

```
QuadraticVote {
    voter: NodeID,
    vote_id: Blake3Hash,
    choice: enum { Yes, No },
    tokens_spent: u64,            // μMHR committed to this vote
    signature: Ed25519Sig,
}

Vote power = sqrt(tokens_spent) × trust_flow_weight

Cost for N units of influence = N² μMHR
```

Properties:
- Splitting tokens across Sybil identities doesn't help: `sqrt(100) = 10` but `10 × sqrt(1) = 10` — same result whether one identity spends 100 or ten identities each spend 1
- Trust flow weight still applies — Sybils with low trust flow get reduced power even with tokens
- Allows expressing "I care a lot about this" vs. "I have a mild preference"
- Tokens are burned (not redistributed) to prevent vote-buying markets

### Liquid Democracy

Delegate your vote to someone you trust:

```
VoteDelegation {
    delegator: NodeID,
    delegate: NodeID,              // who receives my voting power
    scope: HierarchicalScope,      // delegation scope (can be narrow)
    vote_id: Option<Blake3Hash>,   // None = standing delegation, Some = per-vote
    signature: Ed25519Sig,
}

Rules:
    - Delegation chains: A delegates to B, B delegates to C
      → C votes with weight(A) + weight(B) + weight(C)
    - Max chain length: 3 hops (prevents unbounded accumulation)
    - Override: delegator can vote directly at any time, revoking delegation
    - Circular delegation: detected and broken (the delegation with the latest
      timestamp in the cycle is dropped; if timestamps tie, highest NodeID breaks it)
    - Delegation is public (necessary for tally verification)
```

Natural fit for Mehr's trust graph — you delegate to people you've already marked as trusted. Combines the convenience of representative democracy with the option of direct participation.

## Vote Lifecycle

```
1. PROPOSE: Any eligible node publishes a Proposal
   Proposal {
       proposer: NodeID,
       scope: HierarchicalScope,          // who can vote
       title: String,
       description: String,
       mechanism: enum { SimpleMajority, Quadratic, Liquid },
       liveness_challenge: bool,          // require hardware liveness?
       voting_period: u32,                // epochs
       quorum: f32,                       // minimum participation (0.0-1.0)
       signature: Ed25519Sig,
   }

2. ELIGIBILITY: Each node locally computes voter eligibility
   - Run TrustFlow on known trust graph within scope
   - Check personhood vouches, account age, geo verification
   - Nodes that don't meet thresholds cannot submit votes

3. CHALLENGE (optional): If liveness_challenge is true
   - Coordinator broadcasts LoRa challenge
   - Voters respond within window
   - Witnesses attest to distinct transmissions

4. VOTE: Eligible nodes submit votes during voting_period
   - Votes are immutable DataObjects stored via MHR-Store
   - Votes propagate via MHR-Pub within the proposal's scope
   - Each node can submit one vote per proposal (latest wins if resubmitted)

5. TALLY: Each node computes the result locally
   - Collect all Vote DataObjects for this proposal
   - Verify signatures and eligibility for each voter
   - Compute trust_flow_weight for each voter from local trust graph
   - Apply vote weight formula
   - Sum weighted votes per choice
   - Check quorum: total participating weight must meet threshold

6. RESULT: Local tally is the result from each node's perspective
   - In a well-connected community, all honest nodes converge on the same result
   - In a partitioned network, different partitions may see different results
     (consistent with Mehr's eventual consistency — reconcile when partition heals)
```

## Quorum and Validity

| Community Size (eligible voters) | Default Quorum | Rationale |
|----------------------------------|---------------|-----------|
| Fewer than 10 nodes | 60% | Small community, high participation needed |
| 10–50 nodes | 40% | Medium community |
| 50–200 nodes | 25% | Larger community |
| More than 200 nodes | 15% | Large community, representative participation |

Quorum is measured by **trust flow weight**, not node count. A quorum of 25% means "voters representing 25% of total trust flow weight in the scope participated" — not "25% of NodeIDs voted."

## Privacy Considerations

Votes on Mehr are **public by default** — every vote is a signed DataObject visible to anyone in scope. This enables:
- Full auditability (anyone can verify the tally)
- Accountability (voters stand behind their choices)
- Delegation transparency (you can see who your delegate voted for)

The tradeoff is no ballot secrecy, which enables social pressure and coercion. For communities that need secret ballots, Mehr provides a **commitment-scheme secret ballot** mechanism designed for partition-prone meshes.

### Secret Ballot Protocol

```
Secret ballot specification:

  1. COMMIT PHASE (during voting_period):
     Voter publishes:
       VoteCommitment {
           voter: NodeID,
           vote_id: Blake3Hash,
           commitment: Blake3(choice || random_nonce || voter_id),
           commit_epoch: u64,
           signature: Ed25519Sig,
       }
     The commitment hides the vote choice. The voter_id in the hash
     prevents two voters with the same choice+nonce from producing
     identical commitments.

  2. REVEAL PHASE (reveal_period epochs after voting_period ends):
     Voter publishes:
       VoteReveal {
           voter: NodeID,
           vote_id: Blake3Hash,
           choice: enum { Yes, No, Abstain, ... },
           nonce: [u8; 32],
           signature: Ed25519Sig,
       }
     Verifier checks: Blake3(choice || nonce || voter_id) == commitment

  3. TALLY PHASE (after reveal_period ends):
     Count all valid reveals. Unrevealed commits are counted as abstentions.
```

### Partition-During-Reveal Handling

A voter who commits but is partitioned during reveal has their vote lost (counted as abstention). This is unavoidable without global consensus. Mehr mitigates the impact:

```
Partition-safe reveal rules:

  Reveal extension window:
    reveal_period = voting_period × 2  (default)
    The reveal window is deliberately long to tolerate temporary partitions.
    A voter who reconnects within the reveal period can still reveal.

  Quorum adjustment for unrevealed commits:
    unrevealed_count = commits - reveals
    If unrevealed_count / total_commits > 0.20:
      The vote is marked INCONCLUSIVE — too many voters were unable to reveal.
      The proposer may re-run the vote with a longer reveal_period.

  Partial reveal acceptance threshold:
    A tally is valid if reveals >= max(quorum_threshold, 0.80 × total_commits).
    Below 80% reveal rate: vote is INCONCLUSIVE.
    Above 80% reveal rate: tally proceeds normally.

  Rationale: 80% threshold ensures that a partition affecting up to 20%
  of voters does not invalidate the vote, while a larger partition
  (which could change the outcome) triggers re-vote.
```

### Partition Reconciliation

If two partitions each tally independently during a partition:

```
Partition reconciliation for secret ballots:

  Each partition tallies only its local reveals.
  On merge:
    1. Merge all VoteCommitment DataObjects (CRDT union)
    2. Merge all VoteReveal DataObjects (CRDT union)
    3. Late reveals (commits from partition A, revealed in partition B)
       are accepted if received within reveal_period of the ORIGINAL
       commit_epoch (not the merge time)
    4. Re-tally with the merged reveal set
    5. If merged reveal rate >= 80% of merged commits: final tally
       If merged reveal rate < 80%: vote remains INCONCLUSIVE

  The merged tally supersedes both partition-local tallies.
  This is consistent with Mehr's eventual consistency model —
  the pre-merge tallies were preliminary, not authoritative.
```

### Deliberate Withhold Attack

A malicious voter who commits but deliberately withholds reveal to manipulate outcomes:

```
Anti-withhold defenses:

  1. Withhold = abstention: A withheld reveal counts as abstention,
     not as a spoiled ballot. The attacker gains nothing unless the
     abstention itself changes the outcome (which requires the vote
     to be extremely close AND the attacker to have significant weight).

  2. Withhold penalty: If a voter commits but does not reveal in 3
     consecutive secret ballots, their future commitments receive a
     0.5x weight multiplier for 10 epochs (decaying penalty).
     Rationale: occasional non-reveal is normal (genuine partitions);
     repeated non-reveal is suspicious.

  3. Quorum absorbs: The 80% reveal threshold means an attacker must
     withhold >20% of total commit weight to force an INCONCLUSIVE
     result — requiring either massive trust flow weight or many
     colluding voters, both expensive.

  4. Economic cost: A voter spends reputation to participate.
     Repeated withholding triggers the penalty, reducing their
     influence in future votes without any benefit from the current one.
```

## Summary of Anti-Sybil Defenses

| Defense | What It Prevents | Cost to Attacker |
|---------|-----------------|------------------|
| **Trust Flow** | Sybil clusters gaining proportional voting power | Must gain real trust edges from honest nodes |
| **Personhood Vouching** | Anonymous/puppet accounts voting | Must convince real humans to vouch (limit 5/epoch) |
| **Hardware Liveness** | Software-only Sybil farming | Must buy physical radios per identity |
| **Temporal Requirements** | Last-minute account creation | Must maintain presence for 10+ epochs |
| **Economic Liability** | Reckless vouching for Sybils | Vouchers lose vote weight if caught |
| **Service History Bonus** | Pure-consumer Sybils | Must provide real relay/storage/compute service |

No single defense is sufficient. The combination makes Sybil attacks expensive across multiple dimensions — hardware, time, social capital, and economic risk — simultaneously.

---

### Digital Licensing
<!-- Source: docs/applications/licensing.md -->

# Digital Licensing

Mehr enables **cryptographically verifiable licensing** for digital assets — images, music, software, datasets, or any content. A license is a signed agreement between a licensor and licensee, stored on the mesh and verifiable by anyone. No enforcement mechanism exists at the protocol level — just like the real world, enforcement is social and legal. Mehr provides the *proof*.

> **App Manifest**
Digital licensing is packaged as a **Headless** (compute only) [AppManifest](../services/mhr-app). It composes MHR-Store for immutable LicenseOffer and LicenseGrant DataObjects, MHR-Compute for verification contracts that validate signature chains and expiry, MHR-DHT for license and asset discovery, and MHR-Name for licensor identity resolution. The manifest has no UI bundle — licensing logic is invoked by content applications (social, hosting) that embed license verification in their own interfaces.

## Overview

```
                       Licensing Flow

  LICENSOR                                          LICENSEE
  ────────                                          ────────
  Publishes asset ──▶ DataObject on mesh
  Publishes LicenseOffer ──▶ "Here are my terms"

                    ◀── Licensee discovers offer
                    ◀── Licensee pays (MHR or fiat)

  Signs LicenseGrant ──▶ Bilateral signed proof
                         of license

  Licensee uses asset in derivative work:
    PostEnvelope.references includes LicenseGrant hash
    → Anyone can verify the chain:
       derivative → LicenseGrant → LicenseOffer → original asset
```

## LicenseOffer

A **LicenseOffer** is a DataObject published by the asset owner, advertising the terms under which others may license the asset:

```
LicenseOffer {
    licensor: NodeID,
    asset_hash: Blake3Hash,           // hash of the licensed DataObject
    license_type: enum {
        Perpetual,                    // one-time payment, permanent license
        Subscription {
            period: u64,              // epochs per billing cycle
        },
        Free,                         // no payment required (e.g., Creative Commons equivalent)
    },
    terms: LicenseTerms,
    price: u64,                       // μMHR per grant (0 for Free type)
    max_grants: Option<u32>,          // None = unlimited, Some(N) = limited edition
    grants_issued: u32,               // current count (updated by licensor)
    created: Timestamp,
    expires: Option<Timestamp>,       // offer expiry (None = open-ended)
    signature: Ed25519Sig,            // signed by licensor
}

LicenseTerms {
    derivative_allowed: bool,         // can licensee create derivative works?
    attribution_required: bool,       // must licensee credit the licensor?
    commercial_use: bool,             // can licensee use commercially?
    transfer_allowed: bool,           // can licensee transfer the license?
    custom_terms: Option<String>,     // human-readable terms (max 1024 chars)
}
```

### Offer Properties

- **Immutable once published**: A LicenseOffer is an immutable DataObject. To change terms, publish a new offer and let the old one expire.
- **Max grants**: Limited-edition licensing. A photographer might offer 100 licenses for a stock photo. `grants_issued` is tracked by the licensor and visible to potential licensees, but not enforced by the protocol — the licensor's reputation depends on honoring the limit.
- **Custom terms**: Free-text for anything the structured fields don't cover. Clients display this to the licensee before purchase.

## LicenseGrant

A **LicenseGrant** is the bilateral proof that a license was issued:

```
LicenseGrant {
    offer_hash: Blake3Hash,           // references the LicenseOffer
    licensee: NodeID,
    granted: Timestamp,
    expires: Option<Timestamp>,       // None for Perpetual, Some for Subscription
    payment_proof: Option<Blake3Hash>,// hash of payment channel state or settlement record
    signature_licensor: Ed25519Sig,   // signed by licensor
    signature_licensee: Ed25519Sig,   // signed by licensee
}
```

### Grant Properties

- **Bilaterally signed**: Both parties sign the grant. This prevents forged licenses (licensee can't self-sign) and disputed grants (licensor can't deny issuing it).
- **Payment proof**: References the [payment channel](../economics/payment-channels) state or settlement record that proves payment occurred. For fiat payments, this may be `None` — the off-protocol payment is between the parties.
- **Expiry**: Subscription licenses expire and must be renewed. The licensor publishes a new LicenseGrant for each renewal period.
- **Stored as DataObject**: The grant lives on the mesh as an immutable [DataObject](../services/mhr-store), retrievable by anyone who knows its hash.

## Verification

Anyone can verify a license chain:

```
Verification chain:

  1. Derivative work (PostEnvelope)
     └── references: [license_grant_hash]

  2. LicenseGrant
     ├── offer_hash → LicenseOffer
     ├── signature_licensor → verify against licensor's public key
     ├── signature_licensee → verify against licensee's public key
     └── expires → check if still valid

  3. LicenseOffer
     ├── asset_hash → original asset
     ├── signature → verify licensor owns the offer
     └── terms → check derivative_allowed, commercial_use, etc.

  4. Original asset (DataObject)
     └── verify licensor is the author (signature on the DataObject)
```

**How a client verifies a derivative work uses licensed assets:**

1. Read the derivative's `PostEnvelope.references` — look for hashes that resolve to LicenseGrant objects
2. Fetch each LicenseGrant from the DHT
3. Verify both signatures (licensor + licensee)
4. Check expiry (is the grant still valid?)
5. Fetch the LicenseOffer via `offer_hash`
6. Verify terms allow the observed use (derivative, commercial, etc.)
7. Optionally fetch the original asset to confirm the licensor is the author

Clients can display verification status: "Licensed from alice@geo:us/oregon/portland" with a link to the original asset.

## Payment Models

### One-Time (Perpetual)

```
Perpetual license flow:

  1. Licensee discovers LicenseOffer (browse, search, curator recommendation)
  2. Licensee opens payment channel with licensor (or uses existing one)
  3. Licensee pays price in μMHR
  4. Licensor signs LicenseGrant with expires: None
  5. Both parties sign; grant stored on mesh
  6. Licensee uses asset indefinitely
```

### Subscription (Recurring)

```
Subscription license flow:

  1. Licensee pays first period
  2. Licensor signs LicenseGrant with expires: granted + period
  3. Before expiry, licensee pays again
  4. Licensor signs a new LicenseGrant (new hash, new expiry)
  5. If licensee stops paying, license expires naturally
     → No revocation needed — expiry is the default
```

Subscription payment is bilateral — the licensor and licensee maintain a [payment channel](../economics/payment-channels) and settle periodically. There is no automatic billing; the licensee must actively renew.

### Fiat Payment

For off-network payments (fiat, barter, or any other arrangement), the `payment_proof` field is `None`. The LicenseGrant still proves the license was issued (both parties signed it) — the payment happened off-protocol.

This is the expected model for [gateway operator](../economics/token-economics#gateway-operators-fiat-onramp) customers who pay fiat and don't hold MHR.

## Use Cases

### Stock Media

A photographer publishes images on Mehr. Each image has a LicenseOffer:

```
LicenseOffer {
    licensor: photographer_node_id,
    asset_hash: photo_hash,
    license_type: Perpetual,
    terms: LicenseTerms {
        derivative_allowed: true,
        attribution_required: true,
        commercial_use: true,
        transfer_allowed: false,
    },
    price: 50_000,                    // 50,000 μMHR
    max_grants: None,                 // unlimited
    ...
}
```

A blogger finds the photo, pays 50,000 μMHR, receives a LicenseGrant, and includes it in their `PostEnvelope.references`. Any reader can verify the blogger licensed the image.

### Software Licensing

A developer publishes software with a subscription license:

```
LicenseOffer {
    license_type: Subscription { period: 52_560 },  // ~1 year in epochs
    terms: LicenseTerms {
        derivative_allowed: false,
        attribution_required: false,
        commercial_use: true,
        transfer_allowed: false,
    },
    price: 1_000_000,                // 1,000,000 μMHR per year
    max_grants: Some(500),           // 500-seat limit
    ...
}
```

### Creative Commons Equivalent

An artist publishes free-to-use artwork:

```
LicenseOffer {
    license_type: Free,
    terms: LicenseTerms {
        derivative_allowed: true,
        attribution_required: true,
        commercial_use: false,
        transfer_allowed: true,
    },
    price: 0,
    max_grants: None,
    ...
}
```

The LicenseGrant still exists (bilaterally signed) — it proves attribution rights even when no payment occurs.

## Off-Network Verifiability

License grants extend **beyond the Mehr network**. A LicenseGrant contains:

- The licensor's public key (verifiable without network access)
- The licensee's public key (verifiable without network access)
- Both signatures (verifiable with standard Ed25519 libraries)
- The asset hash (verifiable against the original file)

Anyone with a copy of the LicenseGrant and the public keys can verify the license — no network connection required. This means:

- A website can display "Licensed via Mehr" with a verifiable proof
- A court can verify license authenticity using standard cryptographic tools
- An archive can preserve license provenance alongside the licensed asset
- A marketplace outside Mehr can check license validity

## Wire Format

### LicenseOffer

| Field | Size | Description |
|-------|------|-------------|
| `licensor` | 16 bytes | Destination hash |
| `asset_hash` | 32 bytes | Blake3 hash of licensed asset |
| `license_type` | 1 byte | 0=Perpetual, 1=Subscription, 2=Free |
| `subscription_period` | 8 bytes | Epochs per cycle (only if Subscription, 0 otherwise) |
| `terms` | 5 bytes | 4 boolean flags (1 byte packed) + custom_terms length |
| `custom_terms` | variable | Length-prefixed UTF-8 (u16 length, max 1024 chars) |
| `price` | 8 bytes | μMHR |
| `max_grants` | 5 bytes | 1 byte flag + 4 bytes u32 (if present) |
| `grants_issued` | 4 bytes | u32 |
| `created` | 8 bytes | Unix timestamp |
| `expires` | 9 bytes | 1 byte flag + 8 bytes timestamp (if present) |
| `signature` | 64 bytes | Ed25519 signature |

Minimum size (no custom terms, no max_grants, no expiry): ~160 bytes.

### LicenseGrant

| Field | Size | Description |
|-------|------|-------------|
| `offer_hash` | 32 bytes | Blake3 hash of LicenseOffer |
| `licensee` | 16 bytes | Destination hash |
| `granted` | 8 bytes | Unix timestamp |
| `expires` | 9 bytes | 1 byte flag + 8 bytes timestamp (if present) |
| `payment_proof` | 33 bytes | 1 byte flag + 32 bytes hash (if present) |
| `signature_licensor` | 64 bytes | Ed25519 signature |
| `signature_licensee` | 64 bytes | Ed25519 signature |

Total: 226 bytes. Lightweight enough to gossip and store indefinitely.

## Limitations

- **No enforcement**: The protocol proves a license exists. It does not prevent unlicensed use. Enforcement is social (community reputation) and legal (courts), just like the real world.
- **No revocation**: A perpetual LicenseGrant, once signed, cannot be revoked at the protocol level. The licensor can publish a signed statement claiming revocation, but the original grant remains valid cryptographically. Subscription licenses handle this naturally via expiry.
- **Licensor authenticity**: The protocol proves the licensor signed the offer, but cannot prove they are the original creator of the asset. A plagiarist could license stolen work. This is an application-layer problem — reputation, identity claims, and peer vouches provide social proof of authorship.
- **`grants_issued` is self-reported**: The licensor updates this counter. A dishonest licensor could undercount to sell more than `max_grants`. Auditing is possible (count all LicenseGrants referencing the offer on the DHT) but not guaranteed to find all grants.

## Comparison with Existing Systems

| | Traditional Licensing | DRM | Mehr Licensing |
|---|---|---|---|
| **Proof of license** | Paper contract, email receipt | Encrypted content + license server | Bilaterally signed DataObject (cryptographic proof) |
| **Verification** | Contact the licensor | Phone home to license server | Verify Ed25519 signatures (offline-capable) |
| **Enforcement** | Legal system | Technical (content won't play without license) | None at protocol level (social + legal) |
| **Transfer** | Often prohibited or complex | Usually prohibited | If `transfer_allowed: true`, licensee can re-grant |
| **Revocation** | Legal notice | License server revokes | Subscription: expiry. Perpetual: not possible |
| **Works offline** | Paper contract: yes | Usually no (needs license server) | Yes (signatures are self-verifying) |
| **Cost** | Legal fees, platform fees | Platform cut (30%+ on app stores) | Direct bilateral payment (zero platform fee) |

---

### Cloud Storage
<!-- Source: docs/applications/cloud-storage.md -->

# Cloud Storage

Mehr provides **decentralized file storage** — like Dropbox or iCloud, but backed by the mesh instead of a corporate data center. Your files are encrypted client-side, replicated across multiple nodes, and accessible from any of your devices. No account with a cloud provider, no monthly subscription to a tech company, no data on someone else's server.

> **App Manifest**
Cloud storage is packaged as a **Full** (UI + compute) [AppManifest](../services/mhr-app). It composes MHR-Store for encrypted file chunks, FileManifests, and SyncManifests; MHR-Pub for cross-device sync notifications; MHR-DHT for chunk location lookups; and MHR-Name for user-friendly file and folder addressing. The state schema tracks per-device sync state as CRDT registers, and the UI bundle provides file browser, upload, and sharing interfaces.

## Overview

```mermaid
graph LR
    subgraph Devices["Your Devices"]
        Phone
        Laptop
        Tablet
    end

    subgraph Storage["Mesh Storage Nodes"]
        A["Node A<br/>(stores chunks 1-3)"]
        B["Node B<br/>(stores chunks 2-5)"]
        C["Node C<br/>(stores chunks 4-6)"]
    end

    Phone -->|"encrypted chunks<br/>via MHR-Store"| A
    Phone -->|"encrypted chunks<br/>via MHR-Store"| B
    Laptop -->|"encrypted chunks<br/>via MHR-Store"| A
    Laptop -->|"encrypted chunks<br/>via MHR-Store"| B
    Laptop -->|"encrypted chunks<br/>via MHR-Store"| C
    Tablet -->|"encrypted chunks<br/>via MHR-Store"| B
    Tablet -->|"encrypted chunks<br/>via MHR-Store"| C

    Storage -.->|"Replication factor: 3<br/>any 2 of 3 nodes can reconstruct"| Storage
```

```mermaid
graph LR
    F1[File] --> E1[Encrypt] --> Ch1[Chunk] --> EC1[Erasure Code] --> St1[Store on N Nodes]

    F2[Fetch Chunks] --> R2[Reconstruct] --> D2[Decrypt] --> Fi2[File]
```

## How It Works

Cloud storage on Mehr uses the same primitives as every other application:

1. **[MHR-Store](../services/mhr-store)** for persistent storage with replication
2. **[StorageAgreements](../marketplace/agreements)** for payment and SLA terms
3. **Client-side encryption** for privacy (storage nodes see only ciphertext)
4. **[Erasure coding](../services/mhr-store#chunking)** for fault tolerance

### Storing a File

```
Store flow:

  1. Client encrypts file with owner's key (ChaCha20-Poly1305)
  2. Client chunks encrypted file into 4 KB blocks
  3. Client applies Reed-Solomon erasure coding (e.g., 4+2 → any 4 of 6 chunks reconstruct)
  4. Client forms StorageAgreements with N storage nodes
     → each node stores a subset of chunks
     → replication factor chosen by user (default: 3)
  5. Client stores a FileManifest (metadata) as a mutable DataObject:
       → file name, size, chunk map, encryption nonce, replication info
  6. FileManifest is synced to all owner's devices via MHR-Pub
```

### Retrieving a File

```
Retrieve flow:

  1. Client reads FileManifest (from local cache or DHT)
  2. Client fetches chunks from nearest available storage nodes
     → parallel fetch across multiple nodes
     → only need k of n chunks (erasure coding)
  3. Client reconstructs, decrypts, presents file
```

### Syncing Between Devices

A user's devices sync through a **SyncManifest** — a mutable DataObject that lists all stored files:

```
SyncManifest {
    owner: NodeID,
    files: Vec<FileEntry>,
    sequence: u64,                      // monotonic version counter
    signature: Ed25519Sig,
}

FileEntry {
    file_id: Blake3Hash,                // stable ID for this file
    name: String,                       // human-readable filename
    size: u64,                          // original file size (bytes)
    manifest_hash: Blake3Hash,          // hash of the FileManifest DataObject
    modified: Timestamp,                // last modification time
    deleted: bool,                      // soft delete (tombstone)
}
```

When a device stores a new file, it updates the SyncManifest. Other devices subscribe to the SyncManifest via [MHR-Pub](../services/mhr-pub) and pull new files on change. Conflict resolution uses standard mutable DataObject semantics — highest sequence number wins.

## File Manifest

Each stored file has a manifest describing its chunks and encryption:

```
FileManifest {
    file_id: Blake3Hash,                // stable identifier
    owner: NodeID,
    original_name: String,
    original_size: u64,
    content_hash: Blake3Hash,           // hash of unencrypted content (for integrity)
    encryption: EncryptionInfo {
        algorithm: "ChaCha20-Poly1305",
        nonce: [u8; 12],
        key_derivation: "HKDF-SHA256",  // derived from owner's identity key + file_id
    },
    chunks: Vec<ChunkInfo>,
    erasure: ErasureInfo {
        data_chunks: u8,                // k
        parity_chunks: u8,             // m (total = k + m)
    },
    replication_factor: u8,
    storage_nodes: Vec<NodeID>,         // nodes holding chunks
    created: Timestamp,
    modified: Timestamp,
    sequence: u64,                      // for updates (file overwrite)
    signature: Ed25519Sig,
}

ChunkInfo {
    index: u16,
    hash: Blake3Hash,                   // hash of encrypted chunk
    size: u16,                          // chunk size (usually 4 KB, last may be smaller)
    storage_node: NodeID,               // which node holds this chunk
}
```

## Sharing

Files can be shared with other users by granting decryption access:

```
Sharing flow:

  1. Owner generates a file-specific decryption key
     (derived from owner's key + file_id via HKDF)
  2. Owner encrypts the file key with the recipient's public key (X25519)
  3. Owner sends the encrypted key to the recipient
     (via encrypted message or MHR-Pub)
  4. Recipient can now fetch and decrypt the file chunks
     using the shared key

Revoking:
  → Owner re-encrypts file with a new key
  → Updates FileManifest
  → Does NOT send new key to revoked user
  → Revoked user can still access old version (if cached)
    but cannot decrypt future updates
```

Sharing is **out-of-band** from the storage nodes' perspective. Storage nodes hold encrypted chunks — they don't know or care who has the decryption key.

## Earning MHR Through Storage

Sharing your device's spare storage is one of the simplest ways to earn MHR. Any node with available disk space can offer storage services:

```
Storage provider flow:

  1. Configure how much storage to offer
     (e.g., 10 GB of your 256 GB drive)
  2. Advertise storage capability via presence beacon
  3. Accept StorageAgreements from clients
  4. Store encrypted chunks, respond to retrieval requests
  5. Earn μMHR per epoch for each active agreement
  6. Pass periodic proof-of-storage challenges

No special hardware needed — any device with spare disk space qualifies.
```

**Why storage is a good starting point for earning:**

- **Zero marginal cost** — you're using disk space that would otherwise sit empty
- **Passive income** — once configured, storage agreements run automatically
- **Low barrier** — no relay infrastructure, no routing complexity, just storing bytes
- **Scales with hardware** — a Raspberry Pi with a USB drive, a NAS, or a server rack all work
- **Builds reputation** — reliable storage providers earn trust, which enables credit extension and higher-value agreements

```
Typical storage earnings:

  Device                    Offered Storage    Est. Monthly Earnings
  ─────────────────────────────────────────────────────────────────
  Raspberry Pi + 32 GB SD   10 GB              ~500-2,000 μMHR
  Old laptop + 500 GB HDD   200 GB             ~10,000-50,000 μMHR
  NAS (4 TB)                2 TB               ~100,000-500,000 μMHR
  Server rack               10+ TB             ~500,000+ μMHR

  Earnings vary with demand, competition, and reputation.
  Early network participants earn more (less competition).
```

### Storage Provider Configuration

```
StorageConfig {
    // How much space to offer
    max_storage: u64,                   // bytes (e.g., 10_000_000_000 for 10 GB)

    // Pricing
    price_per_byte_per_epoch: u64,      // μMHR (market-set, compete with other providers)

    // Policies
    min_agreement_duration: u64,        // minimum epochs for a storage agreement
    max_agreement_duration: u64,        // maximum epochs
    max_single_object_size: u64,        // largest object you'll accept (bytes)

    // Replication
    accept_replica_requests: bool,      // serve as a replication target for others' data
}
```

## Gateway-Operated Cloud Storage

The [genesis service gateway](../economics/token-economics#genesis-service-gateway) is the first operator to offer fiat-priced cloud storage, providing the initial market benchmark for storage pricing. Any [gateway operator](../economics/token-economics#gateway-operators-fiat-onramp) can offer cloud storage as a fiat-billed service — consumers upload files through the gateway's app, the gateway handles MHR storage agreements on their behalf, and the consumer pays a monthly fiat subscription.

```
Gateway cloud storage:

  Consumer → gateway app → files stored on mesh via MHR-Store
                         → consumer pays fiat monthly
                         → gateway pays MHR to storage nodes
                         → consumer never sees tokens

  Pricing examples (gateway-set, competitive):
    5 GB plan:   $2/month
    50 GB plan:  $5/month
    500 GB plan: $15/month
```

This makes Mehr cloud storage accessible to non-technical users — the same UX as any cloud storage app, but backed by decentralized mesh storage instead of a corporate data center.

## Comparison

| | Traditional Cloud | Mehr Cloud Storage |
|---|---|---|
| **Provider** | Single company (Google, Apple, Dropbox) | Multiple mesh nodes (no single provider) |
| **Encryption** | Provider holds keys (or offers client-side as premium) | Always client-side encrypted; storage nodes see only ciphertext |
| **Availability** | Provider's uptime SLA | Erasure coding + replication across independent nodes |
| **Payment** | Monthly fiat subscription | μMHR per epoch, or fiat via gateway |
| **Data location** | Provider's data centers | Distributed across mesh — you choose replication regions |
| **Censorship** | Provider can terminate your account | No single entity controls your data |
| **Earn by sharing** | Not possible | Share spare storage, earn μMHR |
| **Works offline** | No (requires internet) | Yes — local mesh nodes serve cached content |

## Security Considerations

<details className="security-item">
<summary>Corrupted Storage Chunks</summary>

**Vulnerability:** A malicious storage node returns garbage data instead of the requested chunk, causing file corruption on retrieval.

**Mitigation:** Every chunk is content-addressed — the client verifies `Blake3(chunk) == expected_hash` from the FileManifest's Merkle tree. Corrupted chunks are rejected immediately. The [Proof of Storage](../services/mhr-store#proof-of-storage) challenge-response protocol detects nodes that have silently lost or corrupted data, triggering re-replication to healthy nodes.

</details>

<details className="security-item">
<summary>Device Compromise and SyncManifest Tampering</summary>

**Vulnerability:** If one device in a multi-device setup is compromised, the attacker could push a malicious SyncManifest with a higher sequence number, deleting or replacing files across all synced devices.

**Mitigation:** SyncManifest updates propagate via CRDT merge rules — deletions must be explicitly tombstoned and are visible in the version history. Users can restore from previous manifest versions stored across the erasure-coded mesh. Multi-device setups should use [co-admin delegation](messaging#group-messaging) patterns for critical shared state, requiring multiple device signatures for destructive operations.

</details>

<details className="security-item">
<summary>Identity Key Compromise Exposes All Files</summary>

**Vulnerability:** All file encryption keys derive from the owner's identity key via HKDF. Compromising the identity key exposes every stored file.

**Mitigation:** Perform [key rotation](../services/mhr-id) immediately upon suspicion of compromise. Re-encrypt all files under the new identity key. The key rotation propagates through the trust graph, and old encrypted chunks become inaccessible once the old key is disavowed. For high-value data, use per-file random keys wrapped with the identity key — compromise then requires both the identity key and the wrapped key envelope.

</details>

---

### Business Use Cases
<!-- Source: docs/applications/business.md -->

# Business Use Cases

Mehr isn't just for individuals. Every capability in the protocol — identity, storage, compute, naming, marketplace — maps to real business operations. The economic model rewards service providers, and the trust graph provides a natural framework for business relationships.

> **App Manifest**
Business applications are packaged as [AppManifest](../services/mhr-app) entries. A storefront is a **Full** manifest composing MHR-Store (product catalog), MHR-Name (brand name resolution), MHR-Compute (escrow and order logic), and MHR-Pub (inventory and price updates). A gateway operator is a **Service** manifest composing payment-channels (fiat↔MHR bridging) and trust-neighborhood management.

## Local Business Scenarios

### Neighborhood Gateway Operator

**What you do:** Operate a mesh node with an internet uplink (Starlink, fiber, or cellular) and offer network access to your community.

**How it works:**

1. Deploy a Raspberry Pi or mini PC with LoRa + WiFi + internet uplink
2. Register as a [gateway operator](../economics/token-economics#gateway-operators-fiat-onramp)
3. Subscribers sign up (fiat monthly fee or prepaid)
4. You add subscribers as trusted peers — their traffic flows for free through your node
5. You handle MHR costs for onward relay; subscribers never touch tokens

**Revenue:** Fiat subscriptions + minting rewards from relay/storage volume. In areas with no ISP, you are the ISP — but with $100 in hardware instead of millions in tower infrastructure.

| Scale | Hardware | Monthly Revenue (est.) | Customers |
|-------|----------|----------------------|-----------|
| Apartment building | RPi + LoRa + WiFi | $200–500 | 20–50 units |
| Rural village | Solar node + Starlink | $100–300 | 10–30 households |
| Co-working space | Mini PC + Ethernet | $300–800 | 30–80 members |
| Campus / hotel | Multiple APs + gateway | $1,000–5,000 | 100–500 users |

### Local Marketplace Operator

**What you do:** Run a [curated feed](social) for your community's buy/sell listings.

**How it works:**

1. Create a curated commerce feed tagged with your geographic scope (e.g., `market@geo:us/or/portland`)
2. Merchants post listings (text, photos, price) as PostEnvelopes
3. You curate quality — approve legitimate sellers, flag scammers
4. Buyers browse your feed, contact sellers directly, pay in-person or via MHR escrow
5. You earn curation fees from listing visibility

**Value proposition:** No platform fees (no Etsy 6.5%, no eBay 13%). Buyers and sellers pay only relay costs. Your curation creates the value — and you earn from it directly through the [kickback system](social#kickback-economics).

### Café or Shop WiFi Node

**What you do:** Run a mesh WiFi node in your business. Customers get free local access; cross-mesh traffic earns you MHR.

**How it works:**

1. Set up a mesh-enabled WiFi access point ($30–50 hardware)
2. Customers connect automatically — their devices discover your node
3. Local traffic (within your trust neighborhood) is free — draws customers in
4. Cross-boundary traffic (customers accessing distant content) earns relay fees in MHR
5. Optional: offer premium bandwidth tiers via fiat subscription

**Analogy:** Free WiFi that actually pays you instead of costing you.

## Enterprise Scenarios

### Secure Team Communication

**Problem:** Corporate messaging (Slack, Teams) routes through third-party servers. Sensitive communications are accessible to the platform provider and vulnerable to server-side breaches.

**Mehr solution:** Teams use [group messaging](messaging) with end-to-end encryption. Messages traverse the mesh — no corporate server, no third-party platform. Admin delegation allows team leads to manage groups. The mesh operates on company-owned hardware (office nodes, employee devices).

| Traditional | Mehr |
|-------------|------|
| $8–25/user/month (Slack Business+) | Hardware cost only (one-time) |
| Messages stored on Slack's servers | Messages on employee devices + mesh nodes |
| Requires internet | Works over local mesh (no internet dependency) |
| Platform can read messages | E2E encrypted — only participants read messages |
| Single point of failure (platform outage) | Mesh is resilient — no single point of failure |

### Identity Verification Services

**Problem:** KYC (Know Your Customer), age verification, and credential checking require centralized identity providers that accumulate sensitive personal data.

**Mehr solution:** [MHR-ID](../services/mhr-id) provides self-certifying identity with [verifiable credentials](../services/mhr-id/verification). A verification service attests to a user's identity (e.g., "this person is over 18" or "this business is registered in Oregon") by signing an IdentityClaim. The claim is cryptographically verifiable without contacting the issuer — offline verification works.

**Business model:** Charge fiat per attestation. A notary, bank, or government office issues signed claims. Businesses that need to verify customers check claims locally — no API calls, no database queries, no data sharing with the verification provider.

### Distributed Team Infrastructure

**Problem:** Remote teams rely on cloud infrastructure (AWS, Azure) for file sharing, compute, and communication. Costs scale with usage, and data residency requirements complicate operations.

**Mehr solution:**

- **File sharing:** [Cloud storage](cloud-storage) with client-side encryption. Files distributed across team-operated mesh nodes. No cloud provider holds decryption keys.
- **Compute delegation:** Heavy workloads (CI/CD, data processing, ML inference) run on team members' hardware via [MHR-Compute](../services/mhr-compute) agreements. No cloud bill.
- **Communication:** Encrypted messaging and voice over the mesh. Works on-site (office mesh) and remotely (via internet-connected gateways).

### Supply Chain and IoT

**Problem:** Industrial IoT devices (sensors, controllers, monitoring equipment) typically require cellular connectivity and cloud platforms, creating recurring costs and single points of failure.

**Mehr solution:** IoT devices run as [L0/L1 mesh nodes](../hardware/device-tiers) communicating via LoRa. Sensor data flows through the mesh to collection points. Payment channels handle micro-costs automatically. No cellular SIM, no cloud subscription.

| Scenario | Mesh Advantage |
|----------|---------------|
| **Agricultural monitoring** | Solar-powered LoRa sensors across fields — no cellular coverage needed |
| **Warehouse tracking** | Indoor mesh of ESP32 nodes — asset location without dedicated infrastructure |
| **Fleet management** | Vehicle-mounted nodes form mobile mesh — location/status without cellular |
| **Building management** | HVAC, lighting, access control via mesh — no per-device cloud subscription |

### Content Platform / Digital Publishing

**Problem:** Content platforms (Substack, Patreon, Medium) take 5–30% of creator revenue. Creators depend on platform decisions about visibility, monetization, and content policy.

**Mehr solution:** Publishers host content directly on the mesh via [social feeds](social) and [hosting](hosting). Readers pay creators directly through the [kickback system](social#kickback-economics). No platform cut. Content propagation is driven by genuine demand, not algorithmic promotion.

**Revenue model:**

| Revenue Source | Who Earns | How |
|---------------|-----------|-----|
| Content reads | Creator | Kickback from reader's retrieval fee (~50%) |
| Curation | Curator | Curation fee from recommended reads |
| Premium content | Creator | Higher retrieval fee, full-post paywall |
| Licensing | Creator | [LicenseGrant](licensing) fees for derivatives/commercial use |

### Digital Licensing Business

**Problem:** Stock photo agencies, music libraries, and software vendors need centralized platforms to manage license issuance and verification. Platforms take 40–85% of license fees.

**Mehr solution:** Creators publish [LicenseOffers](licensing) directly. Buyers purchase [LicenseGrants](licensing) — cryptographically signed, verifiable without network access. No platform intermediary. The creator sets all terms and receives the full payment.

## Gateway as a Business

The [gateway operator](../economics/token-economics#gateway-operators-fiat-onramp) model is Mehr's primary business-facing primitive. Gateways bridge the fiat-to-crypto gap, making the network accessible to users and businesses that don't want to deal with tokens.

### Gateway Revenue Streams

| Stream | Source | Margin |
|--------|--------|--------|
| **Subscriber fees** | Monthly fiat subscriptions from consumers | Set by operator |
| **Minting rewards** | MHR minted from relay/storage volume across all service types | Protocol-defined |
| **Premium services** | Higher tiers (more storage, faster relay, compute access) | Set by operator |
| **Enterprise contracts** | Dedicated bandwidth/storage for business customers | Negotiated |
| **Roaming agreements** | Revenue sharing with other gateways for roaming subscribers | Bilateral |

### Competitive Dynamics

Multiple gateways can operate in the same area. Competition drives prices down and quality up — standard market economics. Consumers can switch gateways at any time because identity is self-certifying (no lock-in). This creates a natural check on pricing power.

A gateway in a **monopoly position** (sole operator in a rural area) has natural pricing power but faces the constraint that excessive pricing encourages community members to deploy their own nodes — the $35 cost of a Raspberry Pi is the ceiling on gateway extraction.

<!-- faq-start -->

## Frequently Asked Questions

<details className="faq-item">
<summary>Do I need technical knowledge to run a gateway business?</summary>

Initial setup requires basic technical skills (installing software on a Raspberry Pi or mini PC, configuring WiFi and LoRa radios). Once running, the gateway operates autonomously — the protocol handles routing, billing, and service negotiation. The fiat billing side (subscriptions, payment processing) is an off-protocol business concern handled however you prefer.

</details>

<details className="faq-item">
<summary>How do businesses handle compliance and regulation?</summary>

Mehr provides infrastructure — how businesses use it is subject to local law, just like the internet. Gateway operators may need to comply with telecommunications regulations in their jurisdiction. Identity verification services must comply with data protection laws. The protocol itself is neutral — it provides cryptographic tools, not legal opinions.

</details>

<details className="faq-item">
<summary>Can a business accept MHR as payment for non-network services?</summary>

Yes. MHR is a transferable token — if a business wants to accept it for goods or services, nothing in the protocol prevents that. The business would need to value MHR based on what it can buy on the network (relay, storage, compute time). In practice, gateway operators are the natural fiat↔MHR exchange point.

</details>

<details className="faq-item">
<summary>What's the minimum investment to start a gateway business?</summary>

Hardware: $35–100 (Raspberry Pi + LoRa module + WiFi). Internet uplink: whatever you already have (home fiber, Starlink, cellular). Software: free (open source). The primary ongoing cost is the internet uplink. Revenue potential depends on subscriber count and local demand.

</details>

<!-- faq-end -->

## Security Considerations

<details className="security-item">
<summary>Gateway Operator Monopoly Abuse</summary>

**Vulnerability:** In areas with only one gateway operator, the operator can overcharge subscribers or degrade service quality without competitive pressure.

**Mitigation:** The barrier to entry is a $35 Raspberry Pi. Any community member can deploy a competing gateway. Social pressure (trust revocation) penalizes exploitative operators. Subscribers can leave instantly — identity is self-certifying, no lock-in.

</details>

<details className="security-item">
<summary>Fraudulent Business Listings</summary>

**Vulnerability:** A seller posts fake marketplace listings (products that don't exist, misleading descriptions) to collect payment.

**Mitigation:** Trust-weighted visibility means unknown sellers have low reach. Escrow via MHR payment channels allows conditional release (buyer confirms receipt before payment finalizes). Curator reputation is staked — a curator who includes fraudulent listings loses subscriber trust. Community trust revocation isolates repeat offenders.

</details>

<details className="security-item">
<summary>IoT Device Compromise</summary>

**Vulnerability:** A compromised IoT sensor node could flood the mesh with fake data or act as an attack vector for the wider network.

**Mitigation:** IoT devices operate as L0/L1 nodes with limited capabilities — they cannot run contracts, mint tokens, or participate in governance. Per-epoch credit limits cap the damage any single compromised node can cause. Trust revocation immediately isolates suspicious devices. The mesh's decentralized architecture means compromising one sensor doesn't grant access to the collection endpoint's data.

</details>

---

### Roaming & Connectivity
<!-- Source: docs/applications/roaming.md -->

# Roaming & Connectivity

On the traditional internet, switching networks is disruptive — you change WiFi, your connections drop; you move between cells, there's a handoff delay; you plug into a different ethernet port, your IP changes. On Mehr, **your identity is your cryptographic key, not your network address.** Moving between transports, locations, and access points is seamless.

> **App Manifest**
Roaming is packaged as a **Headless** (compute only) [AppManifest](../services/mhr-app). It composes MHR-Pub for announce-based presence and transport change notifications, MHR-DHT for route discovery across transport boundaries, and MHR-Compute for link-quality scoring and handoff priority logic. No UI bundle is needed — the roaming service runs as a background daemon that other applications depend on for seamless transport switching.

## How Roaming Works

Mehr's transport layer handles multiple interfaces simultaneously and routes through whichever is best. Your identity doesn't change when your network connection does:

```mermaid
flowchart LR
    A["Home WiFi\n100 Mbps\nFull media"] -->|walk| B["LoRa only\nText only"]
    B -->|arrive| C["Café WiFi\n50 Mbps\nFull media"]
    C -->|walk| D["LTE-M\nModerate"]
    ID(["Your identity: same key throughout"]) -.-> A
    ID -.-> B
    ID -.-> C
    ID -.-> D
    style ID fill:none,stroke:none,color:#666
```

**Key properties:**

- **Identity is transport-independent.** Your NodeID (Ed25519 public key) stays the same regardless of which radio, WiFi, or ethernet port you're using. Nobody needs to "re-discover" you.
- **Announce-based routing adapts automatically.** When you connect to a new access point, your node sends an announce. Nearby nodes update their routing tables within seconds. Traffic starts flowing through the new path.
- **Applications adapt to link quality.** When you switch from WiFi to LoRa, apps detect the bandwidth change and adjust — images become blurhash previews, video pauses, voice degrades to push-to-talk. When you reach WiFi again, full quality resumes.
- **No session state on the network.** There are no "sessions" to transfer between access points. Your node is an endpoint, not a session on someone else's server. Moving between mesh nodes doesn't require session migration.

## Ethernet Roaming

Plug your device into any ethernet port on the mesh and you're connected — no IP configuration, no DHCP negotiation, no VPN. Your Mehr identity authenticates you, and traffic routes through the mesh.

```
Ethernet roaming scenario:

  Building A                          Building B
  ──────────                          ──────────
  [Ethernet port] ──mesh──▶          [Ethernet port] ──mesh──▶
       │                                   │
  Plug in laptop                      Unplug, walk over,
  → announce sent                     plug in again
  → routes established                → new announce
  → online in <1 sec                  → routes update
                                      → online in <1 sec

  Same identity, same conversations, same files.
  No reconfiguration. No VPN reconnect. No IT ticket.
```

**Use cases:**

| Scenario | How It Works |
|----------|-------------|
| **Office building** | Ethernet ports on every floor. Move between desks — plug in anywhere |
| **Campus** | Multiple buildings, each with mesh nodes. Walk between them, stay connected |
| **Co-working space** | Gateway operator provides ethernet access, bills via fiat subscription |
| **Manufacturing floor** | Fixed ethernet connections at workstations. Workers roam between stations |
| **Home** | Multiple rooms with ethernet drops. Devices plug in wherever convenient |

### Gateway-Provided Ethernet

[Gateway operators](../economics/token-economics#gateway-operators-fiat-onramp) can deploy ethernet access points as part of their service. A co-working space, hotel, or campus deploys mesh-connected ethernet ports. Customers (gateway subscribers) plug in and have immediate access — the gateway handles authentication via its trusted peer relationship.

```
Gateway ethernet access:

  Customer signs up with gateway (fiat subscription)
    → gateway adds customer to trusted_peers
    → customer plugs into any gateway ethernet port
    → traffic routes through gateway (free for trusted peer)
    → gateway handles MHR costs for onward relay
    → customer experiences: "plug in, works"
```

## WiFi Roaming

WiFi roaming works the same way — your device automatically discovers and connects to nearby mesh WiFi nodes:

```mermaid
flowchart LR
    H["Home AP"] -->|walk| C["Café AP"]
    C -->|walk| L["Library AP"]
    H -.-|announce| H2(["routes set"])
    C -.-|announce| C2(["routes update"])
    L -.-|announce| L2(["routes update"])
    style H2 fill:none,stroke:none
    style C2 fill:none,stroke:none
    style L2 fill:none,stroke:none
```

The transition between WiFi access points takes under a second — the time for an announce to propagate and routes to update. Active connections (messaging, voice) continue without interruption because they're addressed to your NodeID, not to a network address.

## Multi-Transport Handoff

Mehr nodes can have multiple active interfaces. When one interface drops, traffic shifts to the next best option:

```
Handoff priority (configurable per node):

  1. Ethernet (if available) — highest bandwidth, lowest latency
  2. WiFi (if in range) — high bandwidth
  3. LTE/LTE-M (if available) — wide coverage
  4. LoRa (always available in mesh) — fallback, low bandwidth

When WiFi drops:
  → Traffic shifts to next available transport (<1 sec)
  → Applications adapt to new bandwidth
  → No connection reset, no re-authentication
```

### Voice Call Handoff

Voice calls demonstrate seamless handoff:

```mermaid
stateDiagram-v2
    [*] --> WiFi_Home: Start call\n16 kbps Opus
    WiFi_Home --> LoRa: Walk out of range
    LoRa --> WiFi_Dest: Arrive at destination
    WiFi_Dest --> [*]: Call ends

    WiFi_Home: WiFi (High Quality)
    LoRa: LoRa (2.4 kbps Codec2)
    WiFi_Dest: WiFi (16 kbps Opus)

    note right of LoRa: Handoff < 1 second\nCall never drops
```

The call never drops. Quality adapts. Interruption during handoff: < 1 second.

## How Announce-Based Routing Enables Roaming

The transport layer's announce mechanism is what makes all of this work. Every Mehr node periodically broadcasts its identity and routing information:

```
Roaming via announces:

  1. Node arrives at new location (plugs in, enters WiFi range, etc.)
  2. Node sends announce on new interface:
       [NodeID] [MehrExtension] [routing info]
  3. Nearby nodes update routing tables:
       "Node X is now reachable via this interface at this cost"
  4. Traffic destined for Node X takes the new path
  5. Old paths age out (no traffic = removed from routing table)

Time from connection to routable: <1 second (announce propagation)
```

No central mobility controller. No IP address reassignment. No DHCP. No DNS update. The announce is the only message needed — everything else follows from distributed routing.

## Comparison

| | Traditional Networking | Mehr Roaming |
|---|---|---|
| **Identity** | IP address (changes with network) | Cryptographic key (permanent) |
| **Switch networks** | Connections drop, reconnect needed | Seamless, sub-second handoff |
| **Multi-transport** | Manual (WiFi or cellular, rarely both) | Automatic, simultaneous interfaces |
| **Authentication** | Per-network (passwords, certificates) | Identity key (universal) |
| **Configuration** | DHCP, DNS, proxy settings | None — announce-based discovery |
| **Ethernet roaming** | Requires enterprise 802.1X, VLAN config | Plug in anywhere, works immediately |
| **Voice handoff** | Cellular does this; WiFi calling is fragile | Seamless across all transports |
| **Works without internet** | No | Yes — mesh routing is local |

---

## Development

### Roadmap
<!-- Source: docs/development/roadmap.md -->

# Implementation Roadmap

The Mehr implementation follows a **server-first** strategy: get the protocol running on well-connected Linux servers over traditional internet, prove the core services work, then extend to phones and mesh radio. The protocol spec is comprehensive because it needs to be — but implementation is ruthlessly phased. Each phase delivers something people can use, not just something that passes tests.

```mermaid
graph LR
    P1["<b>Phase 1: Linux Server Node (MVP)</b><br/>TCP/IP transport<br/>Storage + DHT<br/>Trust graph<br/>Free tier only<br/><i>Users: server operators</i>"]
    P2["<b>Phase 2: Economics + Social</b><br/>Payment channels<br/>VRF lottery<br/>CRDT ledger<br/>Social feeds<br/><i>Users: economy bootstraps</i>"]
    P3["<b>Phase 3: Mobile + Mesh</b><br/>Phone apps<br/>LoRa relay nodes<br/>WiFi/BLE mesh<br/>Gateway operators<br/><i>Users: mobile communities</i>"]
    P4["<b>Phase 4: Full Ecosystem</b><br/>Advanced compute<br/>Licensing<br/>Onion routing<br/>Protocol bridges<br/><i>Users: mature ecosystem</i>"]

    P1 --> P2 --> P3 --> P4
```

**Principle**: Start where the resources are. Servers have bandwidth, uptime, storage, and compute. Debug the protocol on reliable hardware over reliable links, then extend to constrained devices and radio. The free tier (trusted peer communication) is a complete product on its own. MHR tokens, economics, and advanced features come only after there are real nodes generating real traffic. Token follows utility, never leads it.

---

## Phase 1: Server Node (MVP)

**Focus**: A Linux daemon that lets servers join a decentralized network, providing compute, storage, and relay services over standard internet connections. The free tier only — no tokens, no payment.

**Why server first**: Servers have public IPs (or easily configured port forwarding), reliable uptime, abundant bandwidth, and real storage and compute resources to offer. TCP/IP transport is trivial compared to radio. This is the fastest path to a working protocol — debug routing, gossip, storage, and DHT on hardware that can actually run them well, without fighting radio propagation, phone OS restrictions, or constrained-device limitations. The `no_std` constraint for ESP32 can be added later; starting with the full Rust standard library makes development significantly faster.

### Milestone 1.1: Core Protocol Library (Rust)

- `NodeIdentity` (Ed25519 keypair generation, destination hash derivation, X25519 conversion)
- Link-layer encryption (X25519 ECDH + ChaCha20-Poly1305, counter-based nonces, key rotation)
- Packet framing (Reticulum-compatible: header, addresses, context, data)
- TCP transport interface (outbound connections, bidirectional links, keepalive)
- Announce generation and forwarding with Ed25519 signature verification

**Acceptance**: Two Linux nodes establish an encrypted link over TCP, exchange announces, and forward packets. Unauthenticated nodes are rejected.

### Milestone 1.2: Bootstrap + Peer Discovery

- DNS-based genesis gateway discovery (well-known domain resolves to genesis gateway IPs — primary bootstrap method)
- Hardcoded bootstrap node list as fallback (known IP:port pairs, including genesis gateway addresses)
- Peer exchange protocol (connected peers share their known peer lists)
- Outbound-only NAT traversal (nodes behind NAT connect outbound to bootstrap nodes; TCP connection is bidirectional once established)
- Peer persistence (remember previously connected peers across restarts)

**Acceptance**: A new node discovers the genesis gateway via DNS and connects to the network within 30 seconds. After 3 gossip rounds, the node has discovered peers beyond the genesis gateway. Fallback to hardcoded list works when DNS is unavailable. A node behind NAT connects outbound and participates fully as a relay. Restarting a node reconnects to previously known peers without hitting the bootstrap list.

### Milestone 1.3: Routing + Gossip

- `CompactPathCost` (6-byte encoding/decoding, log-scale math, relay update logic)
- Routing table (`RoutingEntry` with cost, latency, bandwidth, hop count, reliability)
- Greedy forwarding with `PathPolicy` scoring (Cheapest, Fastest, Balanced)
- Gossip protocol (60-second rounds, bloom filter state summaries, delta exchange)
- Bandwidth budget enforcement (4-tier allocation)
- Announce propagation rules (event-driven + 30-min refresh, hop limit, expiry, link failure detection)

**Acceptance**: A 10-node network converges routing tables within 3 gossip rounds. Packets are forwarded via cost-optimal paths. Removing a node causes re-routing within 3 minutes. Gossip overhead stays within 10% budget.

### Milestone 1.4: Trust Graph + Free Relay

- `TrustConfig` implementation (trusted peers, cost overrides, scopes)
- Free relay logic (sender trusted AND destination trusted → no lottery, no channels)
- Adding/removing trusted peers

**Acceptance**: Trusted peers relay traffic for free with zero economic overhead. The full relay stack works with zero tokens in circulation.

### Milestone 1.5: MHR-Store

- `DataObject` types (Immutable, Mutable, Ephemeral)
- Storage agreements (bilateral, free between trusted peers initially)
- Proof of storage (Blake3 Merkle challenge-response)
- Erasure coding (Reed-Solomon, default schemes by size)
- Repair protocol (detect failure → assess → reconstruct → re-store)
- Garbage collection (7-tier priority)
- Chunking (4 KB chunks, parallel retrieval, resumable transfers)

**Acceptance**: A node stores a DataObject with replication factor 3 across the network. Proof-of-storage challenges pass. Erasure coding reconstructs from k of n chunks. Chunked transfer resumes after interruption.

### Milestone 1.6: MHR-DHT + MHR-Pub

- DHT routing (XOR distance + cost weighting, α=0.7)
- k=3 replication with cost-bounded storage set
- Lookup and publication protocols
- Subscription types (Key, Prefix, Node, Scope)
- Delivery modes (Push, Digest, PullHint)
- Bandwidth-adaptive mode selection

**Acceptance**: A node publishes a DataObject and it's discoverable via DHT lookup from any node in the network. MHR-Pub delivers notifications to subscribers within 2 gossip rounds.

### Milestone 1.7: Linux Daemon + CLI

- `mehrd` daemon (background process, systemd service file)
- Configuration file (bootstrap peers, listen address, storage path, trust config)
- CLI tool (`mehr`) for node management:
  - `mehr status` — node identity, connected peers, routing table summary
  - `mehr peers` — list connected peers with link quality metrics
  - `mehr trust add/remove <destination_hash>` — manage trusted peers
  - `mehr store put/get <file>` — store and retrieve data objects
  - `mehr dht lookup <key>` — query the DHT
- Logging and metrics (structured logs, Prometheus-compatible metrics endpoint)

**Acceptance**: A sysadmin can install the daemon, configure bootstrap peers, and join the network. The CLI provides full visibility into node state. The daemon runs unattended and recovers from restarts.

### Phase 1 Deliverable

**A network of Linux servers providing decentralized storage and relay.** Install the daemon, configure a few bootstrap peers, and your server joins the network — contributing storage, relay bandwidth, and DHT capacity. Trust your friends' servers for free relay. This is the foundation everything else builds on.

**Target audiences**: homelabbers, self-hosting enthusiasts, VPS operators, privacy-conscious sysadmins, distributed systems developers.

---

## Phase 2: Economic Layer + Social

**Focus**: MHR token genesis, payment infrastructure, and social features. With real servers generating real traffic and providing real storage, economics can be validated against actual usage patterns.

### Milestone 2.1: Payment Channels

- VRF lottery implementation (ECVRF-ED25519-SHA512-TAI per RFC 9381)
- Adaptive difficulty (local per-link, formula: `win_prob = target_updates / observed_packets`)
- `ChannelState` (200 bytes, dual-signed, sequence-numbered)
- Channel lifecycle (open, update on win, settle, dispute with 2,880-round window, abandon after 4 epochs)
- `SettlementRecord` generation and dual-signature

**Acceptance**: Two nodes relay 1,000 packets. The relay wins the VRF lottery approximately `1000 × win_probability` times (within 2σ). Channel updates occur only on wins. Settlement produces a valid dual-signed record. Dispute resolution correctly rejects old states.

### Milestone 2.2: CRDT Ledger + Epoch Compaction

- `AccountState` (GCounter for earned/spent, GSet for settlements)
- GCounter merge (pointwise max per-node entries)
- GCounter rebase at epoch compaction (prevents overflow from money velocity)
- Settlement flow (validation: 2 sig checks + Blake3 hash + GSet dedup, gossip forward)
- Balance derivation (`earned - spent`, reject negative)
- Epoch trigger logic (3-trigger: settlement count, GSet size, small-partition adaptive)
- Epoch lifecycle (Propose → Acknowledge at 67% → Activate → Verify → Finalize)
- Merkle-tree snapshot (full tree — servers have the memory for it)
- `BalanceProof` generation and verification

**Acceptance**: A 20-node network triggers epochs correctly. Balances converge across the network. GCounter rebase keeps counters bounded.

### Milestone 2.3: Token Genesis + Demand-Backed Mining

- Emission schedule implementation (10^12 μMHR/epoch, halving every 100,000 epochs, shift clamp at 63)
- Tail emission floor (0.1% of circulating supply annually)
- Genesis allocation to genesis gateway operator (transparent, visible in ledger from epoch 0)
- Demand-backed minting eligibility (VRF wins count only on funded-channel traffic)
- Revenue-capped minting (`effective_minting = min(emission_schedule, 0.5 × total_channel_debits)`)
- `RelayWinSummary` aggregation per epoch (demand-backed wins only)
- Mint distribution proportional to verified VRF lottery wins
- Channel-funded relay payments (coexist with minting)

**Acceptance**: The first epoch mints MHR and distributes it to relay nodes. Genesis allocation is visible in the ledger. Distribution is proportional to demand-backed wins. Minting on unfunded-channel traffic is rejected. Revenue cap limits minting to 50% of channel debits. Minting and channel payments coexist. Token supply follows the emission schedule.

### Milestone 2.4: Reputation + Credit

- `PeerReputation` scoring (relay, storage, compute scores 0-10000)
- Score update formulas (success: diminishing gains, failure: 10% penalty)
- Trust-weighted referrals (1-hop, capped at 50%)
- Transitive credit (direct: full, friend-of-friend: 10%, 3+ hops: none)
- `CreditState` tracking per grantee
- Credit rate limiting per trust distance and per epoch

**Acceptance**: Reliable nodes build reputation. Credit extends through trust graph. A friend-of-friend gets exactly 10% of the direct credit line. Default handling absorbs debt correctly.

### Milestone 2.5: Paid Storage + Kickback

- StorageAgreement with payment channel integration (pay-per-duration)
- Kickback fields (revenue sharing between storage node and content author)
- Self-funding content detection (kickback exceeds storage cost)
- Content propagation through scope hierarchy

**Acceptance**: Storage nodes earn for hosting data. Kickback flows correctly on retrieval. Self-funding content persists without author payment.

### Milestone 2.6: Social Layer

- `PostEnvelope` (free layer) + `SocialPost` (paid layer) — mutable DataObjects
- `UserProfile` (display name, bio, avatar, scopes, claims)
- Hierarchical scopes (Geo + Topic) with scope matching
- Five feed types: follow, geographic, interest, intersection, curated
- `CuratedFeed` with curator kickback
- Publishing flow (post_id generation, envelope propagation)
- Editing (mutable DataObject semantics, sequence versioning)
- Replies, boosts, references
- MHR-Name (scope-based naming, conflict resolution, petnames)

**Acceptance**: A user publishes a post tagged with geographic and interest scopes. The post's envelope appears in subscribers' feeds. Readers pay to fetch full content. Kickback flows to author. Curated feeds work end-to-end.

### Milestone 2.7: MHR-Compute

- 47-opcode MHR-Byte interpreter implementation in Rust
- Cycle cost enforcement
- Resource limit enforcement (max_memory, max_cycles, max_state_size)
- WASM execution environment (Wasmtime, Light + Full tiers)
- Compute delegation via capability marketplace
- Reference test vector suite for cross-platform conformance

**Acceptance**: A compute contract executes on a server node. MHR-Byte and WASM produce identical results for the same inputs. Compute delegation routes requests to capable nodes. Cycle cost metering terminates runaway contracts.

### Milestone 2.8: Test Network

- Deploy a 20-50 node test network across multiple server operators
- Instrument for: routing convergence, gossip bandwidth, storage reliability, economic dynamics, social UX
- Run for at least 8 weeks
- Document: failure modes, parameter tuning, real-world performance, economic balance

**Acceptance**: Test network operates continuously for 8 weeks. Storage proofs pass reliably. Token economy reaches equilibrium. Published test report with metrics.

### Phase 2 Deliverable

**A functioning decentralized economy on Linux servers.** MHR tokens enter circulation through proof-of-service. Server operators earn by relaying traffic and hosting storage. Content creators earn through kickback. Compute delegation works. The economic layer is live, validated on real servers with real traffic.

---

## Phase 3: Mobile + Mesh

**Focus**: Bring the proven protocol to phones and mesh radio. The protocol is already battle-tested on servers — now extend it to constrained devices and transport-independent operation.

### Milestone 3.1: `no_std` Core Library

- Factor the core protocol library into `no_std`-compatible crate
- Separate transport-specific code (TCP) from protocol logic
- Verify all crypto operations work without `std` (Ed25519, X25519, ChaCha20, Blake3)
- Sparse Merkle-tree snapshots for constrained devices (under 5 KB)

**Acceptance**: The core protocol library compiles for `no_std` targets. All protocol-level tests pass on both `std` and `no_std` builds.

### Milestone 3.2: Phone Apps

- Android app (Kotlin/Rust FFI) — Android first for broader device support and sideloading
- iOS app (Swift/Rust FFI) — follows Android
- Contact management (add trusted peers via QR code, NFC, or manual key entry)
- Messaging UI (conversations, groups, media sending adapted to link quality)
- E2E encrypted messaging (store-and-forward, offline delivery)
- Group messaging with co-admin delegation
- Voice on WiFi links (Opus codec)
- Connect to server network over internet (phone as a light client)
- WiFi Direct and BLE transport for local phone-to-phone mesh
- Background mesh relay (phone relays traffic while in pocket)
- Multi-transport handoff (internet ↔ WiFi Direct ↔ BLE, seamless)

**Acceptance**: A non-technical user can install the app, add a friend via QR code, and exchange messages. The app connects to the server network over internet for relay and storage. Phone-to-phone WiFi Direct messaging works without internet. Voice calls work on WiFi links.

### Milestone 3.3: LoRa Transport

- LoRa interface implementation (SX1276/SX1262 via `no_std` Rust)
- Off-the-shelf hardware support:
  - Heltec WiFi LoRa 32 (~$15)
  - LILYGO T-Beam (~$25, with GPS)
  - RAK WisBlock (~$30, modular)
  - RNode (Reticulum-native)
- LTE-M and NB-IoT interface support (carrier-managed LPWAN)
- Multi-interface bridging (phone WiFi ↔ LoRa relay ↔ WiFi ↔ server network)
- Solar relay firmware (ESP32 L1: transport, routing, gossip — runs on $30 solar kit)
- Congestion control tuning for constrained links (CSMA/CA, backpressure)

**Acceptance**: A LoRa relay extends the network to areas without internet. A phone sends a message that hops: phone → WiFi → LoRa relay → WiFi → server → destination. Solar relay runs unattended for 30+ days.

### Milestone 3.4: Gateway Operators

- Gateway trust-based onboarding (add consumer to trusted_peers, extend credit)
- Fiat billing integration (subscription, prepaid, pay-as-you-go — gateway's choice)
- Cloud storage via gateway (consumer stores files, gateway handles MHR)
- Gateway-provided connectivity (ethernet ports, WiFi access points)

The [genesis service gateway](../economics/token-economics#genesis-service-gateway) is the first instance of this pattern — it bootstraps the economy in Phase 2. This milestone generalizes gateway mechanics for any operator to deploy.

**Acceptance**: A consumer signs up with a gateway, pays fiat, and uses the network without seeing MHR. Traffic flows through gateway trust. Consumer can switch gateways without losing identity.

### Milestone 3.5: Mesh Test Networks

- Deploy 3-5 physical test networks (urban, rural, campus, event)
- Each network: 10-50 nodes (phones + LoRa relays + server backbone) across at least 2 transports
- Instrument for: routing convergence, gossip bandwidth, storage reliability, mesh UX
- Run for at least 4 weeks per network
- Document: failure modes, parameter tuning, real-world performance

**Acceptance**: Test networks operate continuously for 4 weeks. Users report messaging and mesh features work reliably. Published test report with metrics.

### Phase 3 Deliverable

**The full network: servers as backbone, phones as endpoints, mesh radio for off-grid.** Phone users get encrypted messaging and social feeds backed by the server network's storage and compute. LoRa relays extend coverage to areas without internet. Gateway operators bridge fiat consumers to the mesh economy. The same protocol runs from $30 ESP32 to datacenter servers.

---

## Phase 4: Full Ecosystem

**Focus**: Advanced capabilities, application richness, and ecosystem growth.

### Milestone 4.1: Identity + Governance

- Identity claims and vouches (GeoPresence, CommunityMember, KeyRotation, Capability, ExternalIdentity)
- RadioRangeProof (geographic verification via LoRa beacons)
- Peer attestation and transitive confidence
- Vouch lifecycle (create, gossip, verify, renew, revoke)
- Voting prerequisites (geographic eligibility from verified claims)

### Milestone 4.2: Rich Applications

- Voice (Codec2 on LoRa, Opus on WiFi, bandwidth bridging, seamless handoff)
- Digital licensing (LicenseOffer, LicenseGrant, verification chain, off-network verifiability)
- Cloud storage (client-side encryption, erasure coding, sync between devices, file sharing)
- Forums (append-only logs, moderation contracts)
- Marketplace (listings, escrow contracts)

### Milestone 4.3: Interoperability + Privacy

- Third-party protocol bridges (SSB, Matrix, Briar) — [standalone gateway services](design-decisions#protocol-bridges-standalone-gateway-services) with identity attestation
- Onion routing implementation (`PathPolicy.ONION_ROUTE`, per-packet layered encryption)
- Private compute tiers (Split Inference, Secret Sharing, TEE)

### Milestone 4.4: Ecosystem Growth

- Developer SDK and documentation
- Community-driven capability development
- Hardware partnerships and reference design refinement (informed by real deployment data)
- Custom hardware (only if demand justifies — let usage data guide form factors)

### Phase 4 Deliverable

A full-featured decentralized platform with rich applications, privacy-enhanced routing, identity governance, and interoperability with existing protocols.

---

## Implementation Language

The primary implementation language is **Rust**, chosen for:

- Memory safety without garbage collection (critical for embedded targets)
- `no_std` support for ESP32 firmware (added in Phase 3)
- Strong ecosystem for cryptography and networking
- Single codebase from microcontroller to server
- FFI to Kotlin (Android) and Swift (iOS) for phone apps

Phase 1-2 use the full standard library. The `no_std` factoring happens at the start of Phase 3 when embedded targets are introduced.

## Platform Targets

| Platform | Implementation | Phase |
|----------|---------------|-------|
| Linux server / desktop | Rust native daemon + CLI (full node) | Phase 1 |
| Raspberry Pi / Linux SBC | Rust native (bridge, gateway, storage) | Phase 1-2 |
| Android phone | Rust core + Kotlin UI via FFI | Phase 3 |
| iOS phone | Rust core + Swift UI via FFI | Phase 3 |
| ESP32 + LoRa | Rust `no_std` (L1 relay) | Phase 3 |

All implementations speak the same wire protocol and interoperate on the same network.

## Bootstrap Strategy

New nodes discover the network through a layered bootstrap mechanism:

1. **Hardcoded bootstrap list**: The daemon ships with a list of known bootstrap node IP:port pairs. These are well-connected, high-uptime servers operated by early network participants.
2. **DNS bootstrap**: A well-known domain resolves to current bootstrap node IPs. This allows updating the bootstrap list without software releases.
3. **Peer exchange**: Once connected to any node, the gossip protocol discovers the rest of the network. Connected peers share their known peer lists.
4. **Peer persistence**: Previously connected peers are remembered across restarts, so a node that has been online before rarely needs the bootstrap list.

The bootstrap list is a starting point, not a dependency. After initial connection, the announce mechanism and gossip protocol handle all peer discovery. Bootstrap nodes have no special protocol role — they are ordinary nodes that happen to be well-known.

## NAT Traversal

Most servers targeted in Phase 1 have public IPs or easily configured port forwarding. For nodes behind NAT:

- **Outbound TCP connections** traverse NAT automatically. A node behind NAT connects outbound to a known peer; the TCP connection is bidirectional once established. This covers the vast majority of home server setups.
- **Relay forwarding**: A node that cannot accept inbound connections is still reachable — traffic routes through peers that have direct links to it. This is standard Mehr relay operation.
- **UPnP/NAT-PMP** (optional): Automatic port forwarding on consumer routers, attempted on startup.
- **UDP hole punching** (Phase 3+): For phone-to-phone mesh scenarios where both parties are behind NAT.

No STUN/TURN servers are required. The Mehr relay mechanism itself provides the relay function that TURN would otherwise fill.

## Test Network Strategy

Real distributed test networks, not simulations:

- Phase 1 test networks are server-only (TCP/IP over internet)
- Phase 2 test networks enable economics and measure token dynamics on real traffic
- Phase 3 test networks add phones and LoRa relays, validating the full transport range
- Each test network should represent a different deployment scenario
- Test networks validate both the protocol and the user experience

## Why This Order

The previous roadmap (phone-mesh-first) would require solving radio transport, constrained-device limitations, phone OS restrictions, and `no_std` compatibility before any protocol logic could be tested. This roadmap gets the protocol running first on hardware where development is fast, then extends to harder targets:

| Phase | What Users Get | What the Network Gets |
|-------|---------------|---------------------|
| 1 | Decentralized storage + relay on Linux servers | Real nodes, real traffic, battle-tested protocol |
| 2 | Token economy, social feeds, compute | Real economic data, real market pricing |
| 3 | Phone apps, mesh radio, off-grid operation | Mobile users, mesh coverage, transport diversity |
| 4 | Rich apps, privacy, interoperability | Mature ecosystem |

Each phase is viable on its own. Phase 1 is a useful product even if phones never ship — a decentralized storage and relay network for server operators. Phase 2 adds a functioning economy. Phase 3 extends to mobile and mesh. No phase depends on token speculation or hardware manufacturing for its value.

**Key advantage**: By the time Phase 3 tackles constrained devices and radio, the protocol is already proven. Bugs in routing, gossip, storage, DHT, and economics have been found and fixed on servers where debugging is easy. The `no_std` factoring is a well-scoped engineering task applied to known-good code, not a simultaneous protocol design and embedded engineering challenge.

## Implementability Assessment

Phase 1 is **fully implementable** with the current specification. All protocol-level gaps have been resolved:

| Component | Spec Status | Key References |
|-----------|------------|----------------|
| Identity + Encryption | Complete | [Security](../protocol/security) |
| Packet format + CompactPathCost | Complete (wire format specified) | [Network Protocol](../protocol/network-protocol#mehr-extension-compact-path-cost) |
| Routing + Announce propagation | Complete (scoring, announce rules, expiry, failure detection) | [Network Protocol](../protocol/network-protocol#routing) |
| Gossip protocol | Complete (bloom filter, bandwidth budget, 4-tier) | [Network Protocol](../protocol/network-protocol#gossip-protocol) |
| Congestion control | Complete (3-layer, priority levels, backpressure) | [Network Protocol](../protocol/network-protocol#congestion-control) |
| Trust neighborhoods | Complete (free relay, credit, scopes) | [Trust & Neighborhoods](../economics/trust-neighborhoods) |
| MHR-Store | Complete (agreements, proofs, erasure coding, repair) | [MHR-Store](../services/mhr-store) |
| MHR-DHT + MHR-Pub | Complete (routing, replication, subscriptions) | [MHR-DHT](../services/mhr-dht), [MHR-Pub](../services/mhr-pub) |
| VRF lottery + Payment channels | Complete (RFC 9381, difficulty formula, channel lifecycle) | [Payment Channels](../economics/payment-channels) |
| CRDT ledger + Settlement | Complete (validation rules, GCounter merge, GSet dedup, rebase) | [CRDT Ledger](../economics/crdt-ledger) |

---

### Design Decisions
<!-- Source: docs/development/design-decisions.md -->

# Design Decisions Log

This page documents the key architectural decisions made during Mehr protocol design, including alternatives considered and the rationale for each choice.

## Network Stack: Reticulum as Initial Transport

| | |
|---|---|
| **Chosen** | Use [Reticulum Network Stack](https://reticulum.network/) as the initial transport implementation; treat it as a [swappable layer](#transport-layer-swappable-implementation) |
| **Alternatives** | Clean-room implementation from day one, libp2p, custom protocol |
| **Rationale** | Reticulum already solves transport abstraction, cryptographic identity, mandatory encryption, sender anonymity (no source address), and announce-based routing — all proven on LoRa at 5 bps. Mehr extends it with CompactPathCost annotations and economic primitives rather than reinventing a tested foundation. The transport layer is an implementation detail: Mehr defines the interface it needs and can switch to a clean-room implementation in the future without affecting any layer above. |

## Routing: Kleinberg Small-World with Cost Weighting

| | |
|---|---|
| **Chosen** | Greedy forwarding on a Kleinberg small-world graph with cost-weighted scoring |
| **Alternatives** | Pure Reticulum announce model, Kademlia, BGP-style routing, Freenet-style location swapping |
| **Rationale** | The physical mesh naturally forms a small-world graph: short-range radio links serve as lattice edges, backbone/gateway links serve as Kleinberg long-range contacts. Greedy forwarding achieves O(log² N) expected hops — a formal scalability guarantee. Cost weighting trades path length for economic efficiency. Unlike Freenet, no location swapping is needed because destination hashes are self-assigned and Reticulum announcements build the navigable topology. |

## Payment: Stochastic Relay Rewards

| | |
|---|---|
| **Chosen** | Probabilistic micropayments via VRF-based lottery (channel update only on wins) |
| **Alternatives** | Per-packet accounting, per-minute batched accounting, subscription-based, random-nonce lottery |
| **Rationale** | Per-packet and batched payment require frequent channel state updates, consuming ~2-4% bandwidth on LoRa links. Stochastic rewards achieve the same expected income but trigger updates only on lottery wins — reducing economic overhead by ~10x. Adaptive difficulty ensures fairness across traffic levels. The law of large numbers guarantees convergence for active relays. **The lottery uses a VRF (ECVRF-ED25519-SHA512-TAI, RFC 9381)** rather than a random nonce to prevent relay nodes from grinding nonces to win every packet. The VRF produces exactly one verifiable output per (relay key, packet) pair, reusing the existing Ed25519 keypair. |

## Settlement: CRDT Ledger

| | |
|---|---|
| **Chosen** | CRDT ledger (GCounters + GSet) |
| **Alternatives** | Blockchain, federated sidechain |
| **Rationale** | Partition tolerance is non-negotiable. CRDTs converge without consensus. A blockchain requires global ordering, which is impossible when network partitions are expected operating conditions. **Tradeoff**: double-spend prevention is probabilistic, not perfect. Mitigated by channel deposits, credit limits, reputation staking, and blacklisting — making cheating economically irrational for micropayments. |

## Communities: Emergent Trust Neighborhoods

| | |
|---|---|
| **Chosen** | Trust graph with emergent neighborhoods (no explicit zones) |
| **Alternatives** | Explicit zones with admin keys and admission policies |
| **Rationale** | Explicit zones require someone to create and manage them — centralized thinking in decentralized clothing. They impose UX burden and artificially fragment communities. Trust neighborhoods emerge naturally from who you trust: free communication between trusted peers, paid between strangers. No admin, no governance, no admission policies. Communities form the same way they form in real life — through relationships, not administrative acts. The trust graph provides Sybil resistance economically (vouching peers absorb debts). |

## Compaction: Epoch Checkpoints with Bloom Filters

| | |
|---|---|
| **Chosen** | Epoch checkpoints with bloom filters |
| **Alternatives** | Per-settlement garbage collection, TTL-based expiry |
| **Rationale** | The settlement GSet grows without bound. Bloom filters at 0.01% FPR compress 1M settlement hashes from ~32 MB to ~2.4 MB. A settlement verification window during the grace period recovers any settlements lost to false positives. Epochs are triggered by settlement count (~10,000 batches), not wall-clock time, for partition tolerance. |

## Compute Contracts: MHR-Byte

| | |
|---|---|
| **Chosen** | MHR-Byte (minimal bytecode, ~50 KB interpreter) |
| **Alternatives** | Full WASM everywhere |
| **Rationale** | ESP32 microcontrollers can't run a WASM runtime. MHR-Byte provides basic contract execution on even the most constrained devices. WASM is offered as an optional capability on nodes with sufficient resources. |

## Encryption: Ed25519 + X25519

| | |
|---|---|
| **Chosen** | Ed25519 for identity/signing, X25519 for key exchange (Reticulum-compatible) |
| **Alternatives** | RSA, symmetric-only |
| **Rationale** | Ed25519 has 32-byte public keys (compact for radio), fast signing/verification, and is widely proven. X25519 provides efficient Diffie-Hellman key exchange. Compatible with Reticulum's crypto model. RSA keys are too large for constrained links. |

## Source Privacy: No Source Address (Default)

| | |
|---|---|
| **Chosen** | No source address in packet headers (inherited from Reticulum) as the default; [opt-in onion routing](#onion-routing-per-packet-layered-encryption-opt-in) for high-threat environments |
| **Alternatives** | Mandatory onion routing for all traffic |
| **Rationale** | Onion routing adds 21% payload overhead on LoRa — unacceptable as a default for all traffic. Omitting the source address is free and effective against casual observation. Per-packet layered encryption is available opt-in via `PathPolicy.ONION_ROUTE` for users who need stronger traffic analysis resistance. |

## Naming: Scope-Based, No Global Namespace

| | |
|---|---|
| **Chosen** | Hierarchical-scope-based names (e.g., `alice@geo:us/oregon/portland`) |
| **Alternatives** | Global names via consensus, flat community labels (`alice@portland-mesh`) |
| **Rationale** | Global consensus contradicts partition tolerance. Flat community labels were replaced by [hierarchical scopes](../economics/trust-neighborhoods#hierarchical-scopes) — geographic (`Geo`) and interest (`Topic`) — which provide structured resolution. Names resolve against scope hierarchy: `alice@geo:portland` queries Portland scope first, then broadens. Two different cities named "portland" are disambiguated by longer paths (`alice@geo:us/oregon/portland` vs `alice@geo:us/maine/portland`). Proximity-based resolution handles most cases naturally. Local petnames provide a fallback. |

## Cost Annotations: Compact Path Cost (No Per-Relay Signatures)

| | |
|---|---|
| **Chosen** | 6-byte constant-size `CompactPathCost` (running totals updated by each relay, no per-relay signatures) |
| **Alternatives** | Per-relay signed CostAnnotation (~84 bytes per hop), aggregate signatures, signature-free with Merkle proof |
| **Rationale** | Per-relay signatures make announces grow linearly with path length — 84 bytes × N hops. On a 1 kbps LoRa link with 3% routing budget, this limits convergence to ~1 announce per 22+ seconds. CompactPathCost uses 6 bytes total regardless of path length: log-encoded cumulative cost, worst-case latency, bottleneck bandwidth, and hop count. Per-relay signatures are unnecessary because routing decisions are local (you trust your link-authenticated neighbor), trust is transitive at each hop, and the market enforces honesty (overpriced relays get routed around, underpriced relays lose money). The announce itself remains signed by the destination node. |

## Congestion Control: Three-Layer Design

| | |
|---|---|
| **Chosen** | Link-level CSMA/CA + per-neighbor token bucket + 4-level priority queuing + economic cost response |
| **Alternatives** | Pure CSMA/CA only, rigid TDMA, end-to-end TCP-style congestion control |
| **Rationale** | A single mechanism is insufficient across the bandwidth range (500 bps to 10 Gbps). CSMA/CA handles collision avoidance on half-duplex radio. Token buckets enforce fair sharing across neighbors. Priority queuing ensures real-time traffic (voice) isn't starved by bulk transfers. Economic cost response (quadratic cost increase under congestion) signals scarcity through the existing cost routing mechanism, causing natural traffic rerouting without new protocol extensions. End-to-end congestion control (TCP-style) is wrong for a mesh — the bottleneck is typically a single constrained link, and hop-by-hop control responds faster. Rigid TDMA wastes bandwidth when some slots are unused. |

## Transport Layer: Swappable Implementation

| | |
|---|---|
| **Chosen** | Define transport as an interface with requirements; use Reticulum as the current implementation |
| **Alternatives** | Hard dependency on Reticulum, clean-room from day one |
| **Rationale** | Reticulum provides a proven transport layer tested at 5 bps on LoRa, saving significant implementation effort. But coupling Mehr to Reticulum's codebase or community roadmap creates fragility. Instead, Mehr defines the transport interface it needs (transport-agnostic links, announce-based routing, mandatory encryption, sender anonymity) and uses Reticulum as the current implementation. Mehr extensions are carried as opaque payload in the announce DATA field — a clean separation. Three participation levels (L0 transport-only, L1 Mehr relay, L2 full Mehr) ensure interoperability with the underlying transport. This allows a future clean-room implementation without affecting any layer above transport. |

## Storage: Pay-Per-Duration with Erasure Coding

| | |
|---|---|
| **Chosen** | Bilateral storage agreements, pay-per-duration, Reed-Solomon erasure coding, lightweight challenge-response proofs |
| **Alternatives** | Filecoin-style PoRep/PoSt with on-chain proofs; Arweave-style one-time-payment permanent storage; simple full replication |
| **Rationale** | Filecoin's Proof of Replication requires GPU-level computation (minutes to seal a sector) — impossible on ESP32 or Raspberry Pi. Arweave's permanent storage requires a blockchain endowment model and assumes perpetually declining storage costs. Both require global consensus. Mehr uses simple Blake3 challenge-response proofs (verifiable in under 10ms on ESP32) and bilateral agreements settled via payment channels. Erasure coding (Reed-Solomon) provides the same durability as 3x replication at 1.5x storage overhead. The tradeoff: we can't prove a node stores data *uniquely* (no PoRep), but we can prove it stores data *at all* — and the data owner doesn't care how the node organizes its disk. |

## Mobile Handoff: Presence Beacons + Credit-Based Fast Start

| | |
|---|---|
| **Chosen** | Transport-agnostic presence beacons, credit-based fast start, roaming cache with channel preservation |
| **Alternatives** | Pre-negotiated handoff (cellular-style), pure re-discovery from scratch, always-connected overlay |
| **Rationale** | Cellular handoff requires a central controller — incompatible with decentralized mesh. Pure re-discovery works but is slow for latency-sensitive sessions. Presence beacons (20 bytes, broadcast every 10 seconds on any interface) let mobile nodes passively discover local relays before connecting. Credit-based fast start allows immediate relay if the mobile node has visible CRDT balance, while the payment channel opens in the background. The roaming cache preserves channels for previously visited areas, enabling zero-latency reconnection on return. No explicit teardown needed — old agreements expire via `valid_until`. |

## Light Client Trust: Content Verification + Multi-Source Queries

| | |
|---|---|
| **Chosen** | Three-tier verification: content-hash check (Tier 1), owner-signature check (Tier 2), multi-source queries (Tier 3) |
| **Alternatives** | Merkle proofs over DHT state, SPV-style state root verification, full DHT replication on clients |
| **Rationale** | Merkle proofs require a global state root, which contradicts partition tolerance (no consensus). SPV-style verification has the same problem. Full DHT replication is too bandwidth-heavy for phones. Instead, content addressing (Tier 1) gives zero-overhead verification for the most common case — `Blake3(data) == key` proves authenticity regardless of relay honesty. Signed objects (Tier 2) prevent forgery of mutable data. Multi-source queries (Tier 3, N=2-3 independent nodes) detect censorship and staleness. Trusted relays skip to single-source for all tiers. Overhead is minimal: at most one extra 192-byte query for critical lookups via untrusted relays. |

## Epoch Triggers: Adaptive Multi-Criteria

| | |
|---|---|
| **Chosen** | Three-trigger system: settlement count ≥ 10,000 (large mesh), GSet size ≥ 500 KB (memory pressure), or partition-proportional threshold with gossip-round floor (small partitions) |
| **Alternatives** | Fixed 10,000-settlement-only trigger, wall-clock timer, per-node independent compaction |
| **Rationale** | A 20-node village on LoRa with low traffic might take months to reach 10,000 settlements. ESP32 nodes (520 KB usable RAM) would exhaust memory first. The adaptive trigger fires at max(200, active_set × 10) settlements with a 1,000-gossip-round floor (~17 hours), keeping GSet under ~6.4 KB for small partitions. The 500 KB GSet size trigger is a safety net regardless of partition size. Proposer eligibility adapts too — only 3 direct links needed (not 10) in small partitions. Wall-clock timers were rejected because they require clock synchronization. Per-node compaction was rejected because it fragments global state. |

## Ledger Scaling: Merkle-Tree Snapshots with Sparse Views

| | |
|---|---|
| **Chosen** | Merkle-tree account snapshot; full tree on backbone nodes, sparse view + Merkle root on constrained devices, on-demand balance proofs (~640 bytes) |
| **Alternatives** | Flat snapshot everywhere, sharded epochs, neighborhood-only ledgers |
| **Rationale** | At 1M nodes the flat snapshot is ~32 MB — unworkable on ESP32 or phones. Sharded epochs fragment the ledger and complicate cross-shard verification. Neighborhood-only ledgers lose global balance consistency. A Merkle tree over the sorted account snapshot gives the best of both: backbone nodes store the full tree (32 MB, feasible on SSD), constrained devices store only their own balance + channel partners + trust neighbors (~1.6 KB for 50 accounts) plus the Merkle root. Any balance can be verified on demand with a ~640-byte proof (20 tree levels × 32 bytes). No global state transfer needed. |

## Transforms/Inference: Compute Capability, Not Protocol Primitive

| | |
|---|---|
| **Chosen** | STT/TTS/inference as compute capabilities in the marketplace |
| **Alternatives** | Dedicated transform layer |
| **Rationale** | Speech-to-text, translation, and other transforms are just compute. Making them protocol primitives over-engineers the foundation. The capability marketplace already handles discovery, negotiation, execution, verification, and payment for arbitrary compute functions. Compute execution is opaque — the protocol does not verify the compute method, only the input/output. Verification strategies (reputation, redundant execution, spot-checking) are consumer-side choices, not protocol enforcement. |

## Group Admin: Delegated Co-Admin (No Threshold Signatures)

| | |
|---|---|
| **Chosen** | Delegated co-admin model: group creator signs delegation certificates for up to 3 co-admins; any co-admin can independently rotate keys and manage members |
| **Alternatives** | Threshold signatures (e.g., 2-of-3 Schnorr multisig), single admin only, leaderless consensus |
| **Rationale** | Threshold signatures (Schnorr multisig, FROST) require multi-round key generation and signing protocols that are too expensive for ESP32 and too complex for LoRa latency. Leaderless consensus contradicts the "no global coordination" principle. Instead, the group creator signs `CoAdminCertificate` records (admin public key + permissions + sequence number, ~128 bytes each) that authorize up to 3 co-admins. Any co-admin can independently add/remove members, rotate the group symmetric key, and promote/demote other co-admins (if authorized). Operations are sequence-numbered; conflicts are resolved by highest sequence number, ties broken by lowest admin public key hash. Overhead is one Ed25519 signature per admin action — no new cryptographic primitives needed. If all admins go offline, the group continues functioning with its current key; no key rotation or membership changes occur until at least one admin returns. |

## Reputation: Bounded Trust-Weighted Referrals

| | |
|---|---|
| **Chosen** | First-hand scores primary; 1-hop trust-weighted referrals as advisory bootstrap for unknown peers, capped and decaying |
| **Alternatives** | Pure first-hand only (no referrals), full gossip-based reputation, global reputation aggregation |
| **Rationale** | Pure first-hand scoring means new nodes have zero information about any peer until direct interaction — leading to poor initial routing and credit decisions. Full reputation gossip enables manipulation: a colluding cluster can flood the network with inflated scores. Global aggregation requires consensus. The chosen design: trusted peers (direct trust edges) can share their first-hand scores as referrals. Referrals are weighted by the querier's trust in the referrer (trust_score / max_score × 0.3) and capped at 50% of max reputation — a referral alone cannot make a peer fully trusted. Only 1-hop referrals are accepted (no transitive gossip), limiting the manipulation surface to corruption of direct trusted peers, which already breaks the trust model. Referral scores are advisory: they initialize a peer's reputation but are overwritten by first-hand experience after the first few direct interactions. Referrals expire after 500 gossip rounds (~8 hours) without refresh. This helps new nodes bootstrap while keeping the attack surface minimal. |

## Onion Routing: Per-Packet Layered Encryption (Opt-In)

| | |
|---|---|
| **Chosen** | Per-packet layered encryption via `PathPolicy.ONION_ROUTE`, stateless relays, 3 hops default, optional cover traffic |
| **Alternatives** | Circuit-based onion routing (Tor-style), mix networks, no onion routing (status quo) |
| **Rationale** | Circuit-based onion routing requires multi-round-trip circuit establishment — on a 1 kbps LoRa link with 2-second round trips, building a 3-hop circuit takes ~12 seconds before any data flows. Mix networks add latency by design (batching and reordering), which is unacceptable for interactive messaging. Per-packet layered encryption has zero setup: the sender wraps the message in N encryption layers (default N=3), each layer containing the next-hop destination hash and the inner ciphertext. Each relay decrypts one layer, reads the next hop, and forwards. No circuit state on relays — each packet is independently routable. Overhead: 32 bytes per hop (16-byte nonce + 16-byte Poly1305 tag), so 3 hops = 96 bytes, leaving 369 of 465 usable bytes on LoRa (~21% overhead). Relay selection: sender picks from known relays, requiring at least one relay outside the sender's trust neighborhood. Optional constant-rate cover traffic (1 dummy packet/minute, off by default) provides additional resistance to timing analysis on high-threat links. Onion routing is opt-in and not recommended for real-time voice (latency and payload overhead). The existing no-source-address design remains the default privacy layer for most traffic. |

## MHR-Byte: 47 Opcodes with Reference Interpreter

| | |
|---|---|
| **Chosen** | 47 opcodes in 7 categories, reference interpreter in Rust, cycle costs calibrated to ESP32 |
| **Alternatives** | Formal specification (Yellow Paper-style), minimal 20-opcode set, EVM-compatible opcode set |
| **Rationale** | A formal specification (Ethereum Yellow Paper style) would freeze the design prematurely — the opcode set needs real-world usage feedback before committing to a formal spec. An EVM-compatible set imports unnecessary complexity (256-bit words, gas semantics) that doesn't fit constrained devices. A minimal 20-opcode set omits crypto and system operations needed for the core use cases. The chosen 47 opcodes cover 7 categories: **Stack** (6): PUSH, POP, DUP, SWAP, OVER, ROT. **Arithmetic** (9): ADD, SUB, MUL, DIV, MOD, NEG, ABS, MIN, MAX — all 64-bit integer, overflow traps. **Bitwise** (6): AND, OR, XOR, NOT, SHL, SHR. **Comparison** (6): EQ, NEQ, LT, GT, LTE, GTE. **Control** (7): JMP, JZ, JNZ, CALL, RET, HALT, ABORT. **Crypto** (3): HASH (Blake3), VERIFY_SIG (Ed25519), VERIFY_VRF. **System** (10): BALANCE, SENDER, SELF, EPOCH, TRANSFER, LOG, LOAD, STORE, MSIZE, EMIT. Cycle costs are tiered: stack/arithmetic/bitwise/comparison (1–3 cycles), control (2–5), memory (2–3), crypto (500–2000), system (10–50). ESP32 is the reference platform (~1 μs per base cycle). Faster hardware executes more cycles per second but charges the same cycle cost — gas price in μMHR/cycle is set by each compute provider. A comprehensive test vector suite ensures cross-platform conformance. Formal specification is a post-stabilization goal. |

## Emission Schedule: Epoch-Counted Discrete Halving

| | |
|---|---|
| **Chosen** | Initial reward of 10^12 μMHR per epoch, discrete halving every 100,000 epochs, tail emission floor at 0.1% of circulating supply annualized |
| **Alternatives** | Continuous exponential decay, wall-clock halving (every 2 calendar years), fixed perpetual emission |
| **Rationale** | Wall-clock halving requires clock synchronization, which contradicts partition tolerance — partitioned nodes would disagree on the current halving period. Continuous decay is mathematically elegant but harder to reason about and implement correctly on integer-only constrained devices. Fixed perpetual emission provides no scarcity signal. Discrete halving every 100,000 epochs is epoch-counted (partition-safe), easy to compute (bit-shift), and predictable. At an estimated ~1 epoch per 10 minutes (varies with network activity since epochs are settlement-triggered), 100,000 epochs ≈ 1.9 years — close to the original 2-year target. Initial reward of 10^12 μMHR/epoch yields ~1.5% of the supply ceiling minted in the first halving period, providing strong bootstrap incentive. Tail emission activates when the halved reward drops below `0.001 × circulating_supply / estimated_epochs_per_year` (trailing 1,000-epoch moving average of epoch frequency). This ensures perpetual relay incentive while keeping inflation negligible. Partition minting interaction: each partition mints `(local_active_relays / estimated_global_relays) × epoch_reward`; on merge, the winning epoch's `epoch_balance` snapshot is adopted and the losing partition's settlements are recovered via settlement proofs, preserving individual balances. Overminting is bounded by the existing partition_scale_factor tolerance (max 1.5×). |

## Protocol Bridges: Standalone Gateway Services

| | |
|---|---|
| **Chosen** | Bridges as standalone gateway services advertising in the capability marketplace; one-way identity attestation; bridge operator bears Mehr-side costs |
| **Alternatives** | Bridges as MHR-Compute contracts, bridge as protocol-level primitive, no bridge design (defer entirely) |
| **Rationale** | MHR-Compute contracts are sandboxed with no I/O or network access — bridges require persistent connections to external protocols (SSB replication, Matrix homeserver federation, Briar Tor transport). A protocol-level primitive over-engineers the foundation for what is fundamentally a gateway service. Bridges run as standalone processes that advertise their bridging capability in the marketplace like any other service. **Identity mapping**: Users create signed bridge attestations linking their Mehr Ed25519 key to their external identity. The bridge stores these attestations and handles translation. No global identity registry — the bridge is the only entity that knows the mapping. External users see messages as coming from the bridge identity in the external protocol. **Payment model**: Mehr-to-external traffic — the Mehr sender pays the bridge operator via a standard marketplace agreement. External-to-Mehr traffic — the bridge operator pays Mehr relay costs and recoups via the external protocol's economics (or operates as a public good). This keeps the Mehr economic model clean: the bridge is just another service consumer/provider. Bridge operators can support multiple protocols simultaneously and set their own pricing. |

## WASM Sandbox: Wasmtime with Tiered Profiles

| | |
|---|---|
| **Chosen** | Wasmtime (Bytecode Alliance) as WASM runtime; two tiers: Light (Community, 16 MB / 10^8 fuel / 5s) and Full (Gateway+, 256 MB / 10^10 fuel / 30s); 10 host imports mirroring MHR-Byte System opcodes |
| **Alternatives** | Wasmer, custom interpreter, single WASM profile for all devices |
| **Rationale** | Wasmtime is Rust-native (matches the reference implementation language), provides fuel-based metering that maps directly to MHR-Byte cycle accounting, and supports AOT compilation on Gateway+ nodes. Wasmer has a broader language ecosystem but less tight Rust integration. A custom interpreter would duplicate Wasmtime's battle-tested sandboxing. A single WASM profile either excludes Community-tier devices (too high limits) or handicaps Gateway nodes (too low limits). Two tiers match the natural hardware split: Pi Zero 2W (512 MB RAM, interpreted Cranelift) vs. Pi 4/5+ (4-8 GB, AOT). Host imports are restricted to 10 functions mirroring MHR-Byte System opcodes — no filesystem, network, clock, or RNG access. WASM execution remains pure and deterministic, enabling the same verification methods as MHR-Byte. Contracts exceeding Light limits are automatically delegated to a more capable node via compute delegation. |

## Presence Beacon: 8-Bit Capability Bitfield

| | |
|---|---|
| **Chosen** | 8 assigned capability bits in 16-bit field; bits 8-15 reserved for future use |
| **Alternatives** | Variable-length capability list, TLV-encoded capabilities, separate beacon per capability |
| **Rationale** | The beacon must fit in 20 bytes total and broadcast every 10 seconds on LoRa — every byte matters. A 16-bit bitfield encodes up to 16 boolean capabilities in 2 bytes, zero parsing overhead. The 8 assigned bits cover all current service types: relay (L1+), gateway, storage, compute-byte, compute-wasm, pubsub, DHT, and naming. Bits 8-15 are reserved (must be zero) for future services like inference, bridge, etc. Variable-length lists or TLV encoding would bloat the beacon and complicate parsing on ESP32. Separate beacons per capability would multiply broadcast bandwidth. |

## Ring 1 Discovery: CapabilitySummary Format

| | |
|---|---|
| **Chosen** | 8-byte `CapabilitySummary` per capability type: type (u8), count (u8), min/avg cost (u16 each, log₂-encoded), min/max hops (u8 each) |
| **Alternatives** | Full capability advertisements forwarded, Bloom filter of providers, free-text summaries |
| **Rationale** | Ring 1 gossips summaries every few minutes — bandwidth must stay under ~50 bytes per round. At 8 bytes per type and typically 5-6 types present in a 2-3 hop neighborhood, the total is 40-48 bytes — within budget even on LoRa. Forwarding full capability advertisements would scale linearly with provider count. Bloom filters compress provider identity but lose cost/distance information needed for routing decisions. The log₂-encoded cost fields match `CompactPathCost` encoding, keeping the representation consistent across the protocol. Count is capped at 255 (u8) — sufficient since Ring 1 covers only 2-3 hops. |

## DHT Metadata: 129-Byte Signed Entries

| | |
|---|---|
| **Chosen** | 129-byte `DHTMetadata`: 32-byte Blake3 key, u32 size, u8 content_type (Immutable/Mutable/Ephemeral), 16-byte owner, u32 TTL, u64 Lamport timestamp, 64-byte Ed25519 signature |
| **Alternatives** | Minimal key-only gossip (no metadata), full object gossip, variable-length metadata with optional fields |
| **Rationale** | DHT publication gossips metadata to let storage-set nodes decide whether to pull full data. Key-only gossip forces blind pulls — wasting bandwidth on unwanted or expired data. Full object gossip floods the gossip channel with arbitrarily large payloads. The 129-byte fixed format includes everything a storage node needs to decide: content hash (for deduplication), size (for storage budgeting), content type (for cache policy), owner (for mutable-object freshness ordering), TTL (for garbage collection), and Lamport timestamp (for mutable conflict resolution). The Ed25519 signature prevents metadata forgery — a node cannot falsely advertise objects it doesn't own. Content hash prevents data forgery on pull. For mutable objects, highest Lamport timestamp with valid signature wins; for immutable objects, the content hash is the sole arbiter. Cache invalidation is TTL-based with no push-invalidation — keeping the protocol simple and partition-tolerant. |

## Negotiation Protocol: Single-Round Take-It-or-Leave-It

| | |
|---|---|
| **Chosen** | Single round-trip negotiation: consumer sends `CapabilityRequest` (with desired cost, duration, proof preference, nonce); provider responds with `CapabilityOffer` (actual terms) or reject; consumer accepts or walks away. 30-second timeout. No counter-offers. |
| **Alternatives** | Multi-round bidding/auction, Dutch auction, sealed-bid auction, negotiation-free fixed pricing |
| **Rationale** | Multi-round negotiation is untenable on LoRa where each message takes seconds — a 3-round negotiation would take 30+ seconds before service begins. Auctions require multiple participants to discover each other simultaneously, which contradicts the bilateral, privacy-preserving nature of agreements. Fixed pricing (no negotiation) removes the consumer's ability to express budget constraints. The single-round protocol: the consumer states their maximum acceptable cost; the provider either meets it, undercuts it, or rejects. One round-trip, then service begins. The nonce in `CapabilityRequest` prevents replay attacks; the `request_nonce` echo in `CapabilityOffer` binds offer to request. Both messages are signed — the two signatures together form the `CapabilityAgreement`. If the consumer wants different terms, they send a new request with adjusted parameters — no counter-offer complexity. This keeps capability negotiation under 2 seconds on WiFi and under 10 seconds on LoRa. |

## Formal Verification: Priority-Ordered TLA+ Targets

| | |
|---|---|
| **Chosen** | TLA+ for concurrent protocol properties; 4-tier priority: (1) CRDT merge, (2) payment channels, (3) epoch checkpoints, (4) full composition deferred |
| **Alternatives** | Coq/Lean theorem proving, no formal verification (testing only), verify everything before v1.0 |
| **Rationale** | Coq/Lean have a steep learning curve that limits contributor access. Pure testing cannot prove absence of bugs in concurrent distributed protocols. Verifying everything delays launch indefinitely. TLA+ is battle-tested for distributed systems (used by Amazon for AWS services, by Microsoft for Azure) and has a practical learning curve. **Priority 1 — CRDT merge convergence** (must verify before v1.0): Prove commutativity, associativity, and idempotency of GCounter max-merge and GSet union. If merge diverges, the entire ledger is broken. **Priority 2 — Payment channel state machine** (must verify before v1.0): Prove no balance can go negative, dispute resolution always terminates within the challenge window, and channel states form a total order by sequence number. Direct financial impact if buggy. **Priority 3 — Epoch checkpoint correctness** (should verify): Prove no confirmed settlement is permanently lost after finalization, bloom filter false positive recovery covers all edge cases during the grace period. Property-based testing (QuickCheck-style) initially, formal TLA+ proof if resources allow. **Priority 4 — Full protocol composition** (defer to post-v1.0): Interaction between subsystems (e.g., channel dispute during epoch transition) is tracked as a long-term research goal. Individual component proofs provide sufficient confidence for launch. |

## Genesis Model: Transparent Service Gateway

| | |
|---|---|
| **Chosen** | Genesis Service Gateway + demand-backed minting + revenue-capped emission |
| **Alternatives** | Pure proof-of-service mining (no genesis), airdrop, ICO, DAO treasury |
| **Rationale** | VRF prevents grinding (one output per relay-packet pair) but does not prevent traffic fabrication: a Sybil attacker can fabricate traffic between colluding nodes and run the VRF lottery on fake packets. A genesis service gateway bootstraps the economy with real demand: a known trusted operator provides real services for fiat, creating funded payment channels that generate legitimate relay traffic. Revenue-capped minting (see below) ensures that even paying through the gateway and self-dealing is unprofitable. The genesis allocation is transparent (visible in ledger from epoch 0). DNS provides initial gateway discovery; the hardcoded bootstrap list serves as fallback. |

## Revenue-Capped Minting

| | |
|---|---|
| **Chosen** | `effective_minting = min(emission_schedule, 0.5 × total_channel_debits)` |
| **Alternatives** | Uncapped emission (vulnerable to self-dealing), fixed low emission (doesn't scale with network growth), proof-of-stake gating (concentrates power) |
| **Rationale** | The revenue cap guarantees that self-dealing is unprofitable at all traffic levels: an attacker spending Y MHR on relay fees can receive at most 0.5 × Y in minting rewards, regardless of their share of the network. This holds for any Y, any epoch, and any number of Sybil nodes. The cap also makes supply growth demand-responsive: during low-usage periods, actual minting is well below the emission schedule (supply grows slowly, preventing speculation). As the network matures and relay fees increase, the cap becomes non-binding and supply follows the standard halving schedule. Fixed low emission was rejected because it doesn't scale — a fixed amount becomes either too generous at low traffic or too stingy at high traffic. Proof-of-stake gating was rejected because it concentrates minting power among large holders. |

## Configurable Transitive Credit Ratio

| | |
|---|---|
| **Chosen** | User-configurable transitive credit ratio (0–50%, default 10%) |
| **Alternatives** | Fixed protocol constant (10%), fully uncapped user choice |
| **Rationale** | Direct credit is already configurable per-peer; transitive ratio should be too. The 50% ceiling prevents naive users from excessive exposure to friend-of-friend defaults. Different communities have different risk appetites — a tight-knit village may want 30% transitive credit, while an urban mesh with loose trust edges may prefer 5%. Per-peer overrides (`transitive_ratio_overrides`) allow fine-grained control. A fixed 10% was too rigid; fully uncapped allows users to set 100%, which could cascade defaults through the trust graph. |

## Single Geo Scope Per Node/Content

| | |
|---|---|
| **Chosen** | Max 1 Geo scope + up to 3 Topic scopes per node/content; total scope data ≤ 1 KB |
| **Alternatives** | Unconstrained (up to 8 of either type), total byte budget only (no count limit), no geo scopes on content |
| **Rationale** | Physical presence is singular — you are in one place at a time. Voting is geo-scoped, and multiple geo scopes would enable double-voting on governance issues. RadioRangeProof verifies one location; multiple geo claims would require multiple independent proofs. Interests are multi-dimensional but hierarchical prefix matching means a single deep tag (e.g., `Topic("gaming", "pokemon", "competitive")`) covers multiple query levels — 3 topic slots is expressive enough for most users. The 1 KB byte cap (down from ~2.1 KB for 8 scopes) halves the scope footprint in announces and envelopes, meaningful for ESP32 and LoRa. A pure byte budget (no count limit) was rejected because it creates a perverse incentive: deep, specific tags cost more bytes, penalizing the most useful scopes. A hard count + byte cap gives predictable allocation for constrained devices while letting the UI show "bytes remaining" (like character counts). |

## Per-Hop Independent Relay Rewards

| | |
|---|---|
| **Chosen** | Per-hop independent rewards (VRF lottery per relay, no end-to-end coordination) |
| **Alternatives** | End-to-end payment (sender pays all relays in one transaction), hybrid (per-hop with end-to-end settlement) |
| **Rationale** | End-to-end payment requires the sender to know the full path, which breaks sender anonymity — packets carry no source address by design. Per-hop stochastic rewards already achieve ~0.3% bandwidth overhead; end-to-end would save negligible additional bandwidth. Per-hop uses simple bilateral channels; end-to-end requires multi-hop payment routing (Lightning-style complexity). Each hop is independent — no coordination failure cascade. If one relay goes offline, only that hop is affected; upstream and downstream relays continue operating on their own bilateral channels. The hybrid approach adds complexity without meaningful benefit over pure per-hop. |

---

### Open Questions
<!-- Source: docs/development/open-questions.md -->

# Open Questions

All open questions from the v1.0 and v1.1 spec review rounds have been resolved with concrete specifications.

## Previously Resolved

### v1.1 Resolutions (11 Questions)

#### Architectural — Design Decisions

| # | Question | Resolution | Location |
|---|----------|-----------|----------|
| 1 | **Protocol Governance** — How does the protocol upgrade? | MEP (Mehr Enhancement Proposal) process with trust-weighted version signaling. Nodes signal MEP support via TLV extension in announces; acceptance at ≥67% trust-weighted support. Cross-fork compatibility via gateway bridges. Sunset clause prevents indefinite version limbo. | [Versioning — Governance](../protocol/versioning#governance) |
| 2 | **Secret Ballot Voting** — Commitment schemes on partition-prone mesh | Two-phase commit/reveal with partition-safe rules: reveal_period = 2× voting_period, 80% reveal threshold for valid tally, INCONCLUSIVE status for >20% unrevealed commits. Deliberate withhold penalized (0.5x weight after 3 consecutive non-reveals). Partition reconciliation via merged re-tally. | [Voting — Secret Ballot](../applications/voting#secret-ballot-protocol) |
| 3 | **Post-Quantum VRF** — No production-ready standard | Two-track approach: lattice-based VRF candidate when standardized; hash-chain committed lottery as fallback. The fallback uses epoch-committed secrets (not proof-of-work) — one Blake3 hash per packet, no grinding. Per-channel migration via existing cryptographic migration phases. | [Versioning — PQ VRF Strategy](../protocol/versioning#post-quantum-vrf-strategy) |

#### Implementation — Specification

| # | Question | Resolution | Location |
|---|----------|-----------|----------|
| 4 | **AppManifest State Migration** — Execution semantics | Full state delivered via LOAD opcode as CBOR. Success requires HALT + valid CBOR output + schema conformance + within max_cycles. All-or-nothing (no partial migration). Determinism enforced; hash comparison detects violations. No automatic rollback — users pin old manifest hash. | [MHR-App — Migration](../services/mhr-app/upgrades#migration-contract-execution-semantics) |
| 5 | **AppManifest Schema Compatibility** — Formal definition | Programmatic compatibility checker: no removed fields, no type changes, new fields with defaults only, required→optional allowed. CRDT types cannot change between versions. Unknown fields from newer schemas preserved via LWW fallback during merge. | [MHR-App — Schema Compatibility](../services/mhr-app/upgrades#schema-compatibility-rules) |
| 6 | **MHR-Name Cross-Scope Query Routing** — Vague algorithm | DHT-guided scope routing: nodes register as ScopeAnchors at Blake3(scope_string) in DHT. Cross-scope queries look up the scope key, route to nearest anchor by trust distance → hop count → XOR distance. TTL = 5 × gossip_interval (adapts to transport speed). SCOPE_UNREACHABLE returned if no anchor exists. Hierarchical registration ensures ancestor scopes are queryable. | [MHR-Name — Cross-scope queries](../services/mhr-name#trust-weighted-propagation) |
| 7 | **Visibility-Controlled Claim Updates** — Key rotation + versioning | Revoked peers lose forward access (new key), retain historical access (old key). Claims versioned by (claimant, claim_type, qualifier) with created timestamp ordering. Encrypted claims cached as ciphertext; decrypted on key receipt. Partition key rotation reconciled via KEY UNIFICATION when claimant reconnects. | [MHR-ID — Key Rotation and Claim Updates](../services/mhr-id#key-rotation-and-claim-update-semantics) |
| 8 | **Payment Channel Settlement Atomicity** — Protocol not fully specified | Two-phase signing with 120-gossip-round timeout. Half-signed records are never published. After 3 failed attempts: unilateral settlement with 2,880-round challenge window. Strictly all-or-nothing — CRDT ledger only accepts dual-signed records. Channel close via standard 4-epoch abandonment rules. | [Payment Channels — Settlement](../economics/payment-channels#bilateral-payment-channels) |
| 9 | **Gossip Bandwidth Under Congestion** — Enforcement missing | Dedicated gossip token bucket with 2% floor guarantee (min 10 bytes/sec). Gossip throttled (queued), not dropped. Tier 1-2 never dropped. Starvation recovery mode (20% budget) triggered after 10 missed gossip intervals. Weighted fair queuing with strict floor preemption. | [Network Protocol — Gossip Congestion](../protocol/network-protocol#gossip-congestion-handling) |
| 10 | **Cryptographic Compute Verification** — ZK costs TBD | Decision matrix by workload size. ZK proofs (RISC Zero/SP1/Jolt) viable for contracts up to ~10^6-10^8 cycles at 500-1000x prover overhead. TEE attestation (AMD SEV-SNP, NVIDIA H100 CC) for large models at ~5% overhead. Consumer chooses verification tier; protocol provides framework, market determines adoption. | [MHR-Compute — Cryptographic Verification](../services/mhr-compute#cryptographic-verification-details) |
| 11 | **Voting Hardware Liveness** — No quantified error rates | Multi-feature radio fingerprinting: RSSI pattern (w=0.5) + clock drift/CFO (w=0.35) + timing offset (w=0.15). False positive ~1-5% depending on environment. ~15-30 radios reliably distinguished per km² with 3+ witnesses. Remote voters rely on trust flow + personhood vouching (no hardware liveness); capped at Verified (not StronglyVerified) geo multiplier. | [Voting — Radio Fingerprinting](../applications/voting#radio-fingerprinting-algorithm) |

### v1.0 Resolutions (28 Questions, 5 Rounds)

All questions from the v1.0 spec review rounds (5 rounds, 28 questions total) have been resolved with concrete specifications. The resolution history is preserved in the git log. Key resolution areas:

- **Wire format**: Serialization rules, endianness, TLV extensions, CompactPathCost encoding
- **Timing**: Epoch triggers, DHT rebalancing, beacon collision handling, fragment reassembly
- **Economics**: Settlement timing, channel sequence semantics, credit rate-limiting, difficulty targets
- **Security**: Nonce handling, session key rotation, KeyCompromiseAdvisory replay, reputation initialization
- **Infrastructure**: WASM sandbox tiers, capability bitfield, Ring 1 aggregation, DHT metadata format
- **Design decisions**: Group admin model, reputation gossip, onion routing, MHR-Byte opcodes, emission schedule, protocol bridges, formal verification targets

---

### Design Decision: Partition Defense
<!-- Source: docs/development/partition-defense-comparison.md -->

# Distributed Exchange vs Trust Graph: Partition Defense Comparison

The [isolated partition attack](../economics/token-security#attack-isolated-partition) is the most significant economic threat to Mehr's CRDT-based ledger. An attacker runs 100 virtual nodes on a single machine (~$60/year), isolates them in a partition, and mints MHR uncontested. Post-bootstrap (epoch > 100,000), [GenesisAttestation](../economics/mhr-token#genesis-anchored-minting) sunsets and the network needs a **fully distributed** defense — no genesis root, no central authority.

Two architectures address this:

| | **A: Trust-Gated Active Set + Merge-Time Audit** | **B: Neighborhood-Scoped Minting** |
|---|---|---|
| **One-liner** | One global MHR. Minting requires trust links. Untrusted minting rejected on merge. | Local currencies per neighborhood. Cross-neighborhood exchange at market rates. |
| **Defense type** | Retroactive — minting audited when partitions reconnect | Structural — attacker tokens are a different currency |
| **Status** | **Current design** | Not adopted — see [rationale](#rationale) |

## The Problem

During bootstrap (epoch 0–100,000), [GenesisAttestation](../economics/mhr-token#genesis-anchored-minting) prevents all isolated partition minting by requiring a signed proof of connectivity to a genesis node. This works but is centralized — every attestation chain traces back to one root.

Post-bootstrap, three approaches are possible:

1. **Make GenesisAttestation permanent** — Not viable. This is "centralization with extra steps." Every attestation chain forever traces back to genesis. A community that forms after bootstrap and has never connected to the genesis graph can never mint. Not distributed.

2. **Trust-gated active set + merge-time audit** — Current design. Uses the existing trust graph as a minting gate. Merge-time audit rejects untrusted partition minting on reconnection. Fully distributed post-bootstrap.

3. **Neighborhood-scoped minting** — Not adopted. Each neighborhood mints its own denomination with cross-neighborhood exchange. Structurally eliminates the attack but creates massive new complexity.

## Approach A: Trust-Gated Active Set + Merge-Time Audit

### Design

**Continuous gate** (connected network):
- A node is **minting-eligible** if it is in the [active set](../economics/epoch-compaction#epoch-lifecycle) AND has ≥1 mutual trust link with another active-set member
- "Mutual trust link" = both nodes have each other in `trusted_peers`
- This is a soft Sybil gate — it doesn't prevent the partition attack (attacker nodes trust each other) but provides basic hygiene in the connected network

**Merge-time defense** (partition reconnection):
- When a partition reconnects, CRDT data merges normally (unconditional convergence)
- The **minting component** is audited: cross-partition trust scoring determines what fraction of partition minting is accepted
- `partition_trust_score = (nodes trusted by main-network peers) / (total partition nodes)`
- Untrusted minting enters quarantine (10 epochs), then is permanently rejected if unproven
- See [merge-time trust audit](../economics/token-security#merge-time-trust-audit) for full spec

### Attack Outcomes

| Scenario | Dilution |
|---|---|
| Fresh localhost (100 virtual nodes, 0 trust) | **0.00%** |
| Pre-planned, 1/100 nodes trusted by main network | **0.99%** |
| Pre-planned, 10/100 trusted | 9.09% |
| Deep infiltration, 50/100 trusted | 33.33% |
| Extreme infiltration, 90/100 trusted | 47.37% |

### Properties

- **Fully distributed**: No genesis dependency post-bootstrap. Trust graph is the only authority.
- **One global currency**: MHR is MHR everywhere. Zero exchange friction.
- **Backward compatible**: Nodes that haven't implemented the audit yet degrade gracefully.
- **CRDT-safe**: Audit is an economic overlay on convergent data, not a modification to CRDT merge.
- **~500 lines of new code**: Trust-link check on active set + merge-time audit logic.

## Approach B: Neighborhood-Scoped Minting

### Design

**Local currencies**:
- Each trust neighborhood mints its own denomination: MHR-Portland, MHR-Tehran, MHR-Attacker
- A `neighborhood_tag` (trust anchor hash) tags every minted unit
- Cross-neighborhood payments use bilateral exchange channels at market-negotiated rates
- Exchange rate = supply/demand at boundary nodes (nodes in multiple neighborhoods)

**Structural defense**:
- Attacker creates "MHR-Attacker" in their isolated partition
- Zero real services → zero demand for MHR-Attacker → zero exchange rate
- No impact on any other neighborhood's economy
- The market IS the proof: tokens backed by nothing have no value

### Attack Outcomes

| Scenario | Dilution on honest neighborhoods |
|---|---|
| Fresh localhost (any number of nodes) | **0.00%** (different currency) |
| Pre-planned, any trust level | **0.00%** (different currency) |
| Deep infiltration, any level | **0.00%** (different currency) |
| Attacker provides 10% real services | ~9.09% (exchange rate reflects real services) |
| Attacker provides 50% real services | ~33.33% (exchange rate reflects real services) |

### Properties

- **Fully distributed**: Each neighborhood is self-sovereign. No external dependency.
- **Structurally immune**: Attack produces worthless tokens by definition.
- **Perfect partition tolerance**: Each partition IS its own economy. No merge conflict ever.
- **~5,000+ lines of new code**: Cross-denomination channels, exchange rate protocol, wallet UX.

## Head-to-Head Comparison

### Security

| Attack | A: Trust-Gated+Audit | B: Neighborhood-Scoped |
|---|---|---|
| Fresh identities (the real threat) | **0%** | **0%** |
| 1% trust infiltration | 0.99% | 0% |
| 50% trust infiltration | 33.3% | 0% |
| Attacker provides 50% real services | N/A (one currency) | 33.3% |

**Verdict**: Tied for realistic attacks (fresh IDs). B wins on deep infiltration. But if the attacker in B provides real services (the only way to get exchange value), dilution converges to the same numbers — B just shifts the attack surface from "trust infiltration" to "exchange rate manipulation."

### User Experience

| Scenario | A | B |
|---|---|---|
| Pay for service in your city | Send MHR | Send MHR-Portland |
| Pay for service in another city | Send MHR (same currency) | Exchange MHR-Portland → MHR-Tehran, then send |
| Move to a new city | Nothing changes | Must exchange all tokens |
| Check your balance | "1,000 MHR" | "800 MHR-Portland + 150 MHR-Tehran + 50 MHR-Guild" |
| New node joining | Add a contact, start minting | Join neighborhood, get local denomination |

**Verdict**: A wins decisively. B imposes foreign exchange friction on every cross-community interaction.

### Implementation Complexity

| Component | A | B |
|---|---|---|
| Active set | Add trust-link check (~20 lines) | Add `neighborhood_tag` to active set |
| CRDT ledger | Merge-time audit (already spec'd) | `epoch_balance` becomes per-denomination. GCounter entries tagged. Major rework. |
| Payment channels | No change | Cross-denomination channels: new type, exchange negotiation, dual-currency settlement. Major rework. |
| Emission schedule | No change | Per-neighborhood emission with neighborhood size tracking |
| New protocol | Trust proofs at merge (~200 bytes) | Exchange rate gossip, cross-denom channel ops (~5 new message types) |
| Wallet UX | No change | Multi-denomination display, exchange interface |
| **Total** | **~500 lines** | **~5,000+ lines** |

**Verdict**: A is 10x simpler. B requires changes to nearly every economic subsystem.

### Architecture

| Concern | A | B |
|---|---|---|
| Single global currency | **Preserved** | Destroyed — MHR becomes a token family |
| CRDT properties | Preserved entirely (audit is economic overlay) | GCounter merge becomes denomination-aware |
| Existing spec changes | Moderate (mhr-token.md, crdt-ledger.md) | Massive (every economic doc) |
| Backward compatible | Yes (graceful degradation) | No (breaking change) |

**Verdict**: A is a surgical addition. B is a fundamental architecture change.

### Philosophy

| Principle | A | B |
|---|---|---|
| Distributed? | Yes — trust graph is sole authority | Yes — each neighborhood self-sovereign |
| Communities first? | Communities share one economy. Free local, paid global. | Communities own their economy. Free local, exchange at boundary. |
| Partition tolerance? | Good — minting works during isolation, audited on merge | Perfect — each partition is its own economy |
| Simplicity? | High — one currency, one wallet | Low — multi-currency management |
| Real-world parallel | One country, one dollar (with post-partition audit) | Multi-country with forex markets |

**Verdict**: Philosophical tie. A favors cohesion ("one MHR for everyone"). B favors sovereignty ("each community controls its money"). Both are valid readings of "communities first."

### Edge Cases

| Edge Case | A | B |
|---|---|---|
| 1-node neighborhood | Works — gets trust from any peer, mints global MHR | Mints into local denomination with zero liquidity |
| Nomadic user | MHR works everywhere | Must exchange on every move |
| Bridge node (2 neighborhoods) | Normal node | Must hold 2 denominations, provides liquidity |
| Adversarial exchange rate manipulation | N/A (one currency) | **New attack surface** — attacker manipulates rates between neighborhoods |
| New neighborhood forming | Any trusted peer → in | Must create new denomination. Who accepts your new token? (Chicken-and-egg) |

**Verdict**: A handles edge cases naturally. B creates new problems for single-node communities, nomadic users, and new neighborhoods.

## Design Rationale {#rationale}

1. **Security is equivalent for practical attacks.** Both produce 0% dilution for fresh-identity localhost attacks — the realistic, cheap threat. The gap is deep trust infiltration (50%+ of attacker nodes pre-trusted by real people). This requires sustained social engineering, is slow, visible, and doesn't scale. It's also bounded by the halving schedule.

2. **Approach B solves one problem by creating three.** Exchange rate manipulation, liquidity bootstrapping, and cross-denomination UX are each individually harder unsolved problems than the partition attack itself.

3. **10x simpler.** ~500 lines vs ~5,000+. Zero changes to payment channels, wallet UX, or protocol messages. The merge-time audit is already specified in [crdt-ledger.md](../economics/epoch-compaction#merge-time-supply-audit).

4. **B hurts legitimate users more than attackers.** Users moving between cities, paying for cross-community services, or running multi-region applications all face exchange friction. The attacker pays zero (their tokens are worthless either way).

5. **A is more "communities first."** Communities share a global commons (one MHR), communicate freely within trust boundaries, and pay at the boundary. This matches Mehr's ethos better than fragmenting the economy into sovereign micro-currencies.

### Residual Risk

The remaining exposure in Approach A is **deep trust infiltration** — an attacker who gets ≥50% of their partition nodes trusted by main-network peers before isolating. This worst case produces ~33% dilution per cycle.

Mitigations:
- This requires **sustained social engineering** — building real trust relationships with real people who will absorb your debts. This is the hardest, slowest, most visible form of attack in any system.
- The **halving schedule** bounds cumulative damage. Each halving period (~1.9 years), attacker per-epoch minting halves.
- **Per-cycle visibility**: The attacker must reconnect to spend the minted MHR, making each cycle observable.
- If deep infiltration proves to be a **practical** problem (observed in the wild, not just theoretical), neighborhood-scoped minting can be introduced as a network evolution: A → A+B (dual-mode) → B (if needed). This should be a response to observed attacks, not a pre-emptive architectural bet.

### Key Insight

The partition attack is about **minting**. Neighborhood-scoped minting solves it by making minting local. But the merge-time trust audit solves it just as well for all realistic attacks, without fragmenting the economy. The ~33% worst case requires social engineering at scale — which is already the hardest attack vector in any distributed system.

## Quantitative Model

The full analysis with dilution calculations across all scenarios is in the [defense comparison script](https://github.com/mehr-network/mehr-docs/blob/main/scripts/defense_comparison.py). The [localhost partition analysis](https://github.com/mehr-network/mehr-docs/blob/main/scripts/localhost_partition_analysis.py) demonstrates the cost gap that motivates this defense.

---

### Landscape Analysis
<!-- Source: docs/development/landscape.md -->

# Landscape Analysis

An honest comparison of Mehr against existing decentralized and mesh networking projects. The individual pieces of Mehr aren't novel — the combination is. Every feature exists somewhere else. What no existing project does is combine them all into a single coherent stack.

---

## Prior Art by Feature

| Mehr Feature | Who Did It First / Best |
|---|---|
| Transport-agnostic mesh | [Reticulum](https://reticulum.network/) — Mehr builds on it |
| LoRa mesh messaging | [Meshtastic](https://meshtastic.org/), [goTenna](https://gotenna.com/) |
| Cryptographic identity (no registrar) | [SSB](https://scuttlebutt.nz/), [Briar](https://briarproject.org/), Reticulum, many others |
| Content-addressed storage | [IPFS](https://ipfs.tech/) (2015) |
| Decentralized storage market | [Filecoin](https://filecoin.io/) |
| Paid mesh relay | [Althea](https://althea.net/) (mesh ISP economics) |
| Token incentives for infrastructure | [Helium](https://www.helium.com/) (DePIN) |
| CRDT-based state (no blockchain) | [Holochain](https://www.holochain.org/) (agent-centric + mutual credit) |
| Onion routing | [Tor](https://www.torproject.org/) (2002), [I2P](https://geti2p.net/) |
| Offline-first social | [SSB](https://scuttlebutt.nz/) (append-only feeds, gossip replication) |
| No-source-address privacy | Reticulum |
| DHT routing | Kademlia (2002), used by IPFS, BitTorrent, etc. |
| Erasure coding for storage | Standard technique (Reed-Solomon), Filecoin uses it |
| Smart contracts / compute | Ethereum, Filecoin FVM, Holochain |

---

## Project-by-Project Comparison

### Reticulum

[Reticulum](https://reticulum.network/) is the closest spiritual ancestor — Mehr explicitly builds on its transport layer.

**What it does**: A cryptography-based networking stack for building networks over any medium. Replaces IP-based networking with destination-hash-based addressing using Ed25519 keys.

**Transport**: Fully transport-agnostic. LoRa (via RNode), packet radio, WiFi, Ethernet, serial, BLE, free-space optical, AX.25. Proven on links as slow as 5 bps. TCP/IP can also serve as a carrier.

**Economics**: None. No token, no node compensation. Purely volunteer/altruistic relay.

**Storage**: None. Transport-only stack.

**Compute**: None.

**Privacy**: Strong. No source address in packets (structural sender anonymity). AES-256-CBC with ephemeral ECDH on Curve25519, forward secrecy.

**Constrained devices**: ESP32 serves as radio modem, but the Reticulum stack itself runs on the host (Python on Raspberry Pi or Linux).

**Partition tolerance**: Excellent. Designed for partitioned operation.

**Mehr adds**: Everything above transport — economics, storage, compute, marketplace, social. Reticulum is Layer 0; Mehr is Layers 1-6.

---

### Meshtastic

**What it does**: Open-source firmware that turns cheap LoRa hardware into off-grid mesh communicators. Text messaging and position sharing.

**Transport**: LoRa only for mesh hops. BLE for phone pairing, WiFi for configuration/MQTT bridge.

**Economics**: None.

**Storage**: Minimal store-and-forward for offline messages.

**Privacy**: Limited. AES-256 with a shared pre-shared key — all channel members share the same key.

**Constrained devices**: Runs natively on ESP32 and nRF52840. $15-50 per node.

**Partition tolerance**: Good for broadcasts. Direct messages can fail if the path breaks.

**Mehr adds**: Multi-transport bridging, economic incentives, content-addressed storage, DHT, compute, per-message E2E encryption (not shared PSK), cost-aware routing.

---

### Helium

**What it does**: Decentralized wireless infrastructure (DePIN) providing LoRaWAN for IoT and mobile coverage (5G/WiFi). Hotspot operators earn tokens.

**Transport**: LoRaWAN + WiFi/CBRS 5G. Hotspots connect to the Solana blockchain via internet backhaul. Star topology — not a mesh between hotspots.

**Economics**: HNT token on Solana. Proof of Coverage + data transfer rewards. Data Credits at fixed $0.00001, created by burning HNT.

**Storage**: None.

**Privacy**: Minimal. Hotspot locations are public. All transactions on a public blockchain.

**Constrained devices**: Hotspots require dedicated hardware ($200-500) with persistent internet and power.

**Partition tolerance**: Poor. Each hotspot requires continuous internet. No mesh between hotspots.

**Mehr adds**: Actual mesh topology, partition tolerance, no blockchain dependency, storage + compute, operation without internet.

---

### IPFS / Filecoin

**What it does**: IPFS is a content-addressed distributed file system. Filecoin adds economic incentives for persistent storage. libp2p is the networking library.

**Transport**: libp2p supports TCP, QUIC, WebSocket, WebRTC — but assumes internet connectivity. No LoRa or radio transport.

**Economics**: Filecoin FIL token. Storage providers earn FIL via Proof of Spacetime and Proof of Replication. Clients pay FIL to store data. On-chain settlement.

**Storage**: Core feature. Content-addressed blocks (CIDs) with Kademlia DHT. Filecoin adds persistence guarantees with cryptographic proofs.

**Compute**: Emerging. Filecoin Virtual Machine (FVM) is WASM-based. Compute-over-data in development (Bacalhau project).

**Privacy**: Limited. Data is public by default. No built-in encryption. Filecoin deals are on a public blockchain.

**Constrained devices**: Cannot run on constrained devices. Filecoin storage providers need GPUs for proof generation.

**Partition tolerance**: Moderate for IPFS (content available from any node that has it). Poor for Filecoin (proofs must be submitted to blockchain on schedule).

**Mehr adds**: Transport agnosticism, constrained-device support, no blockchain dependency, partition-tolerant economics, trust-based free tier, mesh radio capability.

---

### Holochain

**What it does**: Agent-centric distributed application framework. Each participant maintains their own signed hash chain and shares data to a DHT. No global consensus.

**Transport**: Internet-based (TCP/WebSocket). No radio or constrained transport support.

**Economics**: HoloFuel mutual-credit currency — balances created by counterparties, not minted from fixed supply. Fundamentally different from token-based systems.

**Storage**: Via validating DHT. Each app defines its own DNA (validation rules). Data sharded across responsible nodes.

**Compute**: Yes — each hApp runs application logic on participating nodes.

**Privacy**: Moderate. Source chains are private. DHT entries can be encrypted. No onion routing.

**Constrained devices**: Can run on Raspberry Pi. Far too heavy for microcontrollers.

**Partition tolerance**: Excellent. Agent-centric model means partitioned networks continue operating independently.

**Mehr adds**: Radio mesh transport, constrained-device support (ESP32), bandwidth-aware protocol design for LoRa, cost-aware routing, integrated connectivity marketplace.

---

### Scuttlebutt (SSB)

**What it does**: Peer-to-peer social networking via append-only logs and gossip. Offline-first, community-centric.

**Transport**: LAN discovery via multicast UDP. Internet via "Pub" relay servers. Direct TCP connections. No radio support.

**Economics**: None. Pubs run by volunteers.

**Storage**: Distributed through social graph replication — each node stores feeds of friends (and optionally friends-of-friends). No DHT.

**Privacy**: Moderate. Secret Handshake protocol authenticates connections. Private messages use asymmetric encryption. Social graph is visible.

**Constrained devices**: Requires Node.js (JavaScript implementation) or similar. Not suitable for microcontrollers.

**Partition tolerance**: Excellent. SSB's strongest feature. Feeds are append-only and self-certifying.

**Mehr adds**: Economic incentives, radio mesh, constrained-device support, content-addressed storage with erasure coding, DHT, compute contracts, cost-aware routing.

---

### Briar

**What it does**: Secure messenger for activists and journalists. Zero reliance on central infrastructure.

**Transport**: Multi-transport — Tor (internet), Bluetooth (10-30m), WiFi Direct. Also USB sneakernet. This multi-transport approach is similar to Mehr's philosophy, though narrower in scope.

**Economics**: None.

**Storage**: Messages stored encrypted on-device only.

**Privacy**: Excellent. E2E encryption, Tor routing hides metadata. Passed independent security audit (Cure53).

**Constrained devices**: Android app (primary). Desktop (beta). Cannot run on microcontrollers.

**Partition tolerance**: Good. Bluetooth and WiFi Direct work without internet. Cannot discover new peers in a partition.

**Mehr adds**: Economic incentives, extended range (LoRa), storage + compute services, DHT, cost-aware routing, social features.

---

### Matrix

**What it does**: Open standard for decentralized, federated real-time communication. Interoperable messaging and VoIP across independent homeservers.

**Transport**: HTTP/HTTPS over TCP/IP. WebSocket for clients. Assumes internet at all times.

**Economics**: None at protocol level. Homeserver operators bear costs.

**Storage**: Homeservers replicate room state. Not content-addressed.

**Privacy**: Moderate-Good. Olm/Megolm E2E encryption. Metadata visible to homeserver operators.

**Constrained devices**: Homeservers require server-class hardware. Not suitable for constrained devices.

**Partition tolerance**: Poor. Federation requires internet. Offline homeserver = unreachable users.

**Mehr adds**: Transport agnosticism, mesh radio, no server dependency, economic incentives, partition tolerance, constrained-device support.

---

### Hyphanet (formerly Freenet)

**What it does**: Censorship-resistant publishing and communication. Anonymous distributed data storage.

**Transport**: Internet-only (TCP/IP). No radio or mesh.

**Economics**: None. Volunteer node operators.

**Storage**: Core feature. Encrypted chunks distributed across the network. Content persists based on popularity.

**Privacy**: Strong. Two modes: Opennet (random connections) and Darknet (trusted peers only). Multi-layer encryption. Publishers and retrievers are anonymous.

**Constrained devices**: Requires Java runtime and persistent disk (10-20 GB). Cannot run on constrained devices.

**Partition tolerance**: Moderate. Darknet mode tolerates some fragmentation. Content availability degrades with partition size.

**Mehr adds**: Radio mesh transport, constrained-device support, economic incentives, cost-aware routing, compute contracts, integrated service marketplace.

---

### Althea

**What it does**: Protocol for decentralized ISP operation. Routers negotiate bandwidth pricing and route payments automatically.

**Transport**: Standard IP networking (WiFi, Ethernet, point-to-point wireless bridges). Uses Babel routing protocol. Not designed for LoRa or constrained radio.

**Economics**: Token-based. Routers pay neighbors per-byte for forwarding. Althea L1 blockchain for governance. Liquid Infrastructure Tokens (LITs) represent revenue-generating assets.

**Storage**: None. Connectivity protocol only.

**Privacy**: Limited. Traffic routing and payments are transparent between nodes. Standard ISP-like model.

**Constrained devices**: Requires OpenWrt-compatible routers. Not suitable for microcontrollers.

**Partition tolerance**: Poor. Economic model requires blockchain access for settlement.

**Mehr adds**: Radio mesh (LoRa), constrained-device support, no blockchain dependency, partition-tolerant economics (CRDT ledger), storage + compute, trust-based free tier.

---

### goTenna

**What it does**: Hardware mesh networking devices for off-grid communication. Primarily targets first responders and military. Proprietary closed-source protocol.

**Transport**: Proprietary radio on VHF/UHF bands. Bluetooth for phone pairing. No internet connectivity — purely radio mesh.

**Economics**: Hardware sales ($849+ per Pro X2 unit). Previously explored token incentives (Lot49 protocol) but did not commercialize.

**Storage**: None beyond store-and-forward.

**Privacy**: Moderate. 384-bit ECC E2E encryption. But proprietary and closed-source — no independent verification.

**Constrained devices**: Requires purchasing proprietary hardware. Cannot use off-the-shelf components.

**Partition tolerance**: Excellent for messaging. Zero-control-packet approach minimizes overhead.

**Mehr adds**: Open protocol, off-the-shelf hardware, economic incentives, storage + compute, DHT, social features, internet bridging, transport agnosticism.

---

### Yggdrasil / CJDNS

**What they do**: Encrypted IPv6 overlay networks with self-healing mesh routing. Cryptographic addresses derived from public keys.

**Transport**: Overlay on existing IP networks. Cannot run on non-IP transports.

**Economics**: None. Volunteer-operated.

**Storage**: None. Routing-only overlays.

**Privacy**: Moderate. All traffic encrypted. Addresses derived from keys. No onion routing — traffic analysis possible.

**Constrained devices**: Yggdrasil (Go) and CJDNS (C) can run on Raspberry Pi and OpenWrt routers. Cannot run on bare microcontrollers.

**Partition tolerance**: Good. Self-healing routing adapts to topology changes.

**Mehr adds**: Radio mesh transport, economic incentives, storage + compute, constrained-device support (ESP32), capability marketplace.

---

### Tor / I2P

**What they do**: Anonymous overlay networks. Tor: circuit-switched onion routing for internet access and hidden services. I2P: packet-switched garlic routing for internal services.

**Transport**: Internet-only. TCP (Tor) or UDP (I2P). Completely non-functional without internet.

**Economics**: None. Volunteer-operated relays.

**Storage**: None.

**Privacy**: Very strong. Multi-hop onion/garlic routing with layered encryption. Tor: 3 hops + directory authorities. I2P: 6-hop unidirectional tunnels.

**Constrained devices**: Cannot run on constrained devices.

**Partition tolerance**: None. Requires internet connectivity.

**Mehr adds**: Transport agnosticism (works without internet), economic incentives, storage + compute, constrained-device support, partition tolerance. Mehr's optional onion routing (`PathPolicy.ONION_ROUTE`) provides privacy for high-threat scenarios without requiring always-on anonymization.

---

### Session

**What it does**: Privacy-focused messenger requiring no phone number or email. Onion routing over a decentralized network of staked nodes.

**Transport**: Internet-based. Three-hop onion routing through Session Nodes. Session Network blockchain for staking and rewards.

**Economics**: Session Token (migrated from OXEN in 2025). Node operators stake tokens and earn rewards for routing and storage.

**Storage**: Temporary. Swarm-based message storage for offline delivery. Not general-purpose.

**Privacy**: Very strong. No registration info required. Onion routing hides IPs. E2E encryption.

**Constrained devices**: Clients on phones/desktops. Session Nodes require server-class hardware. Not suitable for microcontrollers.

**Partition tolerance**: Poor. Requires internet to reach Session Nodes.

**Mehr adds**: Transport agnosticism, mesh radio, constrained-device support, general-purpose storage + compute, no blockchain dependency, partition tolerance, trust-based free relay.

---

### Nostr

**What it does**: Minimalist protocol for decentralized social networking. Cryptographic keys for identity, relays for message distribution.

**Transport**: WebSocket connections to relay servers. Assumes internet.

**Economics**: No built-in token. Relay operators may charge. Lightning Network zaps for tipping.

**Storage**: Relays store events. No content-addressing, no DHT, no erasure coding. Persistence depends entirely on relay willingness.

**Privacy**: Weak. Events typically unencrypted. Relays see all content and metadata. IP addresses visible.

**Constrained devices**: Clients are lightweight. Relays need server hardware.

**Partition tolerance**: Moderate. Clients can connect to any subset of relays.

**Mehr adds**: Transport agnosticism, mesh radio, constrained-device support, protocol-level economic incentives, content-addressed storage with erasure coding, DHT, compute, E2E encryption by default, partition tolerance.

---

### Safe Network (formerly MaidSafe)

**What it does**: Fully autonomous, decentralized data network aiming to replace the client-server model. Self-managing with no human oversight.

**Transport**: Internet-based P2P. No radio or constrained transport support.

**Economics**: Token-based. MaidSafeCoin (MAID) placeholder; Safe Network Tokens (SNT) at mainnet. Users pay for storage; node operators earn for providing resources.

**Storage**: Core feature. Self-encryption, chunking, distributed across the network. Node operators cannot read stored content.

**Privacy**: Very strong by design. Self-encryption ensures data is encrypted before leaving the user's device.

**Constrained devices**: Intended for consumer hardware. Not suitable for microcontrollers.

**Partition tolerance**: Moderate. Section-based architecture tolerates some fragmentation but requires significant connectivity.

**Note**: After 18+ years of development, the project remains in alpha.

**Mehr adds**: Radio mesh transport, constrained-device support, partition tolerance as first-class constraint, shipping strategy (server-first, proven layers before extending).

---

### Dat / Hypercore Protocol

**What it does**: Peer-to-peer data sharing based on append-only, cryptographically signed logs (Hypercores). Now developed as the Pear Runtime for P2P applications.

**Transport**: Internet-based. Hyperswarm handles discovery via DHT with NAT holepunching. Supports LAN discovery. No radio.

**Economics**: None. Purely voluntary replication.

**Storage**: Yes. Append-only logs with Merkle tree verification. Partial replication — peers download only ranges they need. Hyperdrive provides file system abstraction. No incentive to store others' data.

**Privacy**: Limited. Hypercores identified by public keys. No encryption at rest by default. DHT exposes interest metadata.

**Constrained devices**: Node.js-based. Not suitable for microcontrollers.

**Partition tolerance**: Good. Append-only logs are inherently partition-tolerant.

**Mehr adds**: Economic incentives for storage, radio mesh, constrained-device support, erasure coding, cost-aware routing, compute contracts.

---

## Comparison Matrix

How each project maps to Mehr's key design axes.

| Project | Transport Agnostic | Token / Economics | Storage | Compute | Privacy | Runs on MCU | Partition Tolerant |
|---|---|---|---|---|---|---|---|
| **Mehr** | **Yes** (LoRa to fiber) | **Yes** (VRF lottery, CRDT ledger) | **Yes** (erasure-coded) | **Yes** (MHR-Byte + WASM) | **Yes** (no source addr + opt-in onion) | **Yes** (ESP32) | **Yes** (economic convergence) |
| Reticulum | **Yes** | No | No | No | **Yes** | Partial (modem only) | **Yes** |
| Meshtastic | LoRa only | No | No | No | Weak (shared PSK) | **Yes** | **Yes** |
| Helium | LoRaWAN + WiFi | **Yes** (HNT/Solana) | No | No | No | No | No |
| IPFS / Filecoin | IP only | **Yes** (FIL) | **Yes** | Partial (FVM) | No | No | Partial |
| Holochain | IP only | **Yes** (mutual credit) | **Yes** (DHT) | **Yes** (hApps) | Partial | No | **Yes** |
| SSB | LAN + internet | No | Partial (social graph) | No | Partial | No | **Yes** |
| Briar | Tor + BT + WiFi | No | No | No | **Yes** | No | Partial |
| Matrix | HTTP only | No | Partial (room state) | No | Partial (E2EE) | No | No |
| Hyphanet | TCP only | No | **Yes** | No | **Yes** | No | Partial |
| Althea | WiFi / Ethernet | **Yes** (blockchain) | No | No | No | No | No |
| goTenna | Proprietary radio | No | No | No | Partial | Proprietary HW | **Yes** |
| Yggdrasil / CJDNS | IP overlay | No | No | No | Partial | No | Partial |
| Tor / I2P | Internet only | No | No | No | **Yes** | No | No |
| Session | Internet only | **Yes** (staking) | Partial (temp msgs) | No | **Yes** | No | No |
| Nostr | WebSocket | Partial (Lightning) | Partial (relays) | No | No | No | Partial |
| Safe Network | Internet only | **Yes** (planned) | **Yes** (planned) | Partial (planned) | **Yes** | No | Partial |
| Dat / Hypercore | IP + LAN | No | **Yes** (append logs) | No | No | No | Partial |

---

## What's Actually Novel

Six aspects of Mehr's design have no direct equivalent in existing projects:

### 1. ESP32-to-Datacenter Unified Economic Protocol

No project spans this hardware range with a single economic protocol. Meshtastic runs on ESP32 but has no economics. Filecoin has economics but needs GPUs. Holochain needs at minimum a Raspberry Pi. Mehr's [three participation levels](../protocol/physical-transport#participation-levels) (L0 transport-only, L1 relay with lottery, L2 full node) let a $5 microcontroller and a datacenter server participate in the same economy.

### 2. Stochastic Relay Rewards via VRF Lottery

Althea does paid relay but settles on a blockchain. Helium uses Solana. Mehr's [VRF lottery](../economics/payment-channels) reduces channel state updates by ~10x compared to per-packet payment, and the [CRDT ledger](../economics/crdt-ledger) converges without consensus. The specific mechanism — ECVRF-ED25519-SHA512-TAI lottery, bilateral payment channels, CRDT settlement, epoch compaction — doesn't exist elsewhere as a complete system.

### 3. Economic Partition Tolerance

SSB and Holochain handle *data* partition tolerance well. No project addresses what happens to *money* when the network splits. Mehr's CRDT ledger with [partition-aware epoch compaction](../economics/epoch-compaction#epoch-triggers), GCounter rebase, and bounded overminting (max 1.5x) is designed for the case where a village on LoRa is a permanent partition with its own functioning economy.

### 4. Trust-Based Free Tier Integrated with Paid Economics

CJDNS and Hyphanet's darknet use friend-of-friend topology, but without economics on top. Mehr's [trust graph](../economics/trust-neighborhoods) lets friends relay for free while the same protocol charges strangers — and the boundary is fluid. No other project makes this the core economic primitive, with the paid layer activating only when traffic crosses trust boundaries.

### 5. Cost-Aware Kleinberg Small-World Routing

No existing project combines formal small-world routing (O(log² N) hop guarantee) with per-link economic cost metrics. Reticulum has announce-based path discovery but no cost awareness. Althea has cost-aware routing but uses Babel (distance-vector, not small-world optimized). Mehr's [routing model](../protocol/network-protocol#routing) provides both scalability guarantees and economic efficiency.

### 6. Integrated Capability Marketplace

Filecoin does storage + emerging compute. Althea does connectivity. Holochain does storage + compute. No single project unifies storage, compute, and connectivity into one [discovery/negotiation/verification/payment framework](../marketplace/overview) where a node advertises whatever it can do and the market determines its role.

---

## The Closest Projects

Four projects come closest to Mehr's vision, each covering one or two layers:

| Project | What It Covers | What It Lacks |
|---|---|---|
| **Reticulum** | Transport + routing + identity (Layer 0-1) | Economics, storage, compute, marketplace |
| **Holochain** | Agent-centric data + mutual credit + apps | Radio mesh, constrained devices, bandwidth-aware design |
| **Althea** | Mesh ISP economics + paid relay | Radio mesh, storage, compute, partition tolerance (needs blockchain) |
| **IPFS / Filecoin** | Content-addressed storage + emerging compute market | Radio mesh, constrained devices, partition tolerance (needs blockchain) |

Mehr's contribution is integrating these into a single stack that runs from ESP32 to datacenter, works without internet, and converges economically after network partitions.

---
